
[{"content":" Fabrice Kirchner: Mein Weg durch die Tech-Welt # Willkommen auf meiner persönlichen Seite!\nIch bin Fabrice, ein Berliner, der nach einiger Zeit in Bayern nun in Hessen zu Hause ist. Meine Leidenschaft? Technologie, unermüdliche Neugier und das ständige Streben nach neuem Wissen. Ob ich mich in Fachbüchern vertiefe, das Internet nach innovativen Lösungen durchforste oder in Handbüchern versinke - ich bin immer bereit, etwas Neues zu lernen und mein Wissen zu erweitern.\nDiese Seite dient mir hauptsächlich als Spielwiese, um Erfahrungen mit Hugo zu sammeln und berufliche Konzepte auszuprobieren.\nWas mich antreibt: Meine Leidenschaften # Meine Reise in die Technikwelt begann mit dem Basteln an alter Hardware, führte über Spieleprogrammierung und Selbstständigkeit zur Kunst des Self-Hostings. Ich bin zutiefst überzeugt, dass wir die Kontrolle über unsere eigenen Daten zurückgewinnen sollten. Diese Überzeugung prägt meine Projekte und Begeisterungen:\nOpen Source \u0026amp; Linux: Für ein freies, offenes und transparentes Ökosystem. Self-Hosting \u0026amp; Cloud-freie Lösungen: Maximale Souveränität über die eigenen Daten. Heimautomatisierung: Das smarte Zuhause, das mitdenkt und den Alltag vereinfacht. Erneuerbare Energien: Effiziente Nutzung von Solarenergie (PV) für eine nachhaltige Zukunft. Gaming: Aktives Spielen und der Austausch in der Community (Ingress). Abenteuer in der Natur: Ausgleich und neue Energie beim Wandern und Radfahren. Wissen weitergeben: Die nächste Generation für Technik und Innovation begeistern. Familie # Auch wenn meine Leidenschaft der Technik gehört, sind meine größte Inspirationsquelle und mein wichtigster Ausgleich meine Familie. Wenn ich den Laptop zuklappe, beginnt die schönste Zeit des Tages. Die gemeinsamen Momente, das Lachen und der Zusammenhalt sind der wahre Antrieb für alles, was ich tue und erinnern mich immer wieder daran, dass die besten Verbindungen nicht aus Kabeln, sondern aus Liebe und gemeinsamen Erinnerungen bestehen.\nMein ehrenamtliches Engagement # Technisches Hilfswerk (THW) # Mit Stolz engagiere ich mich ehrenamtlich im Technischen Hilfswerk (THW), der Zivil- und Katastrophenschutzorganisation Deutschlands. Meine ehrenamtliche Heimat ist der Ortsverband Marburg. Dort gehöre ich dem Zugtrupp an, der Führungseinheit des Technischen Zuges, und unterstütze den Zugführer bei der Koordination und Leitung von Einsätzen. So helfen wir Menschen in Notlagen schnell und effektiv.\nDurch mein Engagement möchte ich einen aktiven Beitrag für die Sicherheit und den Zusammenhalt in unserer Gesellschaft leisten.\nInteresse geweckt?\nDu bist herzlich eingeladen, uns und unsere Arbeit kennenzulernen. Unser Dienst findet - außerhalb der hessischen Schulferien - jeden Donnerstag ab 19:30 Uhr in unserem Ortsverband in der Molkereistraße 7, 35039 Marburg statt.\nEine Anmeldung ist nicht erforderlich, erleichtert uns aber die Planung. Gib uns einfach kurz per E-Mail Bescheid: mail@thw-marburg.de\nEinen umfassenden Einblick in die bundesweite Arbeit des THW findest du auf der offiziellen Webseite: https://www.thw.de/\nMein beruflicher Fokus # Aktuell leite ich die IT-Abteilung in einem mittelständischen Unternehmen. Hier setze ich mein Wissen ein, um die technische Infrastruktur zu optimieren und einen reibungslosen Betrieb sicherzustellen.\nMeine Spezialgebiete \u0026amp; Kernkompetenzen # Projektmanagement: Als Fachexperte (Subject Matter Expert) steuere ich IT-Projekte von der Konzeption bis zur erfolgreichen Umsetzung. Software \u0026amp; Tools: Entwicklung und Pflege prozessunterstützender Anwendungen. Datenmanagement: Effiziente Verwaltung und Pflege komplexer Datenbestände. Prozessoptimierung: Definition, Dokumentation, Einführung und Automatisierung von Abläufen, inklusive Qualitätskontrolle und Datenbankauswertung. Systemarchitektur: Planung und Implementierung hochverfügbarer Systeme mit automatischem Rollout und Skalierung. IT-Sicherheit \u0026amp; Datenschutz: Entwicklung robuster Konzepte für IT-Security und Datensicherheit, Netzwerkarchitektur sowie TÜV-zertifizierter Datenschutz. Troubleshooting: Systematische Fehleranalyse und -behebung für komplexe IT-Herausforderungen. Meine berufliche Entwicklung # Seit 2019 bin ich bei Casa Due pur GmbH tätig. Zuvor war ich von 2001 bis 2019 für die Beratung, Planung und Umsetzung von IT-Projekten sowie die Installation und Wartung von IT-Anlagen verantwortlich. Darüber hinaus sammelte ich praktische Erfahrungen im Business Development sowie in der Lizenzberatung und -beschaffung in einem auf Softwarelizenzen spezialisierten Unternehmen.\nMeine akademische Ausbildung umfasst Informatik und Geschichte an der Philipps-Universität Marburg sowie Angewandte Informatik an der Fachhochschule Hof.\nAusgewählte Projekterfahrungen # Als Projektleiter und Experte habe ich eine Vielzahl von Projekten in unterschiedlichen Umgebungen erfolgreich gesteuert und umgesetzt:\nAls Projektleiter: Automatisierung von Infrastrukturen Einführung von Infrastructure as Code (IaC) Implementierung von Webshops Migration von IT-Infrastrukturen in Cloud-Lösungen und zurück (Azure, Google) Einführung Controller-basierter WLAN- und Active Directory-Infrastrukturen Software-Testing und Migration von Cloud-Umgebungen zu On-Premise Harmonisierung gewachsener Infrastrukturen, Redesign von Firewalls und VoIP-Transitionen Als Experte: Systemadministration (Fokus Linux, zudem Apple- und Windows-Systeme) Webdesign und Pflege von Webseiten Migration von Windows-Systemen Meine technischen Fähigkeiten # Programmiersprachen # Umfassend: PHP, HTML, CSS, Python, Bash, LPC, SQL, VBA Erfahren: Javascript, Java, Scala, JQuery Grundlagen: C, C++, C#, Assembler, Powershell Betriebssysteme # Umfassend: Debian-basierte Systeme (z.B. Ubuntu), Redhat-basierte Systeme (z.B. CentOS), Microsoft Windows \u0026amp; Windows Server, Apple iOS \u0026amp; macOS, Android. Tools \u0026amp; Technologien # Virtualisierung: VMware, KVM, Proxmox, Xen Datenbanken: MySQL, MariaDB, MS-SQL, Postgresql, Sqlite Server \u0026amp; Dienste: Microsoft AD, Openldap, Apache2, Postfix, Exim, Bind9, PowerDNS Automatisierung \u0026amp; Cloud: Kubernetes, Podman, Docker, Saltstack, Chef, Ansible, Cloud Setups (Google Cloud, Azure, Hetzner) Sicherheit \u0026amp; Netzwerk: Genua Firewall, IPFire, Opensense, PFSense, VPN (Strongswan, Openvpn) Sonstiges: Git, Subversion, Microsoft Office Suite, JTL-Shop, Magento, Dokuwiki, Mediawiki Branchen- \u0026amp; Themenfokus # Meine Branchenerfahrung umfasst u.a. das Hotelgewerbe, Finanzdienstleister, den Einzel- und Onlinehandel, das Gesundheitswesen, den öffentlichen Dienst sowie das Anwalts- und Notarwesen.\nMeine thematischen Schwerpunkte liegen in der Serveradministration, IT-Security, Backup-Konzepten, Monitoring, Automatisierung, Anforderungsanalyse sowie der Planung von Clustersystemen und Netzwerken.\nSprachen # Deutsch Englisch ","date":"24 Juli 2023","externalUrl":null,"permalink":"/about/","section":"Eine (scheinbar) leere Homepage","summary":"\u003ch2 class=\"relative group\"\u003eFabrice Kirchner: Mein Weg durch die Tech-Welt\n    \u003cdiv id=\"fabrice-kirchner-mein-weg-durch-die-tech-welt\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#fabrice-kirchner-mein-weg-durch-die-tech-welt\" aria-label=\"Anker\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cp\u003eWillkommen auf meiner persönlichen Seite!\u003c/p\u003e\n\u003cp\u003eIch bin Fabrice, ein Berliner, der nach einiger Zeit in Bayern nun in Hessen zu Hause ist. Meine Leidenschaft? \u003cstrong\u003eTechnologie, unermüdliche Neugier und das ständige Streben nach neuem Wissen.\u003c/strong\u003e Ob ich mich in Fachbüchern vertiefe, das Internet nach innovativen Lösungen durchforste oder in Handbüchern versinke - ich bin immer bereit, etwas Neues zu lernen und mein Wissen zu erweitern.\u003c/p\u003e","title":"Über mich","type":"page"},{"content":" Hier notiere ich Dinge für mich, und eventuell Interessierte # Wenig Struktur. Wenig Plan. Dafür meine Gedankensammlung, welche ich glaubte aufschreiben zu müssen.\n","date":"30 Juni 2025","externalUrl":null,"permalink":"/posts/","section":"Beiträge","summary":"\u003ch2 class=\"relative group\"\u003eHier notiere ich Dinge für mich, und eventuell Interessierte\n    \u003cdiv id=\"hier-notiere-ich-dinge-für-mich-und-eventuell-interessierte\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#hier-notiere-ich-dinge-f%c3%bcr-mich-und-eventuell-interessierte\" aria-label=\"Anker\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cp\u003eWenig Struktur. Wenig Plan. Dafür meine Gedankensammlung, welche ich glaubte aufschreiben zu müssen.\u003c/p\u003e","title":"Beiträge","type":"posts"},{"content":" Einleitung # Mit der folgenden Datenschutzerklärung möchten wir Sie darüber aufklären, welche Arten Ihrer personenbezogenen Daten (nachfolgend auch kurz als \u0026ldquo;Daten“ bezeichnet) wir zu welchen Zwecken und in welchem Umfang verarbeiten. Die Datenschutzerklärung gilt für alle von uns durchgeführten Verarbeitungen personenbezogener Daten, sowohl im Rahmen der Erbringung unserer Leistungen als auch insbesondere auf unseren Webseiten, in mobilen Applikationen sowie innerhalb externer Onlinepräsenzen, wie z.B. unserer Social-Media-Profile (nachfolgend zusammenfassend bezeichnet als \u0026ldquo;Onlineangebot“).\nDie verwendeten Begriffe sind nicht geschlechtsspezifisch.\nStand: 21. Januar 2022\nVerantwortlicher # Fabrice Kirchner\nHaingasse 3\n35287 Amöneburg\nE-Mail-Adresse: web(at)zyria.de\nÜbersicht der Verarbeitungen # Die nachfolgende Übersicht fasst die Arten der verarbeiteten Daten und die Zwecke ihrer Verarbeitung zusammen und verweist auf die betroffenen Personen.\nMaßgebliche Rechtsgrundlagen # Im Folgenden erhalten Sie eine Übersicht der Rechtsgrundlagen der DSGVO, auf deren Basis wir personenbezogene Daten verarbeiten. Bitte nehmen Sie zur Kenntnis, dass neben den Regelungen der DSGVO nationale Datenschutzvorgaben in Ihrem bzw. unserem Wohn- oder Sitzland gelten können. Sollten ferner im Einzelfall speziellere Rechtsgrundlagen maßgeblich sein, teilen wir Ihnen diese in der Datenschutzerklärung mit.\nZusätzlich zu den Datenschutzregelungen der Datenschutz-Grundverordnung gelten nationale Regelungen zum Datenschutz in Deutschland. Hierzu gehört insbesondere das Gesetz zum Schutz vor Missbrauch personenbezogener Daten bei der Datenverarbeitung (Bundesdatenschutzgesetz - BDSG). Das BDSG enthält insbesondere Spezialregelungen zum Recht auf Auskunft, zum Recht auf Löschung, zum Widerspruchsrecht, zur Verarbeitung besonderer Kategorien personenbezogener Daten, zur Verarbeitung für andere Zwecke und zur Übermittlung sowie automatisierten Entscheidungsfindung im Einzelfall einschließlich Profiling. Des Weiteren regelt es die Datenverarbeitung für Zwecke des Beschäftigungsverhältnisses (§ 26 BDSG), insbesondere im Hinblick auf die Begründung, Durchführung oder Beendigung von Beschäftigungsverhältnissen sowie die Einwilligung von Beschäftigten. Ferner können Landesdatenschutzgesetze der einzelnen Bundesländer zur Anwendung gelangen.\nSicherheitsmaßnahmen # Wir treffen nach Maßgabe der gesetzlichen Vorgaben unter Berücksichtigung des Stands der Technik, der Implementierungskosten und der Art, des Umfangs, der Umstände und der Zwecke der Verarbeitung sowie der unterschiedlichen Eintrittswahrscheinlichkeiten und des Ausmaßes der Bedrohung der Rechte und Freiheiten natürlicher Personen geeignete technische und organisatorische Maßnahmen, um ein dem Risiko angemessenes Schutzniveau zu gewährleisten.\nZu den Maßnahmen gehören insbesondere die Sicherung der Vertraulichkeit, Integrität und Verfügbarkeit von Daten durch Kontrolle des physischen und elektronischen Zugangs zu den Daten als auch des sie betreffenden Zugriffs, der Eingabe, der Weitergabe, der Sicherung der Verfügbarkeit und ihrer Trennung. Des Weiteren haben wir Verfahren eingerichtet, die eine Wahrnehmung von Betroffenenrechten, die Löschung von Daten und Reaktionen auf die Gefährdung der Daten gewährleisten. Ferner berücksichtigen wir den Schutz personenbezogener Daten bereits bei der Entwicklung bzw. Auswahl von Hardware, Software sowie Verfahren entsprechend dem Prinzip des Datenschutzes, durch Technikgestaltung und durch datenschutzfreundliche Voreinstellungen.\nSSL-Verschlüsselung (https): Um Ihre via unserem Online-Angebot übermittelten Daten zu schützen, nutzen wir eine SSL-Verschlüsselung. Sie erkennen derart verschlüsselte Verbindungen an dem Präfix https:// in der Adresszeile Ihres Browsers.\nLöschung von Daten # Die von uns verarbeiteten Daten werden nach Maßgabe der gesetzlichen Vorgaben gelöscht, sobald deren zur Verarbeitung erlaubten Einwilligungen widerrufen werden oder sonstige Erlaubnisse entfallen (z.B. wenn der Zweck der Verarbeitung dieser Daten entfallen ist oder sie für den Zweck nicht erforderlich sind).\nSofern die Daten nicht gelöscht werden, weil sie für andere und gesetzlich zulässige Zwecke erforderlich sind, wird deren Verarbeitung auf diese Zwecke beschränkt. D.h., die Daten werden gesperrt und nicht für andere Zwecke verarbeitet. Das gilt z.B. für Daten, die aus handels- oder steuerrechtlichen Gründen aufbewahrt werden müssen oder deren Speicherung zur Geltendmachung, Ausübung oder Verteidigung von Rechtsansprüchen oder zum Schutz der Rechte einer anderen natürlichen oder juristischen Person erforderlich ist.\nUnsere Datenschutzhinweise können ferner weitere Angaben zu der Aufbewahrung und Löschung von Daten beinhalten, die für die jeweiligen Verarbeitungen vorrangig gelten.\nÄnderung und Aktualisierung der Datenschutzerklärung # Wir bitten Sie, sich regelmäßig über den Inhalt unserer Datenschutzerklärung zu informieren. Wir passen die Datenschutzerklärung an, sobald die Änderungen der von uns durchgeführten Datenverarbeitungen dies erforderlich machen. Wir informieren Sie, sobald durch die Änderungen eine Mitwirkungshandlung Ihrerseits (z.B. Einwilligung) oder eine sonstige individuelle Benachrichtigung erforderlich wird.\nSofern wir in dieser Datenschutzerklärung Adressen und Kontaktinformationen von Unternehmen und Organisationen angeben, bitten wir zu beachten, dass die Adressen sich über die Zeit ändern können und bitten die Angaben vor Kontaktaufnahme zu prüfen.\nRechte der betroffenen Personen # Ihnen stehen als Betroffene nach der DSGVO verschiedene Rechte zu, die sich insbesondere aus Art. 15 bis 21 DSGVO ergeben:\nWiderspruchsrecht: Sie haben das Recht, aus Gründen, die sich aus Ihrer besonderen Situation ergeben, jederzeit gegen die Verarbeitung der Sie betreffenden personenbezogenen Daten, die aufgrund von Art. 6 Abs. 1 lit. e oder f DSGVO erfolgt, Widerspruch einzulegen; dies gilt auch für ein auf diese Bestimmungen gestütztes Profiling. Werden die Sie betreffenden personenbezogenen Daten verarbeitet, um Direktwerbung zu betreiben, haben Sie das Recht, jederzeit Widerspruch gegen die Verarbeitung der Sie betreffenden personenbezogenen Daten zum Zwecke derartiger Werbung einzulegen; dies gilt auch für das Profiling, soweit es mit solcher Direktwerbung in Verbindung steht. Widerrufsrecht bei Einwilligungen: Sie haben das Recht, erteilte Einwilligungen jederzeit zu widerrufen. Auskunftsrecht: Sie haben das Recht, eine Bestätigung darüber zu verlangen, ob betreffende Daten verarbeitet werden und auf Auskunft über diese Daten sowie auf weitere Informationen und Kopie der Daten entsprechend den gesetzlichen Vorgaben. Recht auf Berichtigung: Sie haben entsprechend den gesetzlichen Vorgaben das Recht, die Vervollständigung der Sie betreffenden Daten oder die Berichtigung der Sie betreffenden unrichtigen Daten zu verlangen. Recht auf Löschung und Einschränkung der Verarbeitung: Sie haben nach Maßgabe der gesetzlichen Vorgaben das Recht, zu verlangen, dass Sie betreffende Daten unverzüglich gelöscht werden, bzw. alternativ nach Maßgabe der gesetzlichen Vorgaben eine Einschränkung der Verarbeitung der Daten zu verlangen. Recht auf Datenübertragbarkeit: Sie haben das Recht, Sie betreffende Daten, die Sie uns bereitgestellt haben, nach Maßgabe der gesetzlichen Vorgaben in einem strukturierten, gängigen und maschinenlesbaren Format zu erhalten oder deren Übermittlung an einen anderen Verantwortlichen zu fordern. Beschwerde bei Aufsichtsbehörde: Sie haben unbeschadet eines anderweitigen verwaltungsrechtlichen oder gerichtlichen Rechtsbehelfs das Recht auf Beschwerde bei einer Aufsichtsbehörde, insbesondere in dem Mitgliedstaat ihres gewöhnlichen Aufenthaltsorts, ihres Arbeitsplatzes oder des Orts des mutmaßlichen Verstoßes, wenn Sie der Ansicht sind, dass die Verarbeitung der Sie betreffenden personenbezogenen Daten gegen die Vorgaben der DSGVO verstößt. Begriffsdefinitionen # In diesem Abschnitt erhalten Sie eine Übersicht über die in dieser Datenschutzerklärung verwendeten Begrifflichkeiten. Viele der Begriffe sind dem Gesetz entnommen und vor allem im Art. 4 DSGVO definiert. Die gesetzlichen Definitionen sind verbindlich. Die nachfolgenden Erläuterungen sollen dagegen vor allem dem Verständnis dienen. Die Begriffe sind alphabetisch sortiert.\nPersonenbezogene Daten: \u0026ldquo;Personenbezogene Daten“ sind alle Informationen, die sich auf eine identifizierte oder identifizierbare natürliche Person (im Folgenden \u0026ldquo;betroffene Person“) beziehen; als identifizierbar wird eine natürliche Person angesehen, die direkt oder indirekt, insbesondere mittels Zuordnung zu einer Kennung wie einem Namen, zu einer Kennnummer, zu Standortdaten, zu einer Online-Kennung (z.B. Cookie) oder zu einem oder mehreren besonderen Merkmalen identifiziert werden kann, die Ausdruck der physischen, physiologischen, genetischen, psychischen, wirtschaftlichen, kulturellen oder sozialen Identität dieser natürlichen Person sind. Verantwortlicher: Als \u0026ldquo;Verantwortlicher“ wird die natürliche oder juristische Person, Behörde, Einrichtung oder andere Stelle, die allein oder gemeinsam mit anderen über die Zwecke und Mittel der Verarbeitung von personenbezogenen Daten entscheidet, bezeichnet. Verarbeitung: \u0026ldquo;Verarbeitung\u0026rdquo; ist jeder mit oder ohne Hilfe automatisierter Verfahren ausgeführte Vorgang oder jede solche Vorgangsreihe im Zusammenhang mit personenbezogenen Daten. Der Begriff reicht weit und umfasst praktisch jeden Umgang mit Daten, sei es das Erheben, das Auswerten, das Speichern, das Übermitteln oder das Löschen. ","date":"21 Juli 2023","externalUrl":null,"permalink":"/datenschutz/","section":"Eine (scheinbar) leere Homepage","summary":"\u003ch2 class=\"relative group\"\u003eEinleitung\n    \u003cdiv id=\"einleitung\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#einleitung\" aria-label=\"Anker\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cp\u003eMit der folgenden Datenschutzerklärung möchten wir Sie darüber aufklären, welche\nArten Ihrer personenbezogenen Daten (nachfolgend auch kurz als \u0026ldquo;Daten“\nbezeichnet) wir zu welchen Zwecken und in welchem Umfang verarbeiten. Die\nDatenschutzerklärung gilt für alle von uns durchgeführten Verarbeitungen\npersonenbezogener Daten, sowohl im Rahmen der Erbringung unserer Leistungen als\nauch insbesondere auf unseren Webseiten, in mobilen Applikationen sowie\ninnerhalb externer Onlinepräsenzen, wie z.B. unserer Social-Media-Profile\n(nachfolgend zusammenfassend bezeichnet als \u0026ldquo;Onlineangebot“).\u003c/p\u003e","title":"Datenschutzerklärung","type":"page"},{"content":" Diensteanbieter # Fabrice Kirchner\nHaingasse 3\n35287 Amöneburg\nKontaktmöglichkeiten # E-Mail-Adresse:\nweb(at)zyria.de\nTelefon:\n0160 5513991\nMastodon:\nfabrice\nErstellt mit kostenlosem Datenschutz-Generator.de von Dr. Thomas Schwenke\n","date":"21 Juli 2023","externalUrl":null,"permalink":"/impressum/","section":"Eine (scheinbar) leere Homepage","summary":"\u003ch2 class=\"relative group\"\u003eDiensteanbieter\n    \u003cdiv id=\"diensteanbieter\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#diensteanbieter\" aria-label=\"Anker\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cp\u003eFabrice Kirchner\u003cbr\u003e\nHaingasse 3\u003cbr\u003e\n35287 Amöneburg\u003c/p\u003e\n\n\u003ch2 class=\"relative group\"\u003eKontaktmöglichkeiten\n    \u003cdiv id=\"kontaktmöglichkeiten\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#kontaktm%c3%b6glichkeiten\" aria-label=\"Anker\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cp\u003eE-Mail-Adresse:\u003cbr\u003e\nweb(at)zyria.de\u003c/p\u003e\n\u003cp\u003eTelefon:\u003cbr\u003e\n0160 5513991\u003c/p\u003e\n\u003cp\u003eMastodon:\u003cbr\u003e\n\u003ca\n  href=\"https://kirchner.social/@fabrice\"\n    target=\"_blank\"\n  \u003efabrice\u003c/a\u003e\u003c/p\u003e\n\u003cimg src=\"/qr_30e536884bbd0a86.png\" width=\"154\" height=\"154\" alt=\"QR Code der vCard von Fabrice Kirchner\"\u003e\n\u003cp\u003e\u003ca\n  href=\"https://datenschutz-generator.de/\"title=\"Rechtstext von Dr. Schwenke - für weitere Informationen bitte anklicken.\"\n    target=\"_blank\"\n  \u003eErstellt mit kostenlosem Datenschutz-Generator.de von Dr. Thomas Schwenke\u003c/a\u003e\u003c/p\u003e","title":"Impressum","type":"page"},{"content":" Executive Summary # Der Betrieb von Kubernetes-Clustern, die stateful Workloads (zustandsbehaftete Anwendungen) beherbergen, erfordert rigorose Protokolle für das Lebenszyklusmanagement. Im Gegensatz zu stateless Umgebungen, in denen Nodes als ephemere Ressourcen behandelt werden können, verlangen Cluster, die auf verteiltem Blockspeicher basieren - im Speziellen Longhorn -, eine präzise Orchestrierung während der Shutdown-Prozeduren, um Datenkorruption und Inkonsistenzen auf Volume-Ebene zu verhindern. Zusätzlich führt die Integration von GitOps-Prinzipien durch ArgoCD eine weitere Komplexitätsebene ein: Die Reconciliation-Loops (Abgleichschleifen) müssen kontrolliert ausgesetzt werden, um eine automatisierte, ungewollte Wiederherstellung von Diensten während kritischer Wartungsfenster zu unterbinden. Dieser Artikel liefert eine technische Analyse und einen prozeduralen Leitfaden für das geordnete Herunterfahren und Wiederanfahren eines Kubernetes-Clusters, der auf dem Stack k3s, Longhorn und ArgoCD basiert.\n1. Einleitung: Die Notwendigkeit deterministischer Shutdown-Prozeduren # In der modernen Container-Orchestrierung wird oft angenommen, dass Systeme resilient gegen plötzliche Ausfälle sind. Während Kubernetes hervorragende Mechanismen zur Selbstheilung bietet, gilt dies primär für den Ausfall einzelner Komponenten in einem hochverfügbaren Verbund. Der komplette Shutdown eines Clusters - etwa für Stromwartungen, Rechenzentrumsumzüge oder Hardware-Upgrades - stellt jedoch ein fundamental anderes Szenario dar. Hierbei werden alle Redundanzebenen gleichzeitig entfernt.\n1.1 Das Problem der Persistenz in verteilten Systemen # Bei der Verwendung von Local-Storage oder verteilten Speichersystemen wie Longhorn liegt die Datenhoheit nicht bei einem externen SAN, sondern direkt auf den Compute-Nodes. Ein unkontrolliertes Ausschalten (\u0026ldquo;Hard Power-off\u0026rdquo;) eines Nodes, während ein Longhorn-Volume noch als Attached markiert ist, führt unweigerlich zu \u0026ldquo;Stale Handles\u0026rdquo; in der Metadaten-Datenbank (etcd). Das System glaubt beim Neustart, das Volume sei noch auf dem alten Node gemountet, und verhindert aus Sicherheitsgründen (RWO - ReadWriteOnce Schutz) das erneute Mounten. Dies führt zu langen Ausfallzeiten, die manuelle Eingriffe in die Custom Resource Definitions (CRDs) erfordern.\n1.2 Der GitOps-Konflikt # ArgoCD fungiert als autonomer Agent, der den Cluster-Zustand permanent gegen ein Git-Repository prüft. In einem Wartungsszenario, bei dem ein Administrator Workloads herunterfährt (skaliert auf 0), interpretiert ArgoCD dies als \u0026ldquo;Configuration Drift\u0026rdquo; - eine Abweichung vom Soll-Zustand. Ist \u0026ldquo;Self-Heal\u0026rdquo; aktiviert, wird ArgoCD sofort versuchen, die Workloads wieder zu starten. Dies erzeugt eine Race Condition zwischen dem Shutdown-Skript und dem GitOps-Controller.\n2. Architektur-Analyse und Abhängigkeitsgraph # Um ein robustes Protokoll zu entwickeln, muss zunächst das Zusammenspiel der drei Kernkomponenten verstanden werden: Der Orchestrator (k3s), die Storage-Engine (Longhorn) und der Continuous-Delivery-Controller (ArgoCD).\n2.1 Die k3s Control Plane und Agent Topologie # k3s operiert als konsolidierte Kubernetes-Distribution. In einer typischen Deployment-Struktur ist die Service-Architektur in den k3s-server (Control Plane und API Server) und den k3s-agent (Worker Node) unterteilt.\nProzess-Lebenszyklus: Der Shutdown ist kritisch, da das Kubelet für die geordnete Beendigung von Pods verantwortlich ist. Sendet das Betriebssystem ein SIGKILL an den k3s-Prozess, bevor das Kubelet Zeit hatte, Container-Pre-Stop-Hooks auszuführen, werden Datenbank-Buffer nicht auf die Disk geflusht. Systemd Integration: Ein systemctl stop k3s triggert zwar das Beenden der Kubernetes-Dienste, garantiert aber nicht, dass alle von containerd verwalteten Container sauber beendet wurden. 2.2 Longhorn Distributed Storage Mechanik # Longhorn operiert als Microservices-basiertes verteiltes Blockspeichersystem.\nRisiko des unsauberen Shutdowns: Wird ein Node ausgeschaltet, während ein Longhorn-Volume noch verbunden (attached) ist, hat die Engine keine Zeit, die Verbindung sauber zu schließen. In der etcd-Datenbank verbleibt das Volume im Status Attached oder Faulted. RWO Restriktion: Die meisten Longhorn-Volumes sind ReadWriteOnce (RWO). Wenn der Cluster nicht sauber heruntergefahren wird, bleibt der Lock auf dem Volume bestehen. 2.3 ArgoCD Reconciliation Loops # ArgoCD erzwingt den im Git definierten Zustand.\nKonfliktpotenzial: Wenn ein Administrator manuell Deployments herunterskaliert, um Longhorn-Volumes freizugeben, erkennt ArgoCD dies als \u0026ldquo;Out of Sync\u0026rdquo;. Lösung: Der erste Schritt jedes Wartungsprotokolls muss die globale Suspension des Reconciliation-Loops sein. 3. Strategische Planung des Shutdown-Protokolls # Das Shutdown-Prozedere wird durch eine strikte Abhängigkeitskette definiert.\nPhase 1: Globale GitOps-Suspension # Wir nutzen das Skalieren des argocd-application-controller StatefulSets auf Null. Dies \u0026ldquo;friert\u0026rdquo; den Cluster-Zustand effektiv ein und garantiert, dass keine Hintergrundprozesse versuchen, Ressourcen zu modifizieren.\nPhase 2: Applikations-Dekommissionierung (Workload Draining) # Ist ArgoCD pausiert, müssen zustandsbehaftete Workloads auf Null skaliert werden. Dies fungiert als Trigger für das Container Storage Interface (CSI), um ControllerUnpublishVolume-Aufrufe an Longhorn zu senden.\nIdentifikation: Wir stoppen alle Deployments und StatefulSets in User-Namespaces. Verifikation: Ein Pod im Status Terminating kann immer noch einen Lock auf ein Longhorn-Volume halten. Phase 3: Verifikation der Speicher-Detachment (Abtrennung) # Dies ist der wichtigste Sicherheitscheck. Die Longhorn API muss abgefragt werden. Wenn irgendein Volume status.state: attached meldet, muss der Node-Shutdown abgebrochen werden.\nWichtig: Longhorn-Systemkomponenten (Manager, Driver) müssen weiterlaufen, bis alle User-Volumes abgetrennt sind. Phase 4: Infrastruktur-Shutdown # Stop Agents: Verhindert neues Scheduling. Stop Server: Terminiert die Control Plane. OS Shutdown: shutdown -h now. 4. Automatisierung via Ansible # Die manuelle Ausführung ist fehleranfällig. Nachfolgend die Ansible-Implementierung.\nVoraussetzungen # Ansible Core + kubernetes.core Collection. Python-Libraries: kubernetes, jsonpatch, PyYAML. Gültige kubeconfig. Das Shutdown-Playbook (shutdown_cluster.yml) # 1--- 2- name: Graceful Cluster Shutdown Protocol (Geordnetes Herunterfahren) 3 hosts: localhost 4 connection: local 5 gather_facts: no 6 vars: 7 # Pfad zur Kubeconfig - Anpassen falls notwendig 8 kubeconfig_path: \u0026#34;~/.kube/config\u0026#34; 9 longhorn_ns: \u0026#34;longhorn-system\u0026#34; 10 argocd_ns: \u0026#34;argocd\u0026#34; 11 detachment_timeout: 300 12 13 tasks: 14 # --------------------------------------------------------- 15 # Phase 1: ArgoCD Reconciliation Suspendieren 16 # --------------------------------------------------------- 17 - name: Prüfe Status des ArgoCD Application Controllers 18 kubernetes.core.k8s_info: 19 kubeconfig: \u0026#34;{{ kubeconfig_path }}\u0026#34; 20 kind: StatefulSet 21 namespace: \u0026#34;{{ argocd_ns }}\u0026#34; 22 name: argocd-application-controller 23 register: argocd_controller_state 24 25 - name: Skaliere ArgoCD Application Controller auf 0 (Pause Reconciliation) 26 kubernetes.core.k8s_scale: 27 kubeconfig: \u0026#34;{{ kubeconfig_path }}\u0026#34; 28 api_version: apps/v1 29 kind: StatefulSet 30 name: argocd-application-controller 31 namespace: \u0026#34;{{ argocd_ns }}\u0026#34; 32 replicas: 0 33 wait: yes 34 when: argocd_controller_state.resources | length \u0026gt; 0 35 tags: [argocd] 36 37 # --------------------------------------------------------- 38 # Phase 2: Skalieren der Stateful Workloads 39 # --------------------------------------------------------- 40 - name: Hole alle Deployments im Cluster 41 kubernetes.core.k8s_info: 42 kubeconfig: \u0026#34;{{ kubeconfig_path }}\u0026#34; 43 kind: Deployment 44 register: all_deployments 45 46 - name: Hole alle StatefulSets im Cluster 47 kubernetes.core.k8s_info: 48 kubeconfig: \u0026#34;{{ kubeconfig_path }}\u0026#34; 49 kind: StatefulSet 50 register: all_statefulsets 51 52 - name: Skaliere alle User-Deployments auf 0 herunter 53 kubernetes.core.k8s_scale: 54 kubeconfig: \u0026#34;{{ kubeconfig_path }}\u0026#34; 55 api_version: apps/v1 56 kind: Deployment 57 name: \u0026#34;{{ item.metadata.name }}\u0026#34; 58 namespace: \u0026#34;{{ item.metadata.namespace }}\u0026#34; 59 replicas: 0 60 wait: yes 61 loop: \u0026#34;{{ all_deployments.resources }}\u0026#34; 62 loop_control: 63 label: \u0026#34;{{ item.metadata.namespace }}/{{ item.metadata.name }}\u0026#34; 64 when: 65 - item.metadata.namespace not in [\u0026#39;kube-system\u0026#39;, \u0026#39;longhorn-system\u0026#39;, \u0026#39;argocd\u0026#39;, \u0026#39;monitoring\u0026#39;] 66 - item.spec.replicas \u0026gt; 0 67 ignore_errors: yes 68 tags: [workloads] 69 70 - name: Skaliere alle User-StatefulSets auf 0 herunter 71 kubernetes.core.k8s_scale: 72 kubeconfig: \u0026#34;{{ kubeconfig_path }}\u0026#34; 73 api_version: apps/v1 74 kind: StatefulSet 75 name: \u0026#34;{{ item.metadata.name }}\u0026#34; 76 namespace: \u0026#34;{{ item.metadata.namespace }}\u0026#34; 77 replicas: 0 78 wait: yes 79 loop: \u0026#34;{{ all_statefulsets.resources }}\u0026#34; 80 loop_control: 81 label: \u0026#34;{{ item.metadata.namespace }}/{{ item.metadata.name }}\u0026#34; 82 when: 83 - item.metadata.namespace not in [\u0026#39;kube-system\u0026#39;, \u0026#39;longhorn-system\u0026#39;, \u0026#39;argocd\u0026#39;, \u0026#39;monitoring\u0026#39;] 84 - item.spec.replicas \u0026gt; 0 85 tags: [workloads] 86 87 # --------------------------------------------------------- 88 # Phase 3: Verifikation des Longhorn Volume Detachments 89 # --------------------------------------------------------- 90 - name: Warte darauf, dass alle Longhorn Volumes den Status \u0026#39;detached\u0026#39; haben 91 kubernetes.core.k8s_info: 92 kubeconfig: \u0026#34;{{ kubeconfig_path }}\u0026#34; 93 api_version: longhorn.io/v1beta2 94 kind: Volume 95 namespace: \u0026#34;{{ longhorn_ns }}\u0026#34; 96 register: longhorn_volumes 97 until: \u0026#34;longhorn_volumes.resources | json_query(\u0026#39;[?status.state!=\\\\\u0026#39;detached\\\\\u0026#39;]\u0026#39;) | length == 0\u0026#34; 98 retries: \u0026#34;{{ (detachment_timeout / 10) | int }}\u0026#34; 99 delay: 10 100 msg: \u0026#34;Warte auf Detachment. Prüfen Sie auf hängende Pods, falls dies fehlschlägt.\u0026#34; 101 tags: [storage] 102 103# --------------------------------------------------------- 104# Phase 4: Stop k3s Services auf den physischen Nodes 105# --------------------------------------------------------- 106- name: Stop k3s Services und Shutdown Nodes 107 hosts: all 108 become: yes 109 gather_facts: no 110 tags: [infrastructure] 111 tasks: 112 - name: Stoppe k3s-agent (Worker Nodes) 113 ansible.builtin.systemd: 114 name: k3s-agent 115 state: stopped 116 when: \u0026#34;\u0026#39;k3s_agent\u0026#39; in group_names\u0026#34; 117 ignore_errors: yes 118 119 - name: Stoppe k3s (Server/Master Nodes) 120 ansible.builtin.systemd: 121 name: k3s 122 state: stopped 123 when: \u0026#34;\u0026#39;k3s_server\u0026#39; in group_names\u0026#34; 124 125 - name: Shutdown Operating System 126 community.general.shutdown: 127 delay: 1 128 msg: \u0026#34;Automatischer Shutdown durch Ansible Playbook\u0026#34; 5. Das Recovery-Protokoll (Wiederanfahren) # Das Starten des Clusters erfordert die Umkehrung der Reihenfolge. Das Speichersystem muss gesund sein, bevor die Anwendungen hochskaliert werden.\nDas Startup-Playbook (startup_cluster.yml) # 1--- 2- name: Graceful Cluster Startup Protocol (Startvorgang) 3 hosts: all 4 become: yes 5 tags: [infrastructure] 6 tasks: 7 # --------------------------------------------------------- 8 # Phase 1: Nodes Booten und k3s Starten 9 # --------------------------------------------------------- 10 # Voraussetzung: Nodes sind via Wake-on-LAN oder physisch gestartet 11 12 - name: Starte k3s (Server/Master) 13 ansible.builtin.systemd: 14 name: k3s 15 state: started 16 enabled: yes 17 when: \u0026#34;\u0026#39;k3s_server\u0026#39; in group_names\u0026#34; 18 19 - name: Starte k3s-agent (Worker) 20 ansible.builtin.systemd: 21 name: k3s-agent 22 state: started 23 enabled: yes 24 when: \u0026#34;\u0026#39;k3s_agent\u0026#39; in group_names\u0026#34; 25 26- name: Orchestrate Workload Recovery 27 hosts: localhost 28 connection: local 29 gather_facts: no 30 vars: 31 kubeconfig_path: \u0026#34;~/.kube/config\u0026#34; 32 longhorn_ns: \u0026#34;longhorn-system\u0026#34; 33 argocd_ns: \u0026#34;argocd\u0026#34; 34 35 tasks: 36 # --------------------------------------------------------- 37 # Phase 2: Verifikation der Storage-Gesundheit 38 # --------------------------------------------------------- 39 - name: Warte bis Longhorn Nodes \u0026#39;Ready\u0026#39; sind 40 kubernetes.core.k8s_info: 41 kubeconfig: \u0026#34;{{ kubeconfig_path }}\u0026#34; 42 api_version: longhorn.io/v1beta2 43 kind: Node 44 namespace: \u0026#34;{{ longhorn_ns }}\u0026#34; 45 register: lh_nodes 46 until: lh_nodes.resources | length \u0026gt; 0 47 retries: 30 48 delay: 10 49 tags: [storage] 50 51 - name: Warte auf Longhorn System Pods 52 kubernetes.core.k8s_info: 53 kubeconfig: \u0026#34;{{ kubeconfig_path }}\u0026#34; 54 kind: Pod 55 namespace: \u0026#34;{{ longhorn_ns }}\u0026#34; 56 field_selectors: 57 - status.phase=Running 58 register: lh_pods 59 until: lh_pods.resources | length \u0026gt;= 3 60 retries: 30 61 delay: 10 62 63 # --------------------------------------------------------- 64 # Phase 3: ArgoCD Reaktivierung 65 # --------------------------------------------------------- 66 # Durch das Hochskalieren erkennt ArgoCD den Drift (0 vs 1 im Git) 67 # und stellt die Applikationen automatisch wieder her. 68 69 - name: Skaliere ArgoCD Application Controller auf 1 70 kubernetes.core.k8s_scale: 71 kubeconfig: \u0026#34;{{ kubeconfig_path }}\u0026#34; 72 api_version: apps/v1 73 kind: StatefulSet 74 name: argocd-application-controller 75 namespace: \u0026#34;{{ argocd_ns }}\u0026#34; 76 replicas: 1 77 wait: yes 78 tags: [argocd] 79 80 - name: Warte auf ArgoCD Reconciliation 81 ansible.builtin.debug: 82 msg: \u0026#34;ArgoCD Controller neugestartet. Auto-Sync wird nun initiiert.\u0026#34; 6. Deep Dive: Fehleranalyse und Edge Cases # Blockierende Finalizers auf PVCs # Ein häufiges Problem beim Shutdown ist, dass Ressourcen im Status Terminating hängen bleiben. Ein Pod im Status Terminating kann immer noch einen Lock auf ein Longhorn-Volume halten. Das Ansible Playbook verlässt sich darauf, dass k8s_scale den Controller zwingt, die Pods zu entfernen. Reagiert das Kubelet auf einem Node nicht, bleibt das Pod-Objekt bestehen.\nDie \u0026ldquo;ArgoCD Fighting Back\u0026rdquo; Race Condition # Wird der argocd-application-controller nicht auf 0 skaliert bevor die Anwendungen heruntergefahren werden, entsteht eine Race Condition:\nAnsible skaliert deployment/my-app auf 0. ArgoCD erkennt Drift (Live: 0, Git: 1) und patched zurück auf 1. Longhorn versucht, das Volume neu zu attachen, während Ansible den Node herunterfährt. Resultat: Volume-Korruption. 7. Fazit # Das sichere Herunterfahren eines k3s-Clusters mit persistentem Speicher ist fundamental eine Übung im Abhängigkeitsmanagement. Die Hierarchie lautet:\nGitOps Layer (ArgoCD): Muss zuerst verstummen. Application Layer: Muss terminiert werden (Volume Release). Storage Layer (Longhorn): Muss Detachment bestätigen. Infrastruktur Layer (k3s/OS): Darf erst am Ende ausgeschaltet werden. Durch die Kapselung dieser Logik in Ansible Playbooks wird das Risiko von \u0026ldquo;Split-Brain\u0026rdquo;-Szenarien in der Storage-Engine signifikant reduziert.\n","date":"27 Oktober 2024","externalUrl":null,"permalink":"/posts/homelab/k3s-prod/ansible/shutdown-startup/","section":"Beiträge","summary":"Ein technischer Leitfaden für das geordnete Herunterfahren und Wiederanfahren eines Kubernetes-Clusters (k3s/Longhorn/ArgoCD) zur Vermeidung von Datenkorruption.","title":"Operational Lifecycle Management: Orchestrierter Shutdown für Kubernetes","type":"posts"},{"content":" Servername Hostname Funktion System Notiz dev dev.zyria.de Docker Host Debian Stable IT interne Dienste addc addc.zyria.de Active Directory, Fileserver Debian Stable ","date":"1 Juli 2025","externalUrl":null,"permalink":"/posts/homelab/vms/","section":"Beiträge","summary":"\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eServername\u003c/th\u003e\n          \u003cth\u003eHostname\u003c/th\u003e\n          \u003cth\u003eFunktion\u003c/th\u003e\n          \u003cth\u003eSystem\u003c/th\u003e\n          \u003cth\u003eNotiz\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ca\n  href=\"https://zyria.de/posts/homelab/vms/dev/\"\u003edev\u003c/a\u003e\u003c/td\u003e\n          \u003ctd\u003edev.zyria.de\u003c/td\u003e\n          \u003ctd\u003eDocker Host\u003c/td\u003e\n          \u003ctd\u003eDebian Stable\u003c/td\u003e\n          \u003ctd\u003eIT interne Dienste\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ca\n  href=\"https://zyria.de/posts/homelab/vms/addc/\"\u003eaddc\u003c/a\u003e\u003c/td\u003e\n          \u003ctd\u003eaddc.zyria.de\u003c/td\u003e\n          \u003ctd\u003eActive Directory, Fileserver\u003c/td\u003e\n          \u003ctd\u003eDebian Stable\u003c/td\u003e\n          \u003ctd\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e","title":"Virtuelle Maschinen","type":"posts"},{"content":" Ansible Konfiguration für k3s-prod # Dieses Verzeichnis beherbergt alle notwendigen Ansible-Ressourcen, um den k3s Kubernetes Cluster zu initialisieren, zu konfigurieren und grundlegende Verwaltungsaufgaben durchzuführen.\nHerkunft und Anpassung # Die Basis dieser Ansible-Konfiguration stammt ursprünglich aus dem offiziellen k3s-io/k3s-ansible Projekt. Sie wurde jedoch an die spezifischen Anforderungen und die Infrastruktur von k3s-prod angepasst und erweitert.\nAufgabe und Rolle # Ansible spielt in diesem Setup eine entscheidende Rolle bei:\nInitialer Cluster-Setup: Automatisierte Installation von k3s auf den vorgesehenen Nodes und deren Verbund zu einem funktionsfähigen Kubernetes-Cluster. Node-Management: Vorbereitung der Nodes für die Kubernetes-Workloads (z.B. Installation von Abhängigkeiten, Konfiguration von Dateisystemen). Konfigurationsmanagement: Anwendung von Host-spezifischen Konfigurationen, die für den Cluster-Betrieb notwendig sind. Verzeichnisstruktur # k3s-prod-inv.yaml: Das Haupt-Inventarfile für den k3s-prod Cluster. Hier sind die Hosts, deren Rollen (Master/Worker) und spezifische Variablen definiert. README.md: Diese Datei, die eine Übersicht über die Ansible-Konfiguration bietet. Nutzung # Um die Ansible-Playbooks auszuführen, navigieren Sie in dieses Verzeichnis und verwenden Sie den ansible-playbook-Befehl.\nBeispiel: Cluster initialisieren/aktualisieren # Um den k3s-Cluster basierend auf dem k3s-prod-inv.yaml Inventar zu installieren oder zu aktualisieren, verwenden Sie:\n1ansible-playbook k3s.orchestration.site -i ansible/k3s-prod-inv.yaml -u root Stellen Sie sicher, dass Sie die notwendigen SSH-Zugangsdaten für die Ziel-Nodes konfiguriert haben (z.B. über ssh-agent).\nAbhängigkeiten # Um diese Ansible-Playbooks ausführen zu können, benötigen Sie:\nAnsible: Eine installierte Ansible-Version (empfohlen wird die neueste stabile Version). Python: Eine kompatible Python-Installation auf dem System, von dem aus Ansible ausgeführt wird. Ansible Collections: Spezifische Ansible Collections, die in den Playbooks verwendet werden (diese werden in der Regel automatisch installiert oder können über requirements.yml verwaltet werden). Weiterführende Dokumentation # Für detaillierte Informationen zur k3s-Installation und den spezifischen Playbooks, konsultieren Sie bitte die offizielle k3s-ansible Dokumentation.\n","date":"12 Januar 2026","externalUrl":null,"permalink":"/posts/homelab/k3s-prod/ansible/readme/","section":"Beiträge","summary":"Automatisierung der k3s Cluster-Bereitstellung und -Verwaltung mit Ansible.","title":"Ansible Konfiguration für k3s-prod","type":"posts"},{"content":" Ansible Semaphore # Ansible Semaphore ist eine quelloffene, webbasierte Benutzeroberfläche zur Verwaltung und Ausführung von Ansible Playbooks. Sie bietet eine zentralisierte Plattform für Teams zur Zusammenarbeit bei Automatisierungsaufgaben.\nStatus # Deployment # Quelle # Diese Anwendung wird unter Verwendung des ansible-semaphore Helm-Charts aus dem cloudhippie Repository bereitgestellt.\nDokumentation # Die offizielle Dokumentation von Ansible Semaphore ist unter docs.ansible-semaphore.com zu finden.\nAnsible Semaphore GitHub Repository\nFunktion # Dieser Dienst bietet eine zentrale Schnittstelle für die Verwaltung und Ausführung von Ansible-Automatisierungen. Er ermöglicht die Zusammenarbeit von Teams, die Planung von Aufgaben und die Überwachung von Ausführungen.\nLokale Anpassungen # Die Konfiguration erfolgt über die values.yaml-Datei.\nWichtige Einstellungen # Datenbank: Es wird eine externe MariaDB-Datenbank verwendet. Ingress: Der Zugriff auf die Weboberfläche wird über einen Ingress mit einem Hostnamen ermöglicht und durch Traefik verwaltet. Das TLS-Zertifikat wird über cert-manager bereitgestellt. Speicher: Ein PersistentVolume wird für die Speicherung der Konfiguration und der Playbooks verwendet. Authentifizierung: Die Benutzerauthentifizierung erfolgt über LDAP und OIDC. Installation # Die Anwendung wird mittels Kustomize und Helm durch ArgoCD im Kubernetes-Cluster bereitgestellt. Die Konfiguration befindet sich im apps/ansible-semaphore-Verzeichnis. Eine manuelle Installation kann mit folgendem Befehl durchgeführt werden:\n1kubectl kustomize --enable-helm apps/ansible-semaphore | kubectl apply -n ansible-semaphore -f - Abhängigkeiten # Ein laufender Kubernetes-Cluster. Ein Ingress-Controller (z.B. Traefik) für den externen Zugriff. Eine Datenbank (z.B. MariaDB). Ein LDAP-Server für die Benutzerauthentifizierung (optional). Ein OIDC-Anbieter (z.B. Authentik) für die Authentifizierung (optional). Ein SMTP-Server für E-Mail-Benachrichtigungen (optional). Ein NFS-Server für persistente Speicherung. cert-manager: Wird für die automatische Bereitstellung von TLS-Zertifikaten benötigt. Longhorn/NFS: Ein Storage-Provider wird für die PersistentVolumes benötigt. ","date":"12 Januar 2026","externalUrl":null,"permalink":"/posts/homelab/k3s-prod/apps/ansible-semaphore/readme/","section":"Beiträge","summary":"Dieses Dokument beschreibt die Bereitstellung von Ansible Semaphore, einer webbasierten Benutzeroberfläche zur Verwaltung und Ausführung von Ansible Playbooks.","title":"Ansible Semaphore","type":"posts"},{"content":"","date":"12 Januar 2026","externalUrl":null,"permalink":"/tags/argocd/","section":"Tags","summary":"","title":"Argocd","type":"tags"},{"content":" ArgoCD # Argo CD ist ein deklaratives GitOps-Continuous-Delivery-Tool für Kubernetes. Es automatisiert die Bereitstellung der gewünschten Anwendungszustände in angegebenen Zielumgebungen.\nStatus # Deployment # Komponenten # Diese Argo CD-Bereitstellung umfasst die Kernkomponenten von Argo CD:\nArgo CD Server: Die zentrale Komponente, die die Web-UI, API und gRPC-Server bereitstellt. Application Controller: Überwacht laufend Anwendungen und vergleicht ihren aktuellen Zustand mit dem gewünschten Zustand im Git-Repository. Repo Server: Ein interner Dienst, der Git-Repositories verwaltet und Anwendungsmanifeste generiert. Dex: Ein Identitätsdienst, der die Authentifizierung über verschiedene Identitätsanbieter (z.B. Authentik) ermöglicht. Redis: Ein In-Memory-Datenspeicher, der von Argo CD für Caching verwendet wird. Konfiguration # Die Bereitstellung wird durch Kustomize verwaltet und nutzt das offizielle Argo CD Helm-Chart.\nHelm Chart: argo-cd Chart-Repository: https://argoproj.github.io/argo-helm Lokale Anpassungen # Die values.yaml-Datei konfiguriert die Argo CD-Instanz, einschließlich der Integration mit Authentik für die Authentifizierung. Die Bereitstellung wird durch Keel automatisch aktualisiert, wie durch die Annotationen in values.yaml definiert. Die Datei namespace.yaml stellt den argocd-Namespace sicher. Zusätzliche Repository-Definitionen sind in repos.yaml enthalten. TLS-Zertifikate werden über certs.yaml verwaltet. Wichtige Einstellungen # Ingress: Der Zugriff auf die ArgoCD-Weboberfläche wird über einen Ingress mit dem Hostnamen argocd.zyria.de ermöglicht und durch Traefik verwaltet. Authentifizierung: Die Benutzerauthentifizierung erfolgt über Authentik als OIDC-Provider. Dies ermöglicht Single Sign-On für den Zugriff auf ArgoCD. RBAC: Die Rollenbasierte Zugriffskontrolle ist so konfiguriert, dass Benutzer mit der Gruppe authentik Admins Administratorrechte in ArgoCD erhalten. Status Badge: Die Status-Badge-Funktion ist aktiviert, um den Synchronisationsstatus von Anwendungen extern anzuzeigen. Installation # Der Dienst wird als Helm-Chart bereitgestellt und durch die kustomization.yaml-Datei verwaltet.\n1kubectl kustomize --enable-helm apps/argocd | kubectl apply -n argocd -f - Quellen # Argo CD Dokumentation Argo CD GitHub Repository Argo Helm Charts Repository Abhängigkeiten # Ein laufender Kubernetes-Cluster. Ein Ingress-Controller (z.B. Traefik) für den externen Zugriff. Eine Zertifikatsmanagement-Lösung (z.B. cert-manager) zur Bereitstellung von TLS-Zertifikaten. Authentik für die Authentifizierung. ","date":"12 Januar 2026","externalUrl":null,"permalink":"/posts/homelab/k3s-prod/apps/argocd/readme/","section":"Beiträge","summary":"Dieses Dokument beschreibt die Bereitstellung von Argo CD, einem deklarativen GitOps-Continuous-Delivery-Tool für Kubernetes.","title":"ArgoCD","type":"posts"},{"content":" Authentik # Authentik ist ein quelloffener Identitätsanbieter (IdP), der sich auf Flexibilität und Vielseitigkeit konzentriert. Er kann als zentrales Authentifizierungs- und Autorisierungssystem für Ihre Anwendungen verwendet werden.\nStatus # Deployment # Komponenten # Diese Authentik-Bereitstellung umfasst die folgenden Komponenten:\nAuthentik Server: Die Kernanwendung, die die Identitäts- und Zugriffsverwaltung bereitstellt. PostgreSQL: Die relationale Datenbank, die von Authentik zur Speicherung seiner Daten verwendet wird. Redis: Ein In-Memory-Datenspeicher, der von Authentik für Caching verwendet wird. Quelle # Das Projekt basiert auf der offiziellen Authentik-Website. Das Helm-Chart stammt aus dem offiziellen Authentik-Chart-Repository.\nDokumentation # Offizielle Authentik-Website Authentik GitHub Repository Funktion # Dieser Dienst bietet eine umfassende Lösung für Single Sign-On (SSO), Multi-Faktor-Authentifizierung (MFA) und Benutzerverwaltung. Er ermöglicht die sichere Authentifizierung von Benutzern für verschiedene Anwendungen und Dienste.\nLokale Anpassungen # Die Konfiguration erfolgt über die values.yaml-Datei.\nDie Persistenz für PostgreSQL und Redis wird über vorhandene PVCs (authentik-db, authentik-redis) verwaltet.\nWichtige Einstellungen # Ingress: Der Zugriff auf die Authentik-Weboberfläche wird über einen Ingress mit einem Hostnamen ermöglicht und durch Traefik verwaltet. Datenbank: Authentik verwendet eine dedizierte PostgreSQL-Datenbank, die als Teil des Helm-Charts bereitgestellt wird. Die Daten werden persistent auf einem Volume gespeichert. Redis: Für Caching und Hintergrundaufgaben wird eine dedizierte Redis-Instanz verwendet, deren Daten auf einem Volume gespeichert werden. E-Mail: SMTP ist für den Versand von Benachrichtigungen und Passwort-Resets konfiguriert. Installation # Die Anwendung wird mittels Kustomize und Helm durch ArgoCD im Kubernetes-Cluster bereitgestellt. Die Konfiguration befindet sich im apps/authentik-Verzeichnis. Eine manuelle Installation kann mit folgendem Befehl durchgeführt werden:\n1kubectl kustomize --enable-helm apps/authentik | kubectl apply -n authentik -f - Abhängigkeiten # Ein laufender Kubernetes-Cluster. Ein Ingress-Controller (z.B. Traefik) für den externen Zugriff. Eine Zertifikatsmanagement-Lösung (z.B. cert-manager) zur Bereitstellung von TLS-Zertifikaten. Ein SMTP-Server für E-Mail-Benachrichtigungen. ","date":"12 Januar 2026","externalUrl":null,"permalink":"/posts/homelab/k3s-prod/apps/authentik/readme/","section":"Beiträge","summary":"Dieses Dokument beschreibt die Bereitstellung von Authentik, einem quelloffenen Identitätsanbieter (IdP).","title":"Authentik","type":"posts"},{"content":"","date":"12 Januar 2026","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":" cert-manager # Cert-Manager ist ein natives Kubernetes-Tool zur Automatisierung der Ausstellung und Verwaltung von TLS-Zertifikaten. Es kann Zertifikate von verschiedenen Ausstellern wie Let\u0026rsquo;s Encrypt beziehen und diese automatisch erneuern.\nDeployment # Quelle # Das Projekt basiert auf dem Cert-Manager GitHub Repository. Das Helm-Chart stammt aus dem Jetstack Helm Charts Repository.\nDokumentation # Cert-Manager Dokumentation Cert-Manager GitHub Repository Funktion # Dieser Dienst automatisiert den gesamten Lebenszyklus von TLS-Zertifikaten innerhalb des Kubernetes-Clusters. Er stellt sicher, dass Anwendungen immer über gültige und aktuelle Zertifikate verfügen, was die Sicherheit und Verfügbarkeit erhöht.\nLokale Anpassungen # Die Konfiguration erfolgt über die values.yaml-Datei sowie zusätzliche Manifeste wie clusterissuer.yaml.\nDie values.yaml-Datei konfiguriert globale Einstellungen für Cert-Manager, einschließlich der Leader-Election und der Standard-Issuer-Einstellungen für letsencrypt-production. Eine ClusterIssuer-Ressource (clusterissuer.yaml) ist für Let\u0026rsquo;s Encrypt konfiguriert. Zertifikate für spezifische Dienste werden über certs.yaml definiert. Ein Secret wird aus acmedns.json generiert, das für DNS01-Challenges verwendet wird. Wichtige Einstellungen # ClusterIssuer: Es ist ein ClusterIssuer namens letsencrypt-production konfiguriert, der für die Ausstellung von Zertifikaten für den gesamten Cluster zuständig ist. DNS01-Challenge: Zur Validierung der Domain-Inhaberschaft bei Let\u0026rsquo;s Encrypt wird die DNS01-Challenge-Methode verwendet. Dies geschieht über einen eigenen acme-dns-Server, dessen Zugangsdaten in einem Secret gespeichert sind. Dies ermöglicht die Ausstellung von Wildcard-Zertifikaten und erfordert keine öffentlichen Ingress-Endpunkte während des Validierungsprozesses. Ingress Shim: Das ingress-shim ist so konfiguriert, dass es automatisch Zertifikate für Ingress-Ressourcen anfordert, die entsprechend annotiert sind. Prometheus: Metriken von cert-manager werden für das Monitoring via Prometheus exportiert. Installation # Die Anwendung wird mittels Kustomize und Helm durch ArgoCD im Kubernetes-Cluster bereitgestellt. Die Konfiguration befindet sich im apps/cert-manager-Verzeichnis. Eine manuelle Installation kann mit folgendem Befehl durchgeführt werden:\n1kubectl kustomize --enable-helm apps/cert-manager | kubectl apply -n cert-manager -f - Abhängigkeiten # Ein laufender Kubernetes-Cluster. Ein DNS-Anbieter, der für DNS01-Challenges konfiguriert ist (falls verwendet). ","date":"12 Januar 2026","externalUrl":null,"permalink":"/posts/homelab/k3s-prod/apps/cert-manager/readme/","section":"Beiträge","summary":"Dieses Dokument beschreibt die Bereitstellung von Cert-Manager, einem nativen Kubernetes-Zertifikatsmanagement-Tool.","title":"Cert-Manager","type":"posts"},{"content":" CoreDNS # Deployment # Quelle # Das Projekt basiert auf dem CoreDNS GitHub Repository. Das Helm-Chart stammt aus dem CoreDNS Helm Charts Repository.\nDokumentation # CoreDNS Dokumentation CoreDNS GitHub Repository Funktion # CoreDNS ist ein fundamentaler Bestandteil des Kubernetes-Clusters und verantwortlich für die Namensauflösung (DNS). Es ermöglicht Pods und Services, sich gegenseitig über ihre Namen zu finden (z.B. my-service.my-namespace.svc.cluster.local). Es ist so konfiguriert, dass es Anfragen für interne Cluster-Domains beantwortet und alle anderen Anfragen an Upstream-DNS-Server weiterleitet.\nLokale Anpassungen # Die Konfiguration erfolgt über die values.yaml-Datei.\nWichtige Einstellungen # Replikas: Es werden drei Replikate von CoreDNS betrieben, um Hochverfügbarkeit zu gewährleisten. Dual-Stack: Der Service ist für den Dual-Stack-Betrieb (IPv4 und IPv6) konfiguriert. Plugins: Eine Reihe von Plugins ist aktiviert, darunter: kubernetes: Für die Auflösung von Kubernetes-Services und -Pods. forward: Leitet Anfragen, die nicht vom kubernetes-Plugin aufgelöst werden können, an die DNS-Server in /etc/resolv.conf weiter. cache: Cacht DNS-Antworten, um die Latenz zu verringern und die Last auf den Upstream-Servern zu reduzieren. prometheus: Stellt Metriken für das Monitoring bereit. Scheduling: CoreDNS-Pods werden bevorzugt auf den Control-Plane-Nodes ausgeführt und sind über topologySpreadConstraints auf verschiedene Nodes und Zonen verteilt, um die Ausfallsicherheit zu erhöhen. Installation # Die Anwendung wird mittels Kustomize und Helm durch ArgoCD im Kubernetes-Cluster bereitgestellt. Die Konfiguration befindet sich im apps/coredns-Verzeichnis. Eine manuelle Installation kann mit folgendem Befehl durchgeführt werden:\n1kubectl kustomize --enable-helm apps/coredns | kubectl apply -n kube-system -f - Abhängigkeiten # CoreDNS ist eine Kernkomponente von Kubernetes und hat keine externen Anwendungsabhängigkeiten, ist aber für die Funktion fast aller anderen Anwendungen im Cluster kritisch.\n","date":"12 Januar 2026","externalUrl":null,"permalink":"/posts/homelab/k3s-prod/apps/coredns/readme/","section":"Beiträge","summary":"CoreDNS dient als primärer DNS-Server für die Service Discovery innerhalb des Kubernetes-Clusters und löst sowohl interne als auch externe DNS-Anfragen auf.","title":"CoreDNS","type":"posts"},{"content":" CoreDNS # Deployment # Quelle # Das Projekt basiert auf dem CoreDNS GitHub Repository. Das Helm-Chart stammt aus dem CoreDNS Helm Charts Repository.\nDokumentation # CoreDNS Dokumentation CoreDNS GitHub Repository Funktion # CoreDNS ist ein fundamentaler Bestandteil des Kubernetes-Clusters und verantwortlich für die Namensauflösung (DNS). Es ermöglicht Pods und Services, sich gegenseitig über ihre Namen zu finden (z.B. my-service.my-namespace.svc.cluster.local). Es ist so konfiguriert, dass es Anfragen für interne Cluster-Domains beantwortet und alle anderen Anfragen an Upstream-DNS-Server weiterleitet.\nLokale Anpassungen # Die Konfiguration erfolgt über die values.yaml-Datei.\nWichtige Einstellungen # Replikas: Es werden drei Replikate von CoreDNS betrieben, um Hochverfügbarkeit zu gewährleisten. Dual-Stack: Der Service ist für den Dual-Stack-Betrieb (IPv4 und IPv6) konfiguriert. Plugins: Eine Reihe von Plugins ist aktiviert, darunter: kubernetes: Für die Auflösung von Kubernetes-Services und -Pods. forward: Leitet Anfragen, die nicht vom kubernetes-Plugin aufgelöst werden können, an die DNS-Server in /etc/resolv.conf weiter. cache: Cacht DNS-Antworten, um die Latenz zu verringern und die Last auf den Upstream-Servern zu reduzieren. prometheus: Stellt Metriken für das Monitoring bereit. Scheduling: CoreDNS-Pods werden bevorzugt auf den Control-Plane-Nodes ausgeführt und sind über topologySpreadConstraints auf verschiedene Nodes und Zonen verteilt, um die Ausfallsicherheit zu erhöhen. Installation # Die Anwendung wird mittels Kustomize und Helm durch ArgoCD im Kubernetes-Cluster bereitgestellt. Die Konfiguration befindet sich im apps/coredns-Verzeichnis. Eine manuelle Installation kann mit folgendem Befehl durchgeführt werden:\n1kubectl kustomize --enable-helm apps/coredns | kubectl apply -n kube-system -f - Abhängigkeiten # CoreDNS ist eine Kernkomponente von Kubernetes und hat keine externen Anwendungsabhängigkeiten, ist aber für die Funktion fast aller anderen Anwendungen im Cluster kritisch.\n","date":"12 Januar 2026","externalUrl":null,"permalink":"/posts/homelab/k3s-prod/apps/nameserver.unbound/readme/","section":"Beiträge","summary":"CoreDNS dient als primärer DNS-Server für die Service Discovery innerhalb des Kubernetes-Clusters und löst sowohl interne als auch externe DNS-Anfragen auf.","title":"CoreDNS","type":"posts"},{"content":" CoreDNS # Deployment # Quelle # Das Projekt basiert auf dem CoreDNS GitHub Repository. Das Helm-Chart stammt aus dem CoreDNS Helm Charts Repository.\nDokumentation # CoreDNS Dokumentation CoreDNS GitHub Repository Funktion # CoreDNS ist ein fundamentaler Bestandteil des Kubernetes-Clusters und verantwortlich für die Namensauflösung (DNS). Es ermöglicht Pods und Services, sich gegenseitig über ihre Namen zu finden (z.B. my-service.my-namespace.svc.cluster.local). Es ist so konfiguriert, dass es Anfragen für interne Cluster-Domains beantwortet und alle anderen Anfragen an Upstream-DNS-Server weiterleitet.\nLokale Anpassungen # Die Konfiguration erfolgt über die values.yaml-Datei.\nWichtige Einstellungen # Replikas: Es werden drei Replikate von CoreDNS betrieben, um Hochverfügbarkeit zu gewährleisten. Dual-Stack: Der Service ist für den Dual-Stack-Betrieb (IPv4 und IPv6) konfiguriert. Plugins: Eine Reihe von Plugins ist aktiviert, darunter: kubernetes: Für die Auflösung von Kubernetes-Services und -Pods. forward: Leitet Anfragen, die nicht vom kubernetes-Plugin aufgelöst werden können, an die DNS-Server in /etc/resolv.conf weiter. cache: Cacht DNS-Antworten, um die Latenz zu verringern und die Last auf den Upstream-Servern zu reduzieren. prometheus: Stellt Metriken für das Monitoring bereit. Scheduling: CoreDNS-Pods werden bevorzugt auf den Control-Plane-Nodes ausgeführt und sind über topologySpreadConstraints auf verschiedene Nodes und Zonen verteilt, um die Ausfallsicherheit zu erhöhen. Installation # Die Anwendung wird mittels Kustomize und Helm durch ArgoCD im Kubernetes-Cluster bereitgestellt. Die Konfiguration befindet sich im apps/coredns-Verzeichnis. Eine manuelle Installation kann mit folgendem Befehl durchgeführt werden:\n1kubectl kustomize --enable-helm apps/coredns | kubectl apply -n kube-system -f - Abhängigkeiten # CoreDNS ist eine Kernkomponente von Kubernetes und hat keine externen Anwendungsabhängigkeiten, ist aber für die Funktion fast aller anderen Anwendungen im Cluster kritisch.\n","date":"12 Januar 2026","externalUrl":null,"permalink":"/posts/homelab/k3s-prod/apps/nameserver/readme/","section":"Beiträge","summary":"CoreDNS dient als primärer DNS-Server für die Service Discovery innerhalb des Kubernetes-Clusters und löst sowohl interne als auch externe DNS-Anfragen auf.","title":"CoreDNS","type":"posts"},{"content":" DMARC Report # Das DMARC Report Tool ist eine Anwendung zur Verarbeitung und Visualisierung von DMARC-Berichten. Es hilft Ihnen, die E-Mail-Authentifizierung und -Zustellbarkeit Ihrer Domains zu überwachen und zu verbessern.\nStatus # Deployment # Komponenten # Diese DMARC Report-Bereitstellung umfasst:\nDMARC Report Anwendung: Die Kernanwendung zur Verarbeitung und Anzeige von DMARC-Berichten. MariaDB: Die relationale Datenbank, die von der DMARC Report Anwendung zur Speicherung ihrer Daten verwendet wird. Quelle # Die Anwendung basiert auf dem Docker-Image docker.io/gutmensch/dmarc-report. MariaDB: Offizielles Docker Image\nDokumentation # Die Dokumentation für das dmarc-report-Image ist auf GitHub verfügbar.\nFunktion # Dieser Dienst ruft DMARC-Berichte von einem IMAP-Postfach ab, parst sie und speichert die Daten in einer MariaDB-Datenbank. Die Berichte können dann über eine Weboberfläche visualisiert werden, um Einblicke in die E-Mail-Authentifizierung und potenzielle Missbrauchsfälle zu erhalten.\nLokale Anpassungen # Die Konfiguration erfolgt über die values.yaml-Datei.\nWichtige Einstellungen # Ingress: Der Zugriff auf die Weboberfläche wird über einen Ingress mit einem Hostnamen ermöglicht. Der Zugriff ist durch Authentik geschützt. Datenbank: Eine MariaDB-Datenbank läuft als Sidecar-Container innerhalb desselben Pods. Speicher: Die Datenbankdateien werden persistent auf einem HostPath-Volume gespeichert. Installation # Der Dienst wird als Helm-Chart unter Verwendung des app-template aus dem bjw-s-labs Repository bereitgestellt. Die Bereitstellung wird durch die kustomization.yaml-Datei verwaltet.\n1kubectl kustomize --enable-helm apps/dmarc-report | kubectl apply -n dmarc-report -f - Abhängigkeiten # Ein laufender Kubernetes-Cluster. Ein Ingress-Controller (z.B. Traefik) für den externen Zugriff. Eine Zertifikatsmanagement-Lösung (z.B. cert-manager) zur Bereitstellung von TLS-Zertifikaten. Ein IMAP-Postfach, das DMARC-Berichte empfängt. Ein NFS-Server für persistente Speicherung. ","date":"12 Januar 2026","externalUrl":null,"permalink":"/posts/homelab/k3s-prod/apps/dmarc-report/readme/","section":"Beiträge","summary":"Dieses Dokument beschreibt die Bereitstellung des DMARC Report Tools, das DMARC-Berichte verarbeitet und visualisiert.","title":"DMARC Report","type":"posts"},{"content":" Endpoint Copier Operator # Der SUSE Endpoint Copier Operator ist ein Kubernetes Operator, der einen Kubernetes Service und seine entsprechenden Endpunkte kopiert und synchron hält. Dies ist besonders nützlich in Hochverfügbarkeits-(HA)-Setups.\nDeployment # Quelle # Die Anwendung wird als Helm-Chart von SUSE EDGE bezogen.\nDokumentation # Offizielle Dokumentation des Endpoint Copier Operators Funktion # Der Endpoint Copier Operator ist ein spezialisiertes Werkzeug, das Endpoint-Ressourcen in einem Kubernetes-Cluster beobachtet. Wenn ein Service mit einer spezifischen Annotation (endpoint-copier.suse.com/copy-to-namespaces) versehen wird, kopiert der Operator die zugehörigen Endpoints in die angegebenen Ziel-Namespaces. Dies ist nützlich, um Services über Namespace-Grenzen hinweg verfügbar zu machen, ohne sie extern über einen Ingress oder LoadBalancer freizugeben.\nLokale Anpassungen # Es werden die Standardeinstellungen des Helm-Charts ohne spezifische Anpassungen in einer values.yaml-Datei verwendet. Die Konfiguration erfolgt direkt über Annotationen an den Service-Ressourcen, die kopiert werden sollen.\nInstallation # Der Dienst wird als Helm-Chart aus dem SUSE Edge Chart Repository bereitgestellt. Die Bereitstellung wird durch die kustomization.yaml-Datei verwaltet.\n1kubectl kustomize --enable-helm apps/endpoint-copier-operator | kubectl apply -n endpoint-copier-operator -f - Abhängigkeiten # Der Operator hat keine externen Abhängigkeiten und funktioniert eigenständig innerhalb des Kubernetes-Clusters.\n","date":"12 Januar 2026","externalUrl":null,"permalink":"/posts/homelab/k3s-prod/apps/endpoint-copier-operator/readme/","section":"Beiträge","summary":"Dieses Dokument beschreibt die Bereitstellung des SUSE Endpoint Copier Operators, der Kubernetes Services und deren Endpunkte synchronisiert.","title":"Endpoint Copier Operator","type":"posts"},{"content":" Forgejo # Forgejo ist eine quelloffene, selbst gehostete Git-Plattform, die eine umfassende Lösung für die Versionskontrolle von Code, Projektmanagement und CI/CD bietet. Es ist ein Fork von Gitea und konzentriert sich auf die Community-Entwicklung.\nStatus # Deployment # Quelle # Das Projekt basiert auf der offiziellen Forgejo-Website. Das Docker-Image stammt von codeberg.org/forgejo/forgejo. MariaDB: Offizielles Docker Image\nDokumentation # Die offizielle Dokumentation von Forgejo ist unter forgejo.org/docs zu finden. Forgejo GitHub Repository\nFunktion # Dieser Dienst bietet eine vollständige Git-Hosting-Lösung mit Funktionen wie Repository-Verwaltung, Issue-Tracking, Pull Requests, Wiki und CI/CD-Integration. Er ist über git.zyria.de zugänglich.\nLokale Anpassungen # Die umfangreiche Konfiguration erfolgt über Umgebungsvariablen in der values.yaml-Datei.\nWichtige Einstellungen # Ingress: Der Zugriff auf die Weboberfläche erfolgt über einen Ingress mit einem Hostnamen. SSH: Der Git-SSH-Zugriff wird über einen LoadBalancer-Service auf Port 22 bereitgestellt. Datenbank: Eine MariaDB-Datenbank läuft als Sidecar-Container. Speicher: Sowohl die Git-Daten als auch die Datenbank werden persistent auf einem HostPath-Volume gespeichert. Installation # Die Anwendung wird mittels Kustomize und Helm durch ArgoCD im Kubernetes-Cluster bereitgestellt. Die Konfiguration befindet sich im apps/forgejo-Verzeichnis. Eine manuelle Installation kann mit folgendem Befehl durchgeführt werden:\n1kubectl kustomize --enable-helm apps/forgejo | kubectl apply -n forgejo -f - Abhängigkeiten # Ein laufender Kubernetes-Cluster. Ein Ingress-Controller (z.B. Traefik) für den externen Zugriff. Eine Zertifikatsmanagement-Lösung (z.B. cert-manager) zur Bereitstellung von TLS-Zertifikaten. Ein NFS-Server für persistente Speicherung. Ein SMTP-Server für E-Mail-Benachrichtigungen. ","date":"12 Januar 2026","externalUrl":null,"permalink":"/posts/homelab/k3s-prod/apps/forgejo/readme/","section":"Beiträge","summary":"Dieses Dokument beschreibt die Bereitstellung von Forgejo, einer quelloffenen, selbst gehosteten Git-Plattform.","title":"Forgejo","type":"posts"},{"content":" Forgejo Runner # Ein Forgejo Runner ist ein Daemon, der mit einer Forgejo-Instanz zusammenarbeitet, um CI/CD (Continuous Integration/Continuous Delivery)-Jobs auszuführen. Er ruft Workflows von der Forgejo-Instanz ab, führt sie in einer isolierten Umgebung aus und sendet dann Protokolle und den endgültigen Status zurück.\nDeployment # Quelle # Die Bereitstellung verwendet ein benutzerdefiniertes Helm-Chart aus einem privaten Repository.\nDokumentation # Die Dokumentation für den Runner ist Teil der Forgejo-Dokumentation.\nFunktion # Dieser Dienst stellt Runner für die Ausführung von CI/CD-Pipelines bereit. Es sind zwei Instanzen konfiguriert: eine für die lokale Forgejo-Instanz und eine weitere für Codeberg.\nLokale Anpassungen # Die values.yaml-Datei enthält die Konfiguration für die Haupt-Runner-Instanz, einschließlich der Verbindung zur Forgejo-Instanz und der Runner-Labels. Die values-codeberg.yaml-Datei enthält die Konfiguration für die Codeberg-Runner-Instanz. Beide Instanzen verwenden Docker-in-Docker (DinD) für die Ausführung von Jobs. Wichtige Einstellungen # Instanzen: Es werden zwei Runner-Deployments erstellt: forgejo-runner: Verbindet sich mit der lokalen Instanz. forgejo-runner-codeberg: Verbindet sich mit der öffentlichen Instanz https://codeberg.org. Replikas: Für die lokale Instanz werden 3 Runner-Replikate betrieben, um parallele Job-Ausführung zu ermöglichen. Node Selector: Die Runner werden auf Nodes mit dem Label performance: high ausgeführt, um eine schnelle Job-Ausführung zu gewährleisten. Labels: Die Runner sind mit einer Vielzahl von Labels konfiguriert, die auf verschiedene Docker-Images von catthehacker/ubuntu verweisen. Diese Images sind speziell für die Verwendung mit Gitea/Forgejo Actions vorbereitet. Docker-in-Docker: Die Runner laufen im privileged-Modus und mit host-Netzwerk, um Docker-Befehle innerhalb der CI-Jobs ausführen zu können (Docker-in-Docker). Installation # Der Dienst wird als Helm-Chart aus einem privaten Repository bereitgestellt. Die Bereitstellung wird durch die kustomization.yaml-Datei verwaltet.\n1kubectl kustomize --enable-helm apps/forgejo-runner | kubectl apply -n forgejo-runner -f - Abhängigkeiten # Forgejo: Eine laufende Forgejo-Instanz, bei der sich die Runner registrieren können. Docker: Auf den Kubernetes-Worker-Nodes, auf denen die Runner laufen, muss eine Docker-Engine verfügbar sein, damit die Runner Docker-Container für die Jobs starten können. ","date":"12 Januar 2026","externalUrl":null,"permalink":"/posts/homelab/k3s-prod/apps/forgejo-runner/readme/","section":"Beiträge","summary":"Dieses Dokument beschreibt die Bereitstellung von Forgejo Runner, einem Daemon zum Ausführen von CI/CD-Jobs für Forgejo-Instanzen.","title":"Forgejo Runner","type":"posts"},{"content":"","date":"12 Januar 2026","externalUrl":null,"permalink":"/tags/gitops/","section":"Tags","summary":"","title":"Gitops","type":"tags"},{"content":" Headscale # Headscale ist eine quelloffene, selbst gehostete Implementierung des Tailscale Control Servers. Es ermöglicht Ihnen, Ihr eigenes Tailscale-Netzwerk zu betreiben und somit die vollständige Kontrolle über Ihre Knoten und Daten zu haben.\nDeployment # Quelle # Das Projekt basiert auf der offiziellen Headscale-Website. Das Helm-Chart stammt aus einem privaten Repository.\nDokumentation # Offizielle Headscale Dokumentation Headscale GitHub Repository Funktion # Dieser Dienst bietet einen selbst gehosteten Tailscale Control Server, der die Verwaltung von Tailscale-Knoten und die Einrichtung eines privaten Tailscale-Netzwerks ermöglicht. Er unterstützt die Integration mit Cert-Manager für TLS-Zertifikate und MetalLB für LoadBalancer-IPs.\nLokale Anpassungen # Die values.yaml-Datei konfiguriert den Headscale-Server, einschließlich der Listen-Adressen, Server-URL und DNS-Einstellungen. Die Persistenz wird über einen nfs-client StorageClass verwaltet. Es sind Patches für den DERP-Dienst enthalten, um Dual-Stack (IPv4/IPv6)-Unterstützung zu ermöglichen. TLS-Zertifikate werden über Cert-Manager verwaltet. Private Schlüssel für WireGuard, Noise und DERP werden als Secrets generiert. Wichtige Einstellungen # Server URL: Die öffentliche URL des Headscale-Servers ist https://hs.zyria.de. Datenbank: Headscale verwendet eine SQLite-Datenbank, die auf einem persistenten Volume gespeichert wird. Zertifikate: TLS-Zertifikate für den Ingress werden automatisch von cert-manager ausgestellt. DNS: Nodes im Headscale-Netzwerk erhalten DNS-Namen unter der Domain hsn.zyria.de. DERP: Es werden die öffentlichen DERP-Server von Tailscale verwendet. Ein eigener DERP-Service ist konfiguriert, aber nicht als Server aktiviert. Der STUN-Dienst läuft über einen LoadBalancer-Service. Speicher: Die Konfiguration und die SQLite-Datenbank werden auf einem persistenten Volume via NFS gespeichert. Installation # Die Anwendung wird mittels Kustomize und Helm durch ArgoCD im Kubernetes-Cluster bereitgestellt. Die Konfiguration befindet sich im apps/headscale-Verzeichnis. Eine manuelle Installation kann mit folgendem Befehl durchgeführt werden:\n1kubectl kustomize --enable-helm apps/headscale | kubectl apply -n headscale -f - Abhängigkeiten # Ein laufender Kubernetes-Cluster. Ein Ingress-Controller (z.B. Traefik) für den externen Zugriff. Eine Zertifikatsmanagement-Lösung (z.B. cert-manager) zur Bereitstellung von TLS-Zertifikaten. MetalLB für LoadBalancer-IP-Zuweisung. Ein NFS-Server für persistente Speicherung. ","date":"12 Januar 2026","externalUrl":null,"permalink":"/posts/homelab/k3s-prod/apps/headscale/readme/","section":"Beiträge","summary":"Dieses Dokument beschreibt die Bereitstellung von Headscale, einer quelloffenen, selbst gehosteten Implementierung des Tailscale Control Servers.","title":"Headscale","type":"posts"},{"content":" HedgeDoc # HedgeDoc ist ein kollaborativer Markdown-Editor, der es mehreren Benutzern ermöglicht, gleichzeitig an Dokumenten zu arbeiten. Er bietet eine einfache und intuitive Oberfläche für die Erstellung und Bearbeitung von Markdown-Inhalten.\nStatus # Deployment # Quelle # Die Anwendung wird als generisches app-template Helm-Chart von BJW-S Labs bereitgestellt.\nDas Projekt basiert auf der offiziellen HedgeDoc-Website. Das Docker-Image stammt von quay.io/hedgedoc/hedgedoc. PostgreSQL: Offizielles Docker Image\nDokumentation # Die offizielle Dokumentation von HedgeDoc ist unter docs.hedgedoc.org zu finden.\nHedgeDoc GitHub Repository Funktion # HedgeDoc dient als Plattform für die kollaborative Erstellung von Notizen, Dokumentationen und Präsentationen im Markdown-Format. Es ist ideal für Meetings, Brainstorming-Sitzungen oder das gemeinsame Verfassen von Texten. Die fertigen Dokumente können einfach geteilt und exportiert werden.\nLokale Anpassungen # Die Konfiguration erfolgt über Umgebungsvariablen in der values.yaml-Datei.\nWichtige Umgebungsvariablen # CMD_DB_URL: Die Verbindungs-URL zur PostgreSQL-Datenbank. CMD_DOMAIN: Die öffentliche Domain der Anwendung. CMD_OAUTH2_*: Konfiguriert die Authentifizierung über Authentik (OIDC), um Single Sign-On zu ermöglichen. CMD_ALLOW_ANONYMOUS: Ist auf false gesetzt, sodass nur authentifizierte Benutzer Dokumente erstellen und bearbeiten können. Wichtige Einstellungen # Ingress: Der Zugriff auf die Weboberfläche wird über einen Ingress mit einem Hostnamen ermöglicht. Authentifizierung: Die Benutzeranmeldung erfolgt ausschließlich über Authentik (SSO). Eine lokale Registrierung ist deaktiviert. Datenbank: Eine PostgreSQL-Datenbank läuft als Sidecar-Container. Speicher: Sowohl die hochgeladenen Dateien als auch die Datenbank werden persistent auf einem HostPath-Volume gespeichert. Installation # Die Anwendung wird mittels Kustomize und Helm durch ArgoCD im Kubernetes-Cluster bereitgestellt. Die Konfiguration befindet sich im apps/hedgedoc-Verzeichnis. Eine manuelle Installation kann mit folgendem Befehl durchgeführt werden:\n1kubectl kustomize --enable-helm apps/hedgedoc | kubectl apply -n hedgedoc -f - Abhängigkeiten # Ein laufender Kubernetes-Cluster. Ein Ingress-Controller (z.B. Traefik) für den externen Zugriff. Eine Zertifikatsmanagement-Lösung (z.B. cert-manager) zur Bereitstellung von TLS-Zertifikaten. Ein NFS-Server für persistente Speicherung. Ein LDAP-Server für die Benutzerauthentifizierung (optional). Ein OAuth2-Anbieter (optional). ","date":"12 Januar 2026","externalUrl":null,"permalink":"/posts/homelab/k3s-prod/apps/hedgedoc/readme/","section":"Beiträge","summary":"Dieses Dokument beschreibt die Bereitstellung von HedgeDoc, einem kollaborativen Markdown-Editor.","title":"HedgeDoc","type":"posts"},{"content":"","date":"12 Januar 2026","externalUrl":null,"permalink":"/tags/helm/","section":"Tags","summary":"","title":"Helm","type":"tags"},{"content":" Home Assistant # Deployment # Quelle # Die Anwendung wird als Helm-Chart aus einem privaten Repository auf git.zyria.de bezogen. Das verwendete Docker-Image ist die offizielle home-assistant-Version.\nDokumentation # Die offizielle Dokumentation von Home Assistant ist unter www.home-assistant.io/docs/ zu finden.\nFunktion # Home Assistant (HA) ist eine Steuerungszentrale für Smart-Home-Geräte. Es integriert eine breite Palette von Geräten und Protokollen (z.B. Zigbee, Matter) und ermöglicht es, Automatisierungen zu erstellen, den Zustand von Geräten zu überwachen und alles über eine einheitliche Weboberfläche zu steuern.\nLokale Anpassungen # Die Konfiguration erfolgt über die values.yaml-Datei.\nWichtige Einstellungen # Host-Netzwerk: Home Assistant wird im hostNetwork-Modus betrieben, um die automatische Erkennung von Geräten im lokalen Netzwerk (z.B. über mDNS, UPnP) zu erleichtern. Node Selector: Der Pod wird auf einem bestimmten Node mit dem Label conbee: \u0026quot;true\u0026quot; ausgeführt, an dem die für den Betrieb notwendige Hardware angeschlossen ist. Geräte-Passthrough: Mehrere Geräte werden vom Host-System in den Pod durchgereicht: /dev/ttyACM0: Ein Z-Wave/Zigbee-USB-Stick. /dev/serial/by-id/...ConBee_II...: Ein ConBee II Zigbee-Gateway. Matter-Server: Ein Matter-Server läuft als Sidecar-Container, um die Integration von Matter-Geräten zu ermöglichen. Speicher: Die gesamte Konfiguration von Home Assistant wird persistent auf einem HostPath-Volume gespeichert. Installation # Die Anwendung wird mittels Kustomize und Helm durch ArgoCD im Kubernetes-Cluster bereitgestellt. Die Konfiguration befindet sich im apps/home-assistant-Verzeichnis. Eine manuelle Installation kann mit folgendem Befehl durchgeführt werden:\n1kubectl kustomize --enable-helm apps/home-assistant | kubectl apply -n home-assistant -f - Abhängigkeiten # Spezifische Hardware: Benötigt einen Kubernetes-Node mit angeschlossenen USB-Geräten (ConBee II Stick). NFS: Ein NFS-Server wird für das persistente Speichern der Konfiguration benötigt. ","date":"12 Januar 2026","externalUrl":null,"permalink":"/posts/homelab/k3s-prod/apps/home-assistant/readme/","section":"Beiträge","summary":"Home Assistant ist das Gehirn des Smart Homes. Es ermöglicht die Steuerung und Automatisierung einer Vielzahl von Geräten und Diensten und stellt eine zentrale Benutzeroberfläche bereit.","title":"Home Assistant","type":"posts"},{"content":"","date":"12 Januar 2026","externalUrl":null,"permalink":"/tags/homelab/","section":"Tags","summary":"","title":"Homelab","type":"tags"},{"content":" hx53.de # Status # Deployment # Quelle # Die Anwendung wird als generisches app-template Helm-Chart von BJW-S Labs bereitgestellt. Das verwendete Docker-Image ist ein benutzerdefiniertes Nginx-Image, das von git.zyria.de bezogen wird.\nDokumentation # Der Quellcode der Webseite befindet sich im Git-Repository pyrox/hx53_de auf der internen Forgejo-Instanz.\nFunktion # Dieser Dienst stellt eine einfache statische Webseite bereit. Ein Init-Container oder ein Sidecar-Container innerhalb des Pods klont den pages-Branch des Git-Repositories pyrox/hx53_de. Der Nginx-Container dient dann als Webserver und liefert die geklonten Dateien aus.\nLokale Anpassungen # Die Konfiguration erfolgt über die values.yaml-Datei.\nWichtige Umgebungsvariablen # SITE_REPO: Die URL des Git-Repositories, das den Inhalt der Webseite enthält. SITE_BRANCH: Der Branch, der ausgecheckt werden soll (pages). Wichtige Einstellungen # Ingress: Der Zugriff auf die Webseite wird über einen Ingress mit den Hostnamen hx53.de und www.hx53.de ermöglicht. Installation # Die Anwendung wird mittels Kustomize und Helm durch ArgoCD im Kubernetes-Cluster bereitgestellt. Die Konfiguration befindet sich im apps/hx53-de-Verzeichnis. Eine manuelle Installation kann mit folgendem Befehl durchgeführt werden:\n1kubectl kustomize --enable-helm apps/hx53-de | kubectl apply -n hx53-de -f - Abhängigkeiten # Traefik: Wird als Ingress-Controller für den externen Zugriff benötigt. cert-manager: Wird für die automatische Bereitstellung von TLS-Zertifikaten benötigt. Forgejo: Das Git-Repository mit dem Inhalt der Webseite muss auf der internen Forgejo-Instanz verfügbar sein. ","date":"12 Januar 2026","externalUrl":null,"permalink":"/posts/homelab/k3s-prod/apps/hx53-de/readme/","section":"Beiträge","summary":"Dieser Dienst hostet die Webseite hx53.de. Der Inhalt der Seite wird aus einem Git-Repository geklont und von einem Nginx-Webserver ausgeliefert.","title":"hx53.de","type":"posts"},{"content":" IT-Tools # Status # Deployment # Quelle # Die Anwendung wird als generisches app-template Helm-Chart von BJW-S Labs bereitgestellt. Das verwendete Docker-Image ist corentinth/it-tools.\nDokumentation # Das Projekt und eine Live-Demo sind auf it-tools.tech zu finden. Die Quelldateien sind auf GitHub verfügbar.\nFunktion # IT-Tools bietet eine Sammlung von über 50 nützlichen Werkzeugen für alltägliche IT-Aufgaben. Dazu gehören unter anderem:\nToken-Generatoren (JWT, etc.) Konverter (Timestamp, Base64, etc.) Code-Formatierer (JSON, SQL, XML) Verschlüsselungs- und Hashing-Tools (AES, SHA) Netzwerk-Tools (CIDR-Rechner) Da die Anwendung vollständig im Browser des Benutzers ausgeführt wird, werden keine Daten an den Server gesendet, was die Nutzung sicher macht.\nLokale Anpassungen # Es werden die Standardeinstellungen des Docker-Images ohne spezifische Konfigurationen verwendet.\nWichtige Einstellungen # Ingress: Der Zugriff auf die Webseite wird über einen Ingress mit einem Hostnamen ermöglicht. Installation # Die Anwendung wird mittels Kustomize und Helm durch ArgoCD im Kubernetes-Cluster bereitgestellt. Die Konfiguration befindet sich im apps/ittools-Verzeichnis. Eine manuelle Installation kann mit folgendem Befehl durchgeführt werden:\n1kubectl kustomize --enable-helm apps/ittools | kubectl apply -n ittools -f - Abhängigkeiten # Traefik: Wird als Ingress-Controller für den externen Zugriff benötigt. cert-manager: Wird für die automatische Bereitstellung von TLS-Zertifikaten benötigt. ","date":"12 Januar 2026","externalUrl":null,"permalink":"/posts/homelab/k3s-prod/apps/ittools/readme/","section":"Beiträge","summary":"IT-Tools ist eine rein clientseitige Webanwendung, die eine Vielzahl von Werkzeugen wie Konverter, Formatierer, Generatoren und vieles mehr für den täglichen Gebrauch bereitstellt.","title":"IT-Tools","type":"posts"},{"content":"","date":"12 Januar 2026","externalUrl":null,"permalink":"/tags/k3s/","section":"Tags","summary":"","title":"K3s","type":"tags"},{"content":" k3s Konfiguration # Deployment # Quelle # Die Konfigurationen werden direkt als Kubernetes-Manifeste und über ein benutzerdefiniertes Helm-Chart aus einem privaten Repository auf git.zyria.de bereitgestellt.\nDokumentation # Die offizielle Dokumentation von k3s ist unter docs.k3s.io zu finden.\nFunktion # Dieser Chart und die zugehörigen Manifeste definieren Kernkomponenten der k3s-Installation, die über den Standard hinausgehen.\nLokale Anpassungen # Die Konfiguration erfolgt über die in diesem Verzeichnis enthaltenen YAML-Dateien.\nWichtige Komponenten # Local Path Provisioner: Die Datei local-path.yaml installiert den Local Path Provisioner. Dieser stellt eine einfache Möglichkeit dar, persistente Volumes mithilfe des lokalen Speichers auf den jeweiligen Kubernetes-Nodes zu erstellen. Er ist der Standard-StorageClass im Cluster.\nKubernetes VIP: Die Datei kubernetes-vip.yaml definiert einen Service vom Typ LoadBalancer für den Kubernetes API-Server (kubernetes.default.svc). Dies weist dem API-Server eine stabile virtuelle IP-Adresse (VIP) aus dem Metallb-Adresspool zu. Dies vereinfacht den Zugriff auf die API von außerhalb des Clusters.\nk3s Helm Chart: Ein Platzhalter-Chart, das möglicherweise für zukünftige, direkt mit k3s zusammenhängende Konfigurationen verwendet wird.\nInstallation # Die Komponenten werden durch ArgoCD im Kubernetes-Cluster bereitgestellt. Die Konfiguration befindet sich im apps/k3s-Verzeichnis. Eine manuelle Installation kann mit folgendem Befehl durchgeführt werden:\n1kubectl kustomize --enable-helm apps/k3s | kubectl apply -n kube-system -f - Abhängigkeiten # k3s: Eine laufende k3s-Installation ist die Grundvoraussetzung. Metallb: Wird benötigt, um dem kubernetes-vip-Service eine externe IP-Adresse zuzuweisen. ","date":"12 Januar 2026","externalUrl":null,"permalink":"/posts/homelab/k3s-prod/apps/k3s/readme/","section":"Beiträge","summary":"Dieser Ordner enthält grundlegende Konfigurationen für den k3s Kubernetes-Cluster, einschließlich des Local Path Provisioners und einer virtuellen IP für den Kubernetes API-Server.","title":"k3s Konfiguration","type":"posts"},{"content":"","date":"12 Januar 2026","externalUrl":null,"permalink":"/categories/k3s-prod/","section":"Categories","summary":"","title":"K3s-Prod","type":"categories"},{"content":" Kubernetes Cluster Konfiguration und Anwendungsbereitstellung # Dieses Repository dient als zentrale Quelle für die Konfiguration und das Management eines k3s Kubernetes Produktionsclusters. Es folgt strikt den GitOps-Prinzipien, wobei der gewünschte Zustand des Clusters und aller darauf laufenden Anwendungen deklarativ in Git definiert und durch ArgoCD automatisch synchronisiert wird.\nZweck des Repositories # Das Hauptziel dieses Repositorys ist es, eine transparente, versionierte und nachvollziehbare Verwaltung der gesamten Kubernetes-Infrastruktur zu ermöglichen. Es umfasst:\nCluster-Basiskonfiguration: Grundlegende Einstellungen und Komponenten des k3s-Clusters. Anwendungsbereitstellung: Definition und Konfiguration aller im Cluster bereitgestellten Anwendungen. Automatisierung: Nutzung von ArgoCD für die automatische Synchronisation des Cluster-Zustands mit dem Git-Repository. Verwendete Technologien # Kubernetes (k3s): Eine leichtgewichtige, CNCF-zertifizierte Kubernetes-Distribution, ideal für Edge-Computing und Homelab-Umgebungen. ArgoCD: Ein deklaratives GitOps-Continuous-Delivery-Tool für Kubernetes. Helm: Ein Paketmanager für Kubernetes, der die Definition, Installation und Aktualisierung komplexer Kubernetes-Anwendungen vereinfacht. Kustomize: Ein Tool zur Anpassung von Kubernetes-Konfigurationen ohne Vorlagen. Traefik: Ein moderner Edge Router und Load Balancer für Kubernetes. MetalLB: Eine Load-Balancer-Implementierung für Bare-Metal-Kubernetes-Cluster. Cert-Manager: Automatisiert die Ausstellung und Verwaltung von TLS-Zertifikaten. NFS: Dient als persistenter Speicher für viele Anwendungen im Cluster. Ordnerstruktur # .forgejo/: Konfigurationen für Forgejo-spezifische Workflows und Renovate. ansible/: Ansible-Playbooks für die Cluster-Initialisierung und -Verwaltung. apps/: Der zentrale Ordner für alle Anwendungen. Jeder Unterordner in apps/ repräsentiert eine eigenständige Anwendung oder eine Gruppe verwandter Dienste, die im Cluster bereitgestellt werden. Jede Anwendung enthält eine README.md-Datei mit detaillierter Dokumentation. doc/: Allgemeine Dokumentation und Anleitungen für den Cluster. init.yaml: Initialisierungs-Manifeste für ArgoCD. manifests.yaml: Globale Kubernetes-Manifeste, die clusterweit angewendet werden. renovate.json5: Konfiguration für Renovate Bot zur Automatisierung von Abhängigkeits-Updates. Anwendungen im Cluster # Detaillierte Informationen zu jeder einzelnen Anwendung, ihrer Funktion, Konfiguration und Abhängigkeiten finden Sie in den jeweiligen README.md-Dateien unterhalb des apps/-Verzeichnisses.\nBeispielpfad: apps/\u0026lt;appname\u0026gt;/README.md\nInstallation und Bereitstellung # Für eine detaillierte Anleitung zur Installation des Clusters und zur Bereitstellung der Anwendungen lesen Sie bitte die Dokumentation unter doc/install.md.\nTesten von Anwendungen lokal (z.B. mit Podman) # Um eine Anwendung lokal zu testen, bevor sie im Cluster bereitgestellt wird, können Sie kubectl kustomize in Kombination mit Helm verwenden:\n1kubectl kustomize --enable-helm apps/\u0026lt;appfolder\u0026gt; Für die Bereitstellung in einem lokalen Podman-Kubernetes-Cluster:\n1helm template \u0026lt;project\u0026gt; bjw-s-labs/app-template --values \u0026lt;file.yaml\u0026gt; | podman play kube - Beitrag und Richtlinien # Beiträge zur Verbesserung dieses Repositorys sind willkommen. Bitte beachten Sie die bestehenden Konventionen und die Struktur der README.md-Dateien in den apps/-Unterordnern, um die Konsistenz der Dokumentation zu gewährleisten.\n","date":"12 Januar 2026","externalUrl":null,"permalink":"/posts/homelab/k3s-prod/","section":"Beiträge","summary":"Zentrale Verwaltung eines k3s Kubernetes Clusters und seiner Anwendungen durch GitOps mit ArgoCD, Helm und Kustomize.","title":"k3s-prod: Kubernetes Cluster Konfiguration und Anwendungsbereitstellung","type":"posts"},{"content":" Keel # Status # Deployment # Quelle # Die Anwendung wird als Helm-Chart von keel.sh bezogen.\nDokumentation # Die offizielle Dokumentation von Keel ist unter keel.sh/docs zu finden.\nFunktion # Keel automatisiert den Prozess der Anwendungsaktualisierung. Es kann so konfiguriert werden, dass es Docker-Registries auf neue Image-Tags überwacht. Wenn ein neues Image gefunden wird, das einer vordefinierten Policy entspricht (z.B. semver), aktualisiert Keel das entsprechende Deployment, StatefulSet, etc. im Kubernetes-Cluster. In diesem Setup wird Keel hauptsächlich durch Annotationen an den jeweiligen Workloads gesteuert.\nLokale Anpassungen # Die Konfiguration erfolgt über die values.yaml-Datei.\nWichtige Einstellungen # Polling: Keel ist so konfiguriert, dass es stündlich nach Updates sucht (@every 1h). Helm-Provider: Die Unterstützung für Helm v3 ist aktiviert, sodass Keel auch Helm-Releases aktualisieren kann. Self-Update: Keel ist so konfiguriert, dass es sich selbst automatisch auf die neueste Version aktualisiert (policy: force). Ingress: Der Zugriff auf die Keel-Weboberfläche wird über einen Ingress mit einem Hostnamen ermöglicht. Der Zugriff ist durch Authentik geschützt. Installation # Die Anwendung wird mittels Kustomize und Helm durch ArgoCD im Kubernetes-Cluster bereitgestellt. Die Konfiguration befindet sich im apps/keel-Verzeichnis. Eine manuelle Installation kann mit folgendem Befehl durchgeführt werden:\n1kubectl kustomize --enable-helm apps/keel | kubectl apply -n keel -f - Abhängigkeiten # Traefik: Wird als Ingress-Controller für den externen Zugriff benötigt. Authentik: Wird zur Absicherung des Web-Frontends verwendet. cert-manager: Wird für die automatische Bereitstellung von TLS-Zertifikaten benötigt. ","date":"12 Januar 2026","externalUrl":null,"permalink":"/posts/homelab/k3s-prod/apps/keel/readme/","section":"Beiträge","summary":"Keel überwacht Container-Image-Repositories und Helm-Charts auf neue Versionen und aktualisiert die entsprechenden Deployments im Cluster automatisch gemäß den definierten Policies.","title":"Keel","type":"posts"},{"content":"","date":"12 Januar 2026","externalUrl":null,"permalink":"/tags/kubernetes/","section":"Tags","summary":"","title":"Kubernetes","type":"tags"},{"content":" Kubernetes Descheduler # Der Kubernetes Descheduler ist ein Tool, das die Pod-Platzierung in einem Cluster optimiert, indem es Pods, die nicht den definierten Richtlinien entsprechen, entfernt. Dies hilft, die Ressourcennutzung zu verbessern und die Cluster-Gesundheit zu erhalten.\nDeployment # Quelle # Das Projekt basiert auf dem Kubernetes Descheduler GitHub Repository. Das Helm-Chart stammt aus dem Kubernetes Descheduler Helm Charts Repository.\nDokumentation # Kubernetes Descheduler Dokumentation Kubernetes Descheduler GitHub Repository Funktion # Der Kubernetes Descheduler läuft als CronJob und prüft den Cluster regelmäßig auf Pods, die basierend auf den definierten Policies umverteilt werden sollten. Dies hilft, die Ressourcennutzung zu optimieren und die Cluster-Stabilität zu verbessern. Zum Beispiel kann er Pods von Knoten entfernen, die unter- oder überausgelastet sind, oder Pods, die Anti-Affinity-Regeln verletzen.\nLokale Anpassungen # Die Konfiguration erfolgt über die values.yaml-Datei.\nWichtige Einstellungen # Betriebsart: Der Descheduler wird als CronJob ausgeführt, der alle zwei Minuten läuft. Prioritätsklasse: Er läuft mit der Prioritätsklasse system-cluster-critical, um sicherzustellen, dass er bei Ressourcenknappheit nicht verdrängt wird. Aktivierte Strategien: RemoveDuplicates: Entfernt doppelte Pods, die auf demselben Knoten laufen. RemovePodsHavingTooManyRestarts: Entfernt Pods mit einer hohen Anzahl von Neustarts (mehr als 50). RemovePodsViolatingNodeAffinity, RemovePodsViolatingNodeTaints, RemovePodsViolatingInterPodAntiAffinity, RemovePodsViolatingTopologySpreadConstraint: Stellt sicher, dass Pods, die gegen (Anti-)Affinity-, Taint- oder Topologie-Regeln verstoßen, neu geplant werden. LowNodeUtilization \u0026amp; HighNodeUtilization: Versucht, Pods von unter- und überausgelasteten Knoten zu verschieben, um eine ausgeglichenere Ressourcennutzung zu erreichen. Installation # Die Anwendung wird mittels Kustomize und Helm durch ArgoCD im Kubernetes-Cluster bereitgestellt. Die Konfiguration befindet sich im apps/descheduler-Verzeichnis. Eine manuelle Installation kann mit folgendem Befehl durchgeführt werden:\n1kubectl kustomize --enable-helm apps/descheduler | kubectl apply -n descheduler -f - Abhängigkeiten # Der Descheduler hat keine direkten Anwendungsabhängigkeiten, interagiert aber eng mit dem Kubernetes Scheduler.\n","date":"12 Januar 2026","externalUrl":null,"permalink":"/posts/homelab/k3s-prod/apps/descheduler/readme/","section":"Beiträge","summary":"Dieses Dokument beschreibt die Bereitstellung des Kubernetes Deschedulers, eines Tools zur Optimierung der Pod-Platzierung in einem Cluster.","title":"Kubernetes Descheduler","type":"posts"},{"content":"","date":"12 Januar 2026","externalUrl":null,"permalink":"/tags/kustomize/","section":"Tags","summary":"","title":"Kustomize","type":"tags"},{"content":" Longhorn # Deployment # Quelle # Die Anwendung wird als Helm-Chart von longhorn.io bezogen.\nDokumentation # Die offizielle Dokumentation von Longhorn ist unter longhorn.io/docs zu finden.\nFunktion # Longhorn ist die primäre Lösung für persistenten Speicher (Storage) in diesem Cluster. Es erstellt aus dem lokalen Speicher der einzelnen Kubernetes-Nodes einen verteilten, hochverfügbaren Speicher-Pool. Für jedes Persistent Volume (PV), das von einer Anwendung angefordert wird, erstellt Longhorn ein Volume, das synchron auf mehrere Nodes repliziert wird (standardmäßig 3 Replikate). Fällt ein Node aus, bleiben die Daten auf den anderen Replikaten verfügbar. Longhorn bietet zudem Funktionen wie Snapshots, Backups (z.B. auf S3 oder NFS) und eine intuitive Web-UI zur Verwaltung.\nLokale Anpassungen # Die Konfiguration erfolgt über die values.yaml-Datei.\nWichtige Einstellungen # Netzwerk-Policies: Es werden automatisch Netzwerk-Policies für k3s erstellt, um den Traffic zwischen den Longhorn-Komponenten zu kontrollieren. Ingress: Der Zugriff auf die Longhorn-Weboberfläche wird über einen Ingress mit einem Hostnamen ermöglicht. Der Zugriff ist durch eine Middleware auf Traefik-Ebene auf lokale Netzwerke beschränkt. Installation # Die Anwendung wird mittels Kustomize und Helm durch ArgoCD im Kubernetes-Cluster bereitgestellt. Die Konfiguration befindet sich im apps/longhorn-Verzeichnis. Eine manuelle Installation kann mit folgendem Befehl durchgeführt werden:\n1kubectl kustomize --enable-helm apps/longhorn | kubectl apply -n longhorn-system -f - Abhängigkeiten # Traefik: Wird als Ingress-Controller für den externen Zugriff auf die UI benötigt. cert-manager: Wird für die automatische Bereitstellung von TLS-Zertifikaten für die UI benötigt. ","date":"12 Januar 2026","externalUrl":null,"permalink":"/posts/homelab/k3s-prod/apps/longhorn/readme/","section":"Beiträge","summary":"Longhorn bietet hochverfügbaren, persistenten Blockspeicher für stateful Anwendungen im Kubernetes-Cluster, indem es den lokalen Speicherplatz der Worker-Nodes nutzt und über das Netzwerk repliziert.","title":"Longhorn","type":"posts"},{"content":" Mailstack # Deployment # Quelle # Die Anwendung wird als benutzerdefiniertes Helm-Chart aus einem privaten Repository auf git.zyria.de bezogen. Es bündelt verschiedene Open-Source-Mail-Komponenten.\nDokumentation # Die Konfiguration basiert auf dem Mailu Projekt, wurde aber stark angepasst. Die Dokumentation der einzelnen Komponenten (Postfix, Dovecot, etc.) ist auf deren jeweiligen Projekt-Webseiten zu finden.\nFunktion # Der Mailstack ist für den gesamten E-Mail-Verkehr der gehosteten Domains zuständig. Er besteht aus mehreren ineinandergreifenden Diensten:\nPostfix: Der Mail Transfer Agent (MTA), der E-Mails über SMTP empfängt und versendet. Dovecot: Der Mail Delivery Agent (MDA), der E-Mails in den Postfächern der Benutzer ablegt und den Zugriff über IMAP ermöglicht. Dient auch zur SASL-Authentifizierung für Postfix. Rspamd: Ein fortschrittliches Spam-Filtersystem. Es prüft eingehende E-Mails auf Spam-Merkmale, signiert ausgehende E-Mails mit DKIM und stellt eine Weboberfläche zur Verwaltung bereit. ClamAV: Ein Virenscanner, der E-Mail-Anhänge auf Malware überprüft. Unbound: Ein lokaler DNS-Resolver, der für DNS-Abfragen (z.B. für RBLs) verwendet wird. Redis: Dient als schneller Cache und Speicher für Rspamd. Lokale Anpassungen # Die Konfiguration ist sehr umfangreich und erfolgt über die values.yaml-Datei, sowie diverse ConfigMaps und Secrets.\nWichtige Einstellungen # LoadBalancer: Die Mail-Ports (SMTP, IMAP) werden über einen LoadBalancer-Service mit einer dedizierten externen IP-Adresse bereitgestellt. Ingress: Die Weboberflächen von Rspamd und dem Autodiscover-Dienst sind über Ingresses erreichbar. Der Zugriff auf Rspamd ist durch Authentik geschützt. LDAP-Integration: Postfix und Dovecot sind mit einem LDAP-Server integriert, um Benutzer, Aliase und Weiterleitungen zu verwalten. Speicher: Alle persistenten Daten (Mail-Speicher, Konfigurationen, Rspamd-Daten) werden auf einem NFS-Share unter /srv/container_nfs/mailstack/ gespeichert. Sicherheit: Ausgehende E-Mails werden mit DKIM signiert und es wird MTA-STS für eine sichere E-Mail-Zustellung verwendet. Installation # Die Anwendung wird mittels Kustomize und Helm durch ArgoCD im Kubernetes-Cluster bereitgestellt. Die Konfiguration befindet sich im apps/mailstack-Verzeichnis. Eine manuelle Installation kann mit folgendem Befehl durchgeführt werden:\n1kubectl kustomize --enable-helm apps/mailstack | kubectl apply -n mailstack -f - Abhängigkeiten # LDAP-Server: Ein externer LDAP-Server wird für die Benutzerverwaltung benötigt. Traefik: Wird als Ingress-Controller für den externen Zugriff auf die Web-UIs benötigt. Metallb: Wird benötigt, um dem Mail-Service eine externe IP-Adresse zuzuweisen. NFS: Ein NFS-Server wird für das persistente Speichern aller Daten benötigt. Authentik: Wird zur Absicherung der Rspamd-Weboberfläche verwendet. Nutzerteil # Einleitung und Geltungsbereich # Dieses Dokument beschreibt die technische Konfiguration, die administrativen Prozesse und die Nutzungsrichtlinien für das E-Mail-System. Es richtet sich an IT-Administratoren, den IT-Support sowie an alle Mitarbeiter als Leitfaden für die ordnungsgemäße Nutzung des E-Mail-Dienstes.\nZiel ist es, einen stabilen, sicheren und effizienten E-Mail-Verkehr zu gewährleisten und klare Anweisungen für Standardprozeduren bereitzustellen.\nSystemübersicht # Die E-Mail-Infrastruktur ist vollständig lokal (\u0026ldquo;on-premises\u0026rdquo;) und basiert auf den folgenden Open-Source-Komponenten:\nMail Transfer Agent (MTA): Postfix. Zuständig für den Empfang und Versand von E-Mails über das SMTP-Protokoll.\nMail Delivery \u0026amp; Access (MDA/IMAP): Dovecot. Speichert die E-Mails im Maildir-Format auf unseren Servern und stellt sie den Benutzern über die Protokolle IMAP (zum Empfangen) und POP3 (optional) zur Verfügung.\nE-Mail-Security: rspamd. Ein fortschrittliches Filtersystem, das eingehende E-Mails in Echtzeit auf Spam, Viren und andere Bedrohungen prüft, bevor sie in die Postfächer zugestellt werden.\nWebmail-Client: Roundcube. Ermöglicht den Zugriff auf E-Mails über jeden modernen Webbrowser.\nUnterstützte E-Mail-Clients:\nDesktop: Jeder E-Mail-Client, der IMAP und SMTP unterstützt (z.B. Mozilla Thunderbird, Microsoft Outlook, Apple Mail).\nWeb: Roundcube Webmail.\nMobil: Jede mobile E-Mail-App, die IMAP und SMTP unterstützt.\nBenutzer- und Postfachverwaltung # Die Verwaltung von Postfächern erfolgt durch die IT-Administration über Systemskripte, die mit der zentralen Benutzerdatenbank interagieren.\nAnlage eines neuen Postfach # Ein neuer Benutzer wird in der zentralen Benutzerdatenbank angelegt.\nEin dazugehöriges Postfach im Maildir-Format wird auf dem Mailserver erstellt. (Welcome Mail lässt Dovecot das Postfach anlegen)\nDie primäre E-Mail-Adresse wird konfiguriert. Notwendige Aliase werden in der zentralen Benutzerdatenbank gespeichert.\nDer Benutzer erhält sein initiales Passwort und die Server-Konfigurationsdaten von der IT-Abteilung.\nDeaktivierung eines Postfachs (Offboarding) # Der Benutzer-Account wird am letzten Arbeitstag gesperrt. Der Login ist damit sofort unterbunden.\nNach Rücksprache mit der Fachabteilung wird entschieden, wie mit den bestehenden E-Mails verfahren wird:\nArchivierung: Das Maildir-Verzeichnis des Benutzers wird als komprimiertes Archiv (.tar.gz) gesichert und für die gesetzliche Aufbewahrungsfrist aufbewahrt.\nWeiterleitung: Optional wird eine serverseitige Weiterleitung an den Vorgesetzten oder Nachfolger eingerichtet.\nDas aktive Postfach und der Benutzer werden nach einer Frist von 30 Tagen endgültig vom System gelöscht.\nKonfigurationsanleitungen # Für die Einrichtung Ihres E-Mail-Kontos benötigen Sie die folgenden Server-Einstellungen:\nBenutzername: Ihre volle E-Mail-Adresse Passwort: Das Ihnen zugewiesene Passwort Server für eintreffende E-Mails (IMAP):\nServer: mail.ihredomain.de (Beispiel anpassen)\nPort: 993\nVerbindungssicherheit: SSL/TLS oder STARTTLS\nAuthentifizierungsmethode: Passwort, normal\nServer für ausgehende E-Mails (SMTP):\nServer: mail.ihredomain.de (Beispiel anpassen)\nPort: 587\nVerbindungssicherheit: STARTTLS\nAuthentifizierungsmethode: Passwort, normal\nSicherheitsrichtlinien und Best Practices # Passwörter: Passwörter müssen den Richtlinien entsprechen. Geben Sie Ihr Passwort niemals an Dritte weiter. Sollte das Passwort einmal in die Hände Dritter gelangen ist sofort die IT zu informieren.\nPhishing und verdächtige E-Mails:\nKlicken Sie niemals auf Links oder öffnen Sie Anhänge in E-Mails von unbekannten Absendern oder bei unerwarteten Nachrichten.\nMelden von Phishing: Legen Sie verdächtige E-Mails bitte in das Postfach Junk oder je nach Sprache auch Spam. Dies hilft uns, die Filterregeln in rspamd zu verbessern.\nmgang mit Anhängen:*\nNutzen Sie für größere Anhänge einen Dateidienst.\nGefährliche Dateitypen (z.B. .exe, .bat, .js) werden von rspamd automatisch blockiert.\nVertrauliche Informationen: Für den Versand streng vertraulicher Daten wird eine clientseitige Ende-zu-Ende-Verschlüsselung mittels S/MIME oder PGP/GPG empfohlen. Die Einrichtung liegt in der Verantwortung des Benutzers.\nBackup und Wiederherstellung # Backup-Strategie: Die IT-Administration führt nächtliche Backups der E-Mail-Infrastruktur durch. Dies umfasst:\nDie Maildir-Verzeichnisse aller Benutzer.\nDie Konfigurationsdateien von Postfix, Dovecot und rspamd.\nDie Benutzerdatenbank.\nWiederherstellung von E-Mails: Gelöschte E-Mails können nicht vom Benutzer selbst wiederhergestellt werden. Bitte kontaktieren Sie den IT-Support mit einer genauen Angabe der E-Mail (Absender, Betreff, ungefähres Datum), damit diese aus dem Backup wiederhergestellt werden kann.\nIT Teil # Architektur # Der Mailstack wird auf mehreren virtuellen Maschinen als Kubernetes (k3s) Cluster aufgebaut. Es werden unabhängige Deployments bevorzugt und je Pod nur die nötigsten Dienste betrieben. Der Ein-Dienst-Ein-Container-Ansatz wird nicht empfohlen, da so unnötig viele Container erstellt werden. Da Dovecot und Postfix die gleiche IP Adressen haben sollen und beide Container die echte IP des Endnutzers sehen müssen, ist es notwendig beide Container in einem Pod zu betreiben. Siehe dazu die Metallb Dokumentation.\nHier ist ein Diagramm, das den Fluss einer eingehenden E-Mail visualisiert:\n1sequenceDiagram 2 participant Externer MTA 3 participant Postfix (MTA) 4 participant Rspamd (Filter) 5 participant Dovecot (MDA) 6 participant Maildir (Speicher) 7 8 Externer MTA-\u0026gt;\u0026gt;+Postfix (MTA): E-Mail via SMTP 9 Postfix (MTA)-\u0026gt;\u0026gt;+Rspamd (Filter): Prüfung via Milter 10 Rspamd (Filter)--\u0026gt;\u0026gt;-Postfix (MTA): Ergebnis (Clean/Spam) 11 Postfix (MTA)-\u0026gt;\u0026gt;+Dovecot (MDA): Zustellung via LMTP 12 Dovecot (MDA)-\u0026gt;\u0026gt;+Maildir (Speicher): Speichert E-Mail Vorhandene Infrastruktur (Out of scope für dieses Dokument) # Zeitsynchronisation (NTP - Network Time Protocol) # Eine exakte und synchronisierte Systemzeit ist entscheidend für korrekte Zeitstempel in E-Mail-Headern, die Analyse von Logdateien und die Gültigkeit von Authentifizierungstickets\nTransportverschlüsselung (TLS/SSL) # Verschlüsselt die Kommunikation zwischen E-Mail-Clients und dem Server sowie zwischen den Mailservern untereinander (via STARTTLS). Dies schützt Anmeldedaten und E-Mail-Inhalte vor dem Mitlesen.\nZentrale Benutzerauthentifizierung (LDAP / Active Directory # Anstatt Benutzerkonten und Passwörter lokal auf dem Mailserver zu verwalten, kann der Server an einen zentralen Verzeichnisdienst angebunden werden.\nMonitoring und Logging # Eine lückenlose Protokollierung aller Vorgänge ist für die Fehleranalyse (z.B. bei Zustellproblemen) und die Erkennung von Sicherheitsvorfällen (z.B. kompromittierte Konten) unerlässlich.\nDNS Konfiguration # Folgende DNS Einträge müssen gesetzt werden # Alle Einträge müssen in den entsprechenden Zonen gesetzt werden, für welche der Mailserver zuständig sein soll.\nBasis:\nA AAAA (wenn wir das aktuell nicht nutzen bitte NICHT setzen) MX PTR Sicherheit:\nTXT SPF (Sender Policy Framework) DMARC (Domain-based Message Authentication, Reporting, and Conformance) DKIM (DomainKeys Identified Mail) MTA-STS TLSRPT CAA SRV smtp ssmtp imap imaps submission Optional:\nTXT autodiscover autoconfig Bei Delegierung der ACME Challenge auch einen CNAME Eintrag.\nDienste welche mit dem Mailstack interagieren # MTA-STS um externen Mailservern unsere TLS Policy bereitzustellen Dmarc-Report Roundcube Webmail Wer hat eigentlich auf welche Mailbox Zugriff? # Nutzer können Ihr Mailboxen beliebig mit anderen Nutzern teilen. Die IT hat einige Freigaben bereits eingerichtet. Mit folgendem Befehlen lassen sich diese setzen:\nBetrieb und Wartung # Wichtige Komponenten (Spam/Virus-Schutz) # Anti-Spam, Anti-Virus Rspamd # Quelle # Docker Hub\nRepository # Repo\nFunktion # Spam/Malware Filter\nMDA, IMAP, Sieve Dovecot # Quelle # Docker Hub\nRepository # Repo\nFunktion # Dovecot fungiert in diesem Setup als MDA, Sieve und IMAP Server.\nMTA Postfix # Quelle # Docker Hub\nRepository # Repo\nFunktion # Postfix ist der Mail Transfer Agent (MTA). Er nimmt E-Mails von externen Servern entgegen, leitet sie zur Prüfung an Rspamd weiter und übergibt sie anschließend zur lokalen Zustellung an Dovecot. Er ist ebenfalls für den Versand ausgehender E-Mails zuständig.\nBackup \u0026amp; Wiederherstellung # Was wird gesichert # Gesichert werden sollen lediglich Nutzerdaten. Die Konfiguration sowie das Deployment selbst sind in Git Repositories und werden extern gesichert.\nWie wird gesichert # Backup erfolgt durch VEEAM auf Fileserver Ebene. Eine Sicherung der Nodes oder PV Daten ist nicht notwendig.\nWie sieht der Restore-Prozess aus # Kommunikation: Informieren Sie die Nutzer über die geplante Downtime.\nDienste stoppen: Skalieren Sie die betroffenen Deployments im mailstack-Namespace auf 0 Replicas, um Schreibzugriffe zu verhindern.\n1kubectl scale deployment -n mailstack --replicas=0 \u0026lt;deployment-name\u0026gt; Daten wiederherstellen: Stellen Sie das Backup des Mail-Datenverzeichnisses über VEEAM auf dem Fileserver wieder her.\nBerechtigungen prüfen: Stellen Sie sicher, dass die Dateiberechtigungen (owner, group, permissions) im wiederhergestellten Verzeichnis korrekt sind.\nDienste starten: Skalieren Sie die Deployments wieder auf die ursprüngliche Anzahl an Replicas hoch.\nFunktionstest: Überprüfen Sie die Logs auf Fehler und führen Sie einen Sende- und Empfangstest durch.\nTroubleshooting # Dieser Abschnitt beschreibt die ersten Schritte zur Analyse von häufigen Problemen.\nLogs prüfen # Der erste Schritt bei Problemen ist immer die Analyse der Log-Dateien der jeweiligen Container. Annahme ist, dass die Pods im Namespace mailstack laufen.\n1# Logs von Postfix/Dovecot live verfolgen 2kubectl logs -f -n mailstack \u0026lt;postfix-dovecot-pod-name\u0026gt; 3 4# Logs von Rspamd prüfen 5kubectl logs -f -n mailstack \u0026lt;rspamd-pod-name\u0026gt; Mail-Warteschlange (Queue) prüfen # Wenn E-Mails nicht versendet werden oder sich verzögern, gibt die Postfix-Warteschlange Aufschluss über die Ursache.\n1# In den Postfix-Container wechseln 2kubectl exec -it -n mailstack \u0026lt;postfix-pod-name\u0026gt; -- /bin/bash 3 4# Warteschlange anzeigen (entspricht `postqueue -p`) 5mailq 6 7# Eine bestimmte Nachricht in der Queue genauer ansehen 8postcat -q \u0026lt;QUEUE_ID\u0026gt; Troubleshooting # Dieser Abschnitt beschreibt die ersten Schritte zur Analyse von häufigen Problemen.\nLogs prüfen # Der erste Schritt bei Problemen ist immer die Analyse der Log-Dateien der jeweiligen Container. Annahme ist, dass die Pods im Namespace mailstack laufen.\n1# Logs von Postfix/Dovecot live verfolgen 2kubectl logs -f -n mailstack \u0026lt;postfix-dovecot-pod-name\u0026gt; 3 4# Logs von Rspamd prüfen 5kubectl logs -f -n mailstack \u0026lt;rspamd-pod-name\u0026gt; Mail-Warteschlange (Queue) prüfen # Wenn E-Mails nicht versendet werden oder sich verzögern, gibt die Postfix-Warteschlange Aufschluss über die Ursache.\n1# In den Postfix-Container wechseln 2kubectl exec -it -n mailstack \u0026lt;postfix-pod-name\u0026gt; -- /bin/bash 3 4# Warteschlange anzeigen (entspricht `postqueue -p`) 5mailq 6 7# Eine bestimmte Nachricht in der Queue genauer ansehen 8postcat -q \u0026lt;QUEUE_ID\u0026gt; ","date":"12 Januar 2026","externalUrl":null,"permalink":"/posts/homelab/k3s-prod/apps/mailstack/readme/","section":"Beiträge","summary":"Dieser Mailstack integriert mehrere Komponenten wie Postfix, Dovecot, Rspamd und ClamAV, um einen robusten und sicheren E-Mail-Dienst mit Spam- und Virenfilterung bereitzustellen.","title":"Mailstack","type":"posts"},{"content":" Maintenance Page # Deployment # Quelle # Die Anwendung wird als generisches app-template Helm-Chart von BJW-S Labs bereitgestellt. Das verwendete Docker-Image ist ein benutzerdefiniertes Nginx-Image, das von git.zyria.de bezogen wird und den Inhalt der Wartungsseite aus einem Git-Repository klont.\nDokumentation # Der Quellcode der Wartungsseite befindet sich im Git-Repository pyrox/maintenance-page auf der internen Forgejo-Instanz.\nFunktion # Die Maintenance Page hat zwei Hauptfunktionen:\nCatch-All-Route: Ein Traefik IngressRoute mit niedriger Priorität (priority: 1) fängt alle Anfragen ab, für die keine spezifischere Route existiert. Diese Anfragen werden an den Nginx-Server weitergeleitet. Fehlerseite: Eine Traefik Middleware ist so konfiguriert, dass sie bei bestimmten HTTP-Fehlern (z.B. 404 Not Found) den Benutzer auf diese Wartungsseite umleitet. Der Nginx-Server selbst ist so konfiguriert, dass er für jede Anfrage einen HTTP-Statuscode 503 Service Unavailable zurückgibt und den Inhalt der index.html ausliefert.\nLokale Anpassungen # Die Konfiguration erfolgt über die values.yaml-Datei.\nWichtige Einstellungen # Git-Repository: Der Inhalt der Seite wird aus dem pages-Branch des pyrox/maintenance-page-Repositories geklont. Nginx-Konfiguration: Eine benutzerdefinierte Nginx-Konfiguration wird per ConfigMap bereitgestellt, um das 503-Verhalten zu implementieren. Traefik CRDs: Die IngressRoute und Middleware werden direkt als rawResources im Helm-Chart definiert, um die tiefe Integration mit Traefik zu ermöglichen. Installation # Die Anwendung wird mittels Kustomize und Helm durch ArgoCD im Kubernetes-Cluster bereitgestellt. Die Konfiguration befindet sich im apps/maintenance-page-Verzeichnis. Eine manuelle Installation kann mit folgendem Befehl durchgeführt werden:\n1kubectl kustomize --enable-helm apps/maintenance-page | kubectl apply -n maintenance-page -f - Abhängigkeiten # Traefik: Zwingend erforderlich, da die Kernfunktionalität auf Traefik-CRDs (IngressRoute, Middleware) basiert. Forgejo: Das Git-Repository mit dem Inhalt der Wartungsseite muss verfügbar sein. ","date":"12 Januar 2026","externalUrl":null,"permalink":"/posts/homelab/k3s-prod/apps/maintenance-page/readme/","section":"Beiträge","summary":"Dieser Dienst stellt eine generische Wartungsseite bereit, die von Traefik als Fallback für nicht erreichbare Dienste oder bei Fehlern wie 404 (Not Found) angezeigt werden kann.","title":"Maintenance Page","type":"posts"},{"content":" Mastodon # Status # Deployment # Quelle # Die Anwendung wird als generisches app-template Helm-Chart von BJW-S Labs bereitgestellt. Die verwendeten Docker-Images sind die offiziellen Mastodon-Images von ghcr.io/mastodon/mastodon.\nDokumentation # Die offizielle Dokumentation von Mastodon ist unter docs.joinmastodon.org zu finden.\nFunktion # Diese Mastodon-Instanz läuft unter der Domain kirchner.social. Sie besteht aus mehreren Prozessen, die in einem einzigen Pod als Container laufen:\nWeb: Der Ruby-on-Rails-Webserver, der die Haupt-Weboberfläche und die API bereitstellt. Streaming: Ein Node.js-Server, der Echtzeit-Updates über WebSockets an die Clients sendet. Sidekiq: Ein Hintergrundprozess-Worker, der rechenintensive Aufgaben wie das Versenden von E-Mails oder das Föderieren von Beiträgen abarbeitet. PostgreSQL: Die primäre Datenbank für alle Mastodon-Daten. Redis: Wird für Caching und die Verwaltung der Sidekiq-Warteschlangen verwendet. Lokale Anpassungen # Die Konfiguration erfolgt über Umgebungsvariablen, die in einer ConfigMap in der values.yaml-Datei definiert sind.\nWichtige Umgebungsvariablen # LOCAL_DOMAIN: Die Domain der Instanz. DB_* / REDIS_*: Konfiguration für die Verbindung zur PostgreSQL-Datenbank und zu Redis. OIDC_*: Konfiguriert die Authentifizierung über Authentik (OIDC) als Alternative zur passwortbasierten Anmeldung. VAPID_*: Schlüssel für die Web-Push-Benachrichtigungen. SECRET_KEY_BASE, OTP_SECRET: Geheime Schlüssel für die Anwendungssicherheit. Wichtige Einstellungen # Ingress: Der Zugriff auf die Weboberfläche und die Streaming-API wird über einen Ingress mit einem Hostnamen ermöglicht. Speicher: Alle persistenten Daten (PostgreSQL-DB, Redis-DB, hochgeladene Medien) werden auf einem NFS-Share gespeichert. Installation # Die Anwendung wird mittels Kustomize und Helm durch ArgoCD im Kubernetes-Cluster bereitgestellt. Die Konfiguration befindet sich im apps/mastodon-Verzeichnis. Eine manuelle Installation kann mit folgendem Befehl durchgeführt werden:\n1kubectl kustomize --enable-helm apps/mastodon | kubectl apply -n mastodon -f - Abhängigkeiten # Traefik: Wird als Ingress-Controller für den externen Zugriff benötigt. cert-manager: Wird für die automatische Bereitstellung von TLS-Zertifikaten benötigt. Authentik: Wird für die optionale Benutzerauthentifizierung via OIDC benötigt. NFS: Ein NFS-Server wird für das persistente Speichern aller Daten benötigt. ","date":"12 Januar 2026","externalUrl":null,"permalink":"/posts/homelab/k3s-prod/apps/mastodon/readme/","section":"Beiträge","summary":"Mastodon ist eine Open-Source-Alternative zu kommerziellen sozialen Netzwerken. Es ermöglicht Benutzern, eigene, unabhängige Server (Instanzen) zu betreiben, die über das ActivityPub-Protokoll miteinander kommunizieren können (Föderation).","title":"Mastodon","type":"posts"},{"content":" Matrix # Deployment # Quelle # Die Anwendung wird als generisches app-template Helm-Chart von BJW-S Labs bereitgestellt. Die verwendeten Docker-Images sind matrixdotorg/synapse für den Server und vectorim/element-web für den Client.\nDokumentation # Synapse: matrix-org.github.io/synapse Element: element.io/help Funktion # Dieser Dienst stellt eine vollständige Matrix-Infrastruktur bereit:\nSynapse: Der Homeserver, das Herzstück des Dienstes. Er speichert die Benutzerkonten, die Raum-Historie und kommuniziert mit anderen Homeservern im globalen Matrix-Netzwerk (Föderation). Er wird unter matrix.kirchner.social bereitgestellt. PostgreSQL: Die Datenbank, in der Synapse alle seine Daten speichert. Element: Ein funktionsreicher Web-Client für Matrix, der unter element.kirchner.social erreichbar ist und sich mit dem lokalen Synapse-Server verbindet. Lokale Anpassungen # Die Konfiguration erfolgt über die values.yaml-Datei sowie Konfigurationsdateien, die auf einem persistenten Volume gespeichert sind.\nWichtige Einstellungen # Ingress: Der Element-Web-Client ist unter element.kirchner.social erreichbar. Der Synapse-Homeserver ist für Client-Server- und Server-Server-Kommunikation unter matrix.kirchner.social erreichbar. Ein dedizierter EntryPoint synapse auf Port 8448 wird für die Föderation verwendet. Datenbank: Synapse verwendet eine dedizierte PostgreSQL-Datenbank, die als Sidecar-Container läuft. Speicher: Alle persistenten Daten (Synapse-Konfiguration und -Medien, PostgreSQL-DB, Element-Konfiguration) werden auf einem NFS-Share gespeichert. Die Konfiguration von Synapse (homeserver.yaml) und Element (config.json) erfolgt direkt in den auf dem Volume liegenden Dateien. Installation # Die Anwendung wird mittels Kustomize und Helm durch ArgoCD im Kubernetes-Cluster bereitgestellt. Die Konfiguration befindet sich im apps/matrix-Verzeichnis. Eine manuelle Installation kann mit folgendem Befehl durchgeführt werden:\n1kubectl kustomize --enable-helm apps/matrix | kubectl apply -n matrix -f - Abhängigkeiten # Traefik: Wird als Ingress-Controller für den externen Zugriff benötigt. Es muss ein TCP-EntryPoint für den Föderations-Port (8448) konfiguriert sein. cert-manager: Wird für die automatische Bereitstellung von TLS-Zertifikaten benötigt. NFS: Ein NFS-Server wird für das persistente Speichern aller Daten benötigt. ","date":"12 Januar 2026","externalUrl":null,"permalink":"/posts/homelab/k3s-prod/apps/matrix/readme/","section":"Beiträge","summary":"Diese Anwendung stellt einen Matrix-Homeserver (Synapse) und einen Web-Client (Element) bereit, um einen föderierten, Ende-zu-Ende-verschlüsselten Chat-Dienst zu betreiben.","title":"Matrix","type":"posts"},{"content":" Matrix Synapse Admin # Status # Deployment # Quelle # Die Anwendung wird als generisches app-template Helm-Chart von BJW-S Labs bereitgestellt. Das verwendete Docker-Image ist awesometechnologies/synapse-admin.\nDokumentation # Die Dokumentation für Synapse Admin ist auf GitHub zu finden.\nFunktion # Diese Anwendung stellt eine Weboberfläche zur Verfügung, die sich mit der Admin-API des Synapse-Homeservers verbindet. Sie ermöglicht Administratoren, Benutzer zu erstellen, zu bearbeiten und zu löschen, Räume aufzulisten und zu verwalten und andere administrative Aufgaben durchzuführen.\nLokale Anpassungen # Die Konfiguration erfolgt über die values.yaml-Datei.\nWichtige Einstellungen # Ingress: Der Zugriff auf die Weboberfläche wird über einen Ingress mit einem Hostnamen ermöglicht. Authentifizierung: Der Zugriff ist durch Authentik geschützt. Homeserver-URL: Die Anwendung ist so konfiguriert, dass sie sich ausschließlich mit dem Homeserver unter https://matrix.kirchner.social verbindet. Installation # Die Anwendung wird mittels Kustomize und Helm durch ArgoCD im Kubernetes-Cluster bereitgestellt. Die Konfiguration befindet sich im apps/matrix-admin-Verzeichnis. Eine manuelle Installation kann mit folgendem Befehl durchgeführt werden:\n1kubectl kustomize --enable-helm apps/matrix-admin | kubectl apply -n matrix-admin -f - Abhängigkeiten # Matrix Synapse: Ein laufender Synapse-Homeserver ist zwingend erforderlich. Traefik: Wird als Ingress-Controller für den externen Zugriff benötigt. Authentik: Wird zur Absicherung des Web-Frontends verwendet. cert-manager: Wird für die automatische Bereitstellung von TLS-Zertifikaten benötigt. ","date":"12 Januar 2026","externalUrl":null,"permalink":"/posts/homelab/k3s-prod/apps/matrix-admin/readme/","section":"Beiträge","summary":"Synapse Admin bietet eine einfache grafische Oberfläche, um Benutzer und Räume auf einem Synapse Homeserver zu verwalten, ohne die Admin-API direkt verwenden zu müssen.","title":"Matrix Synapse Admin","type":"posts"},{"content":" MetalLB # Deployment # Quelle # Die Anwendung wird als Helm-Chart von metallb.github.io bezogen.\nDokumentation # Die offizielle Dokumentation von MetalLB ist unter metallb.universe.tf zu finden.\nFunktion # MetalLB ist eine essenzielle Komponente für diesen Cluster. Es implementiert die Funktionalität eines Netzwerk-Load-Balancers. Wenn ein Service vom Typ LoadBalancer erstellt wird, weist MetalLB ihm eine externe IP-Adresse aus einem vordefinierten Adresspool zu. Es arbeitet im L2-Modus, was bedeutet, dass ein Node im Cluster die Verantwortung für die zugewiesene Service-IP übernimmt und den Traffic an den entsprechenden Service weiterleitet.\nLokale Anpassungen # Die Konfiguration der IP-Adresspools erfolgt nicht über dieses Helm-Chart, sondern vermutlich direkt über IPAddressPool- und L2Advertisement-Custom-Ressourcen, die separat im Cluster angewendet werden.\nWichtige Einstellungen # Namespace: metallb-system Helm Chart: metallb/metallb IP Address Pool (pool): Example IPv4 range: 192.168.10.0/24 Example IPv6 range: fc00:f853:ccd:e793::/64 L2 Advertisement: All addresses from the pool are announced via L2. Logging: The log level for the controller and speaker is set to warn. FRR: Free Range Routing (FRR) is enabled for the speaker. Installation # Die Anwendung wird mittels Kustomize und Helm durch ArgoCD im Kubernetes-Cluster bereitgestellt. Die Konfiguration befindet sich im apps/metallb-Verzeichnis. Eine manuelle Installation kann mit folgendem Befehl durchgeführt werden:\n1kubectl kustomize --enable-helm apps/metallb | kubectl apply -n metallb-system -f - Abhängigkeiten # MetalLB ist eine grundlegende Netzwerkinfrastruktur-Komponente und hat keine externen Anwendungsabhängigkeiten. Viele andere Dienste in diesem Cluster sind jedoch von MetalLB abhängig, um externe IP-Adressen zu erhalten.\n","date":"12 Januar 2026","externalUrl":null,"permalink":"/posts/homelab/k3s-prod/apps/metallb/readme/","section":"Beiträge","summary":"MetalLB ermöglicht die Verwendung von Kubernetes-Services vom Typ \u003ccode\u003eLoadBalancer\u003c/code\u003e in Clustern, die nicht bei einem Cloud-Anbieter laufen, indem es IP-Adressen aus einem konfigurierten Pool zuweist.","title":"MetalLB","type":"posts"},{"content":" Mosquitto # Deployment # Quelle # Die Anwendung wird als generisches app-template Helm-Chart von BJW-S Labs bereitgestellt. Das verwendete Docker-Image ist eclipse-mosquitto.\nDokumentation # Die offizielle Dokumentation von Mosquitto ist unter mosquitto.org/documentation/ zu finden.\nFunktion # Mosquitto ist ein MQTT-Broker, der als zentrale Vermittlungsstelle für Nachrichten zwischen Geräten fungiert. Geräte (Clients) können Nachrichten zu bestimmten Themen (Topics) veröffentlichen (Publish) oder Themen abonnieren (Subscribe), um Nachrichten zu empfangen. Dies entkoppelt die Geräte voneinander und ermöglicht eine flexible und skalierbare IoT-Architektur.\nLokale Anpassungen # Die Konfiguration erfolgt über die values.yaml-Datei, in der die mosquitto.conf direkt als ConfigMap definiert ist.\nWichtige Einstellungen # LoadBalancer: Der Broker ist über einen LoadBalancer-Service auf Port 1883 (MQTT) und 8883 (MQTT über WebSockets) im lokalen Netzwerk erreichbar. Anonymer Zugriff: Der Broker ist so konfiguriert, dass er anonyme Verbindungen ohne Benutzername/Passwort erlaubt. Bridges: Es sind mehrere MQTT-Bridges zum Cloud-Dienst von Zendure (mqtteu.zen-iot.com) konfiguriert. Diese Bridges leiten Nachrichten von spezifischen Zendure-Geräten (SolarFlow) in den lokalen MQTT-Broker weiter und umgekehrt. Persistenz: Die Mosquitto-Datenbank, die z.B. \u0026ldquo;retained messages\u0026rdquo; speichert, wird auf einem Persistent Volume gespeichert. Installation # Die Anwendung wird mittels Kustomize und Helm durch ArgoCD im Kubernetes-Cluster bereitgestellt. Die Konfiguration befindet sich im apps/mosquitto-Verzeichnis. Eine manuelle Installation kann mit folgendem Befehl durchgeführt werden:\n1kubectl kustomize --enable-helm apps/mosquitto | kubectl apply -n mosquitto -f - Abhängigkeiten # Metallb: Wird benötigt, um dem MQTT-Service eine externe IP-Adresse zuzuweisen. Longhorn/NFS: Ein Storage-Provider wird für das Persistent Volume benötigt. ","date":"12 Januar 2026","externalUrl":null,"permalink":"/posts/homelab/k3s-prod/apps/mosquitto/readme/","section":"Beiträge","summary":"Mosquitto dient als zentraler Message Broker für das MQTT-Protokoll und ermöglicht die Kommunikation zwischen verschiedenen IoT-Geräten und -Diensten im Smart Home.","title":"Mosquitto","type":"posts"},{"content":" MTA-STS Policy Server # Status # Deployment # Quelle # Die Anwendung wird als generisches app-template Helm-Chart von BJW-S Labs bereitgestellt. Das verwendete Docker-Image ist ein benutzerdefiniertes Nginx-Image von git.zyria.de.\nDokumentation # Informationen zum MTA-STS-Standard sind im RFC 8461 zu finden.\nFunktion # MTA-STS (Mail Transfer Agent Strict Transport Security) ist ein Sicherheitsstandard, der es E-Mail-Anbietern ermöglicht, zu deklarieren, dass ihre Mailserver TLS zur Sicherung von SMTP-Verbindungen verwenden. Dieser Dienst stellt die dafür notwendige Richtliniendatei (mta-sts.txt) über HTTPS bereit. Sendende Mailserver rufen diese Datei ab, bevor sie eine E-Mail zustellen, um sicherzustellen, dass die Verbindung verschlüsselt ist und um Man-in-the-Middle-Angriffe zu verhindern.\nLokale Anpassungen # Die Konfiguration erfolgt über die values.yaml-Datei und eine ConfigMap.\nWichtige Einstellungen # Policy-Datei: Der Inhalt der mta-sts.txt-Datei wird zentral in einer ConfigMap verwaltet und in den Nginx-Pod gemountet. Ingress: Ein Ingress ist so konfiguriert, dass er Anfragen für /.well-known/mta-sts.txt auf mehreren Domains (z.B. mta-sts.zyria.de, mta-sts.hx53.de) entgegennimmt und an den Nginx-Server weiterleitet. Installation # Die Anwendung wird mittels Kustomize und Helm durch ArgoCD im Kubernetes-Cluster bereitgestellt. Die Konfiguration befindet sich im apps/mta-sts-Verzeichnis. Eine manuelle Installation kann mit folgendem Befehl durchgeführt werden:\n1kubectl kustomize --enable-helm apps/mta-sts | kubectl apply -n mta-sts -f - Abhängigkeiten # Ein laufender Kubernetes-Cluster. Ein konfigurierter Ingress-Controller (z.B. Traefik). Eine Zertifikatsmanagement-Lösung (z.B. cert-manager) zur Bereitstellung von TLS-Zertifikaten. ","date":"12 Januar 2026","externalUrl":null,"permalink":"/posts/homelab/k3s-prod/apps/mta-sts/readme/","section":"Beiträge","summary":"Dieser Dienst verwendet einen Nginx-Server, um eine statische \u003ccode\u003emta-sts.txt\u003c/code\u003e-Datei unter dem \u003ccode\u003e.well-known\u003c/code\u003e-Pfad für verschiedene Domains zu veröffentlichen und so die E-Mail-Sicherheit zu erhöhen.","title":"MTA-STS Policy Server","type":"posts"},{"content":" Nextcloud # Status # Deployment # Quelle # Die Anwendung wird als generisches app-template Helm-Chart von BJW-S Labs bereitgestellt. Das verwendete Docker-Image ist ghcr.io/linuxserver/nextcloud.\nKomponenten # Diese Nextcloud-Instanz besteht aus mehreren Komponenten, die zusammenarbeiten:\nNextcloud: Die Hauptanwendung, die die Dateihosting- und Kollaborationsfunktionen bereitstellt. MariaDB: Die relationale Datenbank, die zur Speicherung der Metadaten von Nextcloud verwendet wird. Redis: Ein In-Memory-Datenspeicher, der für Caching und Dateisperren verwendet wird, um die Leistung zu verbessern. ClamAV: Eine Antiviren-Engine, die hochgeladene Dateien auf Malware scannt. PhpMyAdmin: Ein webbasiertes Verwaltungstool für die MariaDB-Datenbank, verfügbar unter /php. Dokumentation # Nextcloud Documentation MariaDB Documentation Redis Documentation ClamAV Documentation PhpMyAdmin Documentation Funktion # Nextcloud ist die zentrale Plattform für das Datei-Hosting und die Zusammenarbeit. Sie besteht aus mehreren Diensten, die in einem Pod zusammengefasst sind:\nNextcloud: Die Hauptanwendung (PHP), die die Weboberfläche und die APIs bereitstellt. MariaDB: Die Datenbank, in der alle Metadaten, Benutzerinformationen und Konfigurationen gespeichert werden. Redis: Dient als schneller In-Memory-Cache und für das Transactional File Locking, um die Leistung zu verbessern. ClamAV: Ein Virenscanner, der hochgeladene Dateien automatisch auf Malware überprüft. phpMyAdmin: Eine Weboberfläche zur Verwaltung der MariaDB-Datenbank wird ebenfalls bereitgestellt. Lokale Anpassungen # Die Konfiguration erfolgt über die values.yaml-Datei.\nWichtige Einstellungen # Ingress: Der Zugriff auf die Weboberfläche wird über einen Ingress mit einem Hostnamen ermöglicht. Speicher: Alle persistenten Daten (Benutzerdateien, Konfiguration, Datenbank, Redis-DB, ClamAV-Signaturen) werden auf einem NFS-Share gespeichert. Virenscanner: Der integrierte ClamAV-Dienst ist so konfiguriert, dass er über einen Socket mit der Nextcloud-Anwendung kommuniziert. Installation # Die Anwendung wird mittels Kustomize und Helm durch ArgoCD im Kubernetes-Cluster bereitgestellt. Die Konfiguration befindet sich im apps/nextcloud-Verzeichnis. Eine manuelle Installation kann mit folgendem Befehl durchgeführt werden:\n1kubectl kustomize --enable-helm apps/nextcloud | kubectl apply -n nextcloud -f - Abhängigkeiten # Ein laufender Kubernetes-Cluster. Ein konfigurierter Ingress-Controller (z.B. Traefik). Eine Zertifikatsmanagement-Lösung (z.B. cert-manager) zur Bereitstellung von TLS-Zertifikaten. Ein NFS-Server für persistente Speicherung. ","date":"12 Januar 2026","externalUrl":null,"permalink":"/posts/homelab/k3s-prod/apps/nextcloud/readme/","section":"Beiträge","summary":"Nextcloud ist das Gehirn des Smart Homes. Es ermöglicht die Steuerung und Automatisierung einer Vielzahl von Geräten und Diensten und stellt eine zentrale Benutzeroberfläche bereit.","title":"Nextcloud","type":"posts"},{"content":" NFS Subdir External Provisioner # Deployment # Quelle # Die Anwendung wird als Helm-Chart von Kubernetes SIGs bezogen.\nDokumentation # Die Dokumentation für den Provisioner ist auf GitHub zu finden.\nFunktion # Der NFS Subdir External Provisioner ist ein Hilfsprogramm, das die dynamische Bereitstellung von Speicher im Cluster automatisiert. Anstatt dass Administratoren manuell Persistent Volumes (PVs) auf dem NFS-Server erstellen müssen, beobachtet dieser Provisioner Persistent Volume Claims (PVCs) von Anwendungen. Wenn ein PVC eine StorageClass anfordert, die von diesem Provisioner bedient wird, erstellt er automatisch ein neues Unterverzeichnis auf dem konfigurierten NFS-Share und erstellt ein entsprechendes PV, das an dieses Verzeichnis gebunden ist.\nLokale Anpassungen # Die Konfiguration erfolgt über die values.yaml-Datei.\nWichtige Einstellungen # NFS-Server: Der Provisioner ist so konfiguriert, dass er sich mit dem NFS-Server unter einer IPv6-Adresse verbindet. NFS-Pfad: Alle persistenten Volumes werden als Unterverzeichnisse innerhalb eines bestimmten Pfades auf dem NFS-Server erstellt. Standard-StorageClass: Die von diesem Provisioner erstellte StorageClass (nfs-client) ist als defaultClass im Cluster festgelegt. Das bedeutet, dass PVCs, die keine spezifische StorageClass angeben, automatisch von diesem Provisioner bedient werden. Installation # Die Anwendung wird mittels Kustomize und Helm durch ArgoCD im Kubernetes-Cluster bereitgestellt. Die Konfiguration befindet sich im apps/nfs-provisioner-Verzeichnis. Eine manuelle Installation kann mit folgendem Befehl durchgeführt werden:\n1kubectl kustomize --enable-helm apps/nfs-provisioner | kubectl apply -n default -f - Abhängigkeiten # NFS-Server: Ein externer, funktionierender NFS-Server ist zwingend erforderlich. ","date":"12 Januar 2026","externalUrl":null,"permalink":"/posts/homelab/k3s-prod/apps/nfs-provisioner/readme/","section":"Beiträge","summary":"This document describes the deployment of the NFS Subdir External Provisioner, which enables dynamic provisioning of Persistent Volumes using an existing NFS server.","title":"NFS Subdir External Provisioner","type":"posts"},{"content":" OpenDTU # Status # Deployment # Quelle # Die Anwendung wird als generisches app-template Helm-Chart von BJW-S Labs bereitgestellt. Das verwendete Docker-Image ist ein benutzerdefiniertes Nginx-Image (pyrox/opendtu-nginx) von git.zyria.de.\nDokumentation # Die Dokumentation für das OpenDTU-Projekt ist auf GitHub zu finden.\nFunktion # Dieser Dienst dient als Backend und Frontend für ein OpenDTU-Gerät. Ein OpenDTU (Data Transfer Unit) ist ein ESP32-basiertes Gerät, das drahtlos mit Hoymiles-Solarinvertern kommuniziert und deren Leistungsdaten ausliest. Diese Daten werden an diesen Dienst gesendet, der sie in einer MariaDB-Datenbank speichert. Eine Nginx-Weboberfläche visualisiert diese Daten und ermöglicht die Überwachung der Solarstromproduktion in Echtzeit.\nLokale Anpassungen # Die Konfiguration erfolgt über die values.yaml-Datei.\nWichtige Einstellungen # Ingress: Der Zugriff auf die Weboberfläche wird über einen Ingress mit einem Hostnamen ermöglicht. Datenbank: Eine MariaDB-Datenbank läuft als Sidecar-Container zur Speicherung der Zeitreihendaten. Speicher: Die Datenbank wird persistent auf einem HostPath-Volume gespeichert. Installation # Die Anwendung wird mittels Kustomize und Helm durch ArgoCD im Kubernetes-Cluster bereitgestellt. Die Konfiguration befindet sich im apps/opendtu-Verzeichnis. Eine manuelle Installation kann mit folgendem Befehl durchgeführt werden:\n1kubectl kustomize --enable-helm apps/opendtu | kubectl apply -n opendtu -f - Abhängigkeiten # Ein laufender Kubernetes-Cluster. Ein konfigurierter Ingress-Controller (z.B. Traefik). Eine Zertifikatsmanagement-Lösung (z.B. cert-manager) zur Bereitstellung von TLS-Zertifikaten. Ein OpenDTU-Gerät, das unter der konfigurierten externen IP-Adresse läuft. ","date":"12 Januar 2026","externalUrl":null,"permalink":"/posts/homelab/k3s-prod/apps/opendtu/readme/","section":"Beiträge","summary":"Dieser Dienst stellt eine Weboberfläche bereit, die Daten von einem OpenDTU-Gerät empfängt, in einer Datenbank speichert und zur Visualisierung und Analyse anzeigt.","title":"OpenDTU","type":"posts"},{"content":" Paperless-ngx # Paperless-ngx is a document management system that transforms your physical documents into a searchable online archive. It processes your scanned documents, extracts metadata, and makes them easily accessible.\nStatus # Deployment # Komponenten # Die Anwendung wird als generisches app-template Helm-Chart von BJW-S Labs bereitgestellt. Das verwendete Docker-Image ist ghcr.io/paperless-ngx/paperless-ngx. Diese Paperless-ngx-Instanz besteht aus mehreren Komponenten:\nPaperless-ngx: Die Kernanwendung für das Dokumentenmanagement. PostgreSQL: Die relationale Datenbank zum Speichern von Dokumentenmetadaten. Redis: Wird für Caching und Aufgabenwarteschlangen verwendet. Gotenberg: Eine Docker-basierte zustandslose API zum Konvertieren verschiedener Dokumentformate in PDF, die für Dokumentenvorschauen verwendet wird. Apache Tika: Ein Toolkit zum Erkennen und Extrahieren von Metadaten und strukturiertem Textinhalt aus verschiedenen Dateitypen, das für die anfängliche Dokumentenanalyse verwendet wird. Quelle # Paperless-ngx: GitHub Repository Gotenberg: GitHub Repository Apache Tika: Apache Tika Website PostgreSQL: Official Docker Image Redis: Official Docker Image Dokumentation # Die offizielle Dokumentation von Paperless-ngx ist unter docs.paperless-ngx.com zu finden.\nPaperless-ngx Documentation Gotenberg Documentation Apache Tika Documentation PostgreSQL Documentation Redis Documentation Funktion # Paperless-ngx ist ein System zur Archivierung digitaler Dokumente. Es überwacht einen \u0026ldquo;consume\u0026rdquo;-Ordner auf neue Dateien (z.B. von einem Netzwerkscanner). Neue Dokumente werden verarbeitet:\nOCR: Der Textinhalt wird mittels optischer Zeichenerkennung (OCR) extrahiert, um das Dokument durchsuchbar zu machen. Tagging: Basierend auf dem Inhalt werden automatisch Tags, Korrespondenten und Dokumententypen zugewiesen. Archivierung: Das Originaldokument und die OCR-Daten werden gespeichert und über eine Weboberfläche zugänglich gemacht. Der Dienst besteht aus mehreren Containern:\nPaperless-ngx: Die Hauptanwendung. PostgreSQL: Die Datenbank. Redis: Message-Broker für Hintergrundaufgaben. Gotenberg \u0026amp; Tika: Dienste zur Konvertierung von Office-Dokumenten und anderen Formaten in PDFs für die Verarbeitung. Lokale Anpassungen # Die Konfiguration erfolgt über Umgebungsvariablen in der values.yaml-Datei.\nWichtige Einstellungen # Ingress: Der Zugriff auf die Weboberfläche wird über einen Ingress mit einem Hostnamen ermöglicht. Speicher: Alle persistenten Daten (Dokumente, Datenbank, etc.) werden auf einem NFS-Share gespeichert. Installation # Die Anwendung wird mittels Kustomize und Helm durch ArgoCD im Kubernetes-Cluster bereitgestellt. Die Konfiguration befindet sich im apps/paperless-Verzeichnis. Eine manuelle Installation kann mit folgendem Befehl durchgeführt werden:\n1kubectl kustomize --enable-helm apps/paperless | kubectl apply -n paperless -f - Abhängigkeiten # Ein laufender Kubernetes-Cluster. Ein konfigurierter Ingress-Controller (z.B. Traefik). Eine Zertifikatsmanagement-Lösung (z.B. cert-manager) zur Bereitstellung von TLS-Zertifikaten. Ein NFS-Server für persistente Speicherung. ","date":"12 Januar 2026","externalUrl":null,"permalink":"/posts/homelab/k3s-prod/apps/paperless/readme/","section":"Beiträge","summary":"This document describes the Paperless-ngx deployment, a document management system that transforms physical documents into searchable digital archives.","title":"Paperless-ngx","type":"posts"},{"content":" Pi-hole # Pi-hole is a Linux network-level advertisement and Internet tracker blocking application that acts as a DNS sinkhole. It protects all devices on your network from unwanted content without needing individual software installations.\nStatus # Deployment # Quelle # Die Anwendung wird als Helm-Chart von mojo2600 bereitgestellt. This Pi-hole deployment includes:\nPi-hole: The core ad-blocking and DNS server. Pi-hole Helper: A custom helper application (cn2pihole) that likely provides additional functionality or integrations for the Pi-hole instance. This helper downloads the zones entries to persist a cache in case of upstream dns failures. Dokumentation # Pi-hole: Pi-hole Official Website Pi-hole Kubernetes Helm Chart: GitHub Repository Pi-hole Helper: Private repository (git.zyria.de/pyrox/cn2pihole) Funktion # Pi-hole fungiert als DNS-Server für das lokale Netzwerk. Wenn ein Gerät eine DNS-Anfrage stellt, um eine Domain aufzulösen, prüft Pi-hole, ob diese Domain auf einer der Blocklisten steht. Wenn ja, wird die Anfrage blockiert und das Gerät kann keine Verbindung herstellen. Anfragen an legitime Domains werden an einen konfigurierten Upstream-DNS-Server (hier Google DNS) weitergeleitet. Dies blockiert Werbung und Tracker auf allen Geräten im Netzwerk, ohne dass auf jedem Gerät eine separate Software installiert werden muss.\nLokale Anpassungen # Die Konfiguration erfolgt über die values.yaml-Datei.\nWichtige Einstellungen # The deployment uses the official Pi-hole Helm chart along with a custom app-template chart for the helper. The Pi-hole service is exposed as a LoadBalancer with specific IPv4 and IPv6 addresses. Ingress is configured for the Pi-hole web interface at pihole.zyria.de. Persistent storage is configured using hostPath volumes. Patches are applied to the Pi-hole service and deployment for dual-stack support and web server port configuration. Installation # Die Anwendung wird mittels Kustomize und Helm durch ArgoCD im Kubernetes-Cluster bereitgestellt. Die Konfiguration befindet sich im apps/pihole-Verzeichnis. Eine manuelle Installation kann mit folgendem Befehl durchgeführt werden:\n1kubectl kustomize --enable-helm apps/pihole | kubectl apply -n pihole -f - Abhängigkeiten # A running Kubernetes cluster. A configured Ingress controller (e.g., Traefik). A certificate management solution (e.g., cert-manager) to provide TLS certificates, as configured in certs.yaml. MetalLB für die Zuweisung von LoadBalancer-IPs. ","date":"12 Januar 2026","externalUrl":null,"permalink":"/posts/homelab/k3s-prod/apps/pihole/pihole/","section":"Beiträge","summary":"This document describes the Pi-hole deployment, a network-wide ad blocker and DNS sinkhole.","title":"Pi-hole","type":"posts"},{"content":" Plex Media Server # Status # Deployment # Quelle # Die Anwendung wird als generisches app-template Helm-Chart von BJW-S Labs bereitgestellt. Das verwendete Docker-Image ist ghcr.io/linuxserver/plex.\nDokumentation # Die offizielle Dokumentation von Plex ist unter support.plex.tv zu finden.\nFunktion # Plex Media Server ist eine Anwendung zur Verwaltung und zum Streaming von Medien. Es scannt und organisiert Mediendateien (Filme, Fernsehsendungen, Musik, Fotos) aus lokalen Ordnern, reichert sie mit Metadaten aus dem Internet an und stellt sie den Plex-Client-Anwendungen auf einer Vielzahl von Geräten (Smart-TVs, Mobiltelefone, Webbrowser) zur Verfügung.\nLokale Anpassungen # Die Konfiguration erfolgt über die values.yaml-Datei.\nWichtige Einstellungen # LoadBalancer: Der Plex-Server ist über einen LoadBalancer-Service auf Port 32400 im lokalen Netzwerk erreichbar, um eine optimale Streaming-Leistung zu gewährleisten. Ingress: Zusätzlich ist die Weboberfläche über einen Ingress mit einem Hostnamen erreichbar. Hardware-Transcoding: Das /dev/dri-Gerät des Host-Systems wird in den Pod durchgereicht. Dies ermöglicht es Plex, die integrierte GPU (Intel Quick Sync Video) für die Hardware-beschleunigte Videotranskodierung zu nutzen, was die CPU-Last erheblich reduziert. Speicher: Die Plex-Konfiguration sowie die Medienbibliotheken (Filme, TV-Serien) werden von einem NFS-Share gemountet. Installation # Die Anwendung wird mittels Kustomize und Helm durch ArgoCD im Kubernetes-Cluster bereitgestellt. Die Konfiguration befindet sich im apps/plex-Verzeichnis. Eine manuelle Installation kann mit folgendem Befehl durchgeführt werden:\n1kubectl kustomize --enable-helm apps/plex | kubectl apply -n plex -f - Abhängigkeiten # Ein laufender Kubernetes-Cluster. Ein konfigurierter Ingress-Controller (z.B. Traefik). Eine Zertifikatsmanagement-Lösung (z.B. cert-manager) zur Bereitstellung von TLS-Zertifikaten. MetalLB für LoadBalancer IP-Zuweisung. Ein NFS-Server für persistente Speicherung. ","date":"12 Januar 2026","externalUrl":null,"permalink":"/posts/homelab/k3s-prod/apps/plex/readme/","section":"Beiträge","summary":"Dieses Dokument beschreibt die Bereitstellung des Plex Media Servers, einer beliebten Anwendung zum Organisieren und Streamen von Medien.","title":"Plex Media Server","type":"posts"},{"content":"","date":"12 Januar 2026","externalUrl":null,"permalink":"/tags/produktion/","section":"Tags","summary":"","title":"Produktion","type":"tags"},{"content":" Prometheus # Prometheus is an open-source monitoring and alerting toolkit designed for reliability and efficiency, particularly in cloud-native environments. It collects and stores metrics as time series data.\nDeployment # Components # This Prometheus deployment includes:\nPrometheus Server: The core component for scraping, storing, and querying metrics. Prometheus Node Exporter: Collects host-level metrics (CPU, memory, disk I/O, network statistics) from Kubernetes nodes. Kube-State-Metrics: Exposes metrics about the state of Kubernetes objects (e.g., deployments, pods, nodes). Prometheus Pushgateway: Ermöglicht kurzlebigen und Batch-Jobs, ihre Metriken Prometheus zur Verfügung zu stellen. Quelle # Prometheus: Prometheus Official Website Prometheus Community Helm Charts: GitHub Repository Dokumentation # Die offizielle Dokumentation von Prometheus ist unter prometheus.io/docs zu finden.\nFunktion # Prometheus ist ein Pull-basiertes Monitoring-System. Es fragt in regelmäßigen Abständen Metrik-Endpunkte von verschiedenen Zielen (targets) ab, speichert die Ergebnisse und stellt eine leistungsstarke Abfragesprache (PromQL) zur Analyse dieser Daten bereit. In diesem Cluster wird es verwendet, um Metriken von:\nKubernetes-Komponenten: API-Server, kubelet, etc. Node Exporter: Stellt Hardware- und Betriebssystem-Metriken der einzelnen Nodes bereit. Anwendungen: Viele der im Cluster laufenden Anwendungen (z.B. ArgoCD, cert-manager) stellen eigene Prometheus-kompatible Metrik-Endpunkte bereit, die automatisch über ServiceMonitor-Ressourcen erkannt werden. Der Alertmanager ist in diesem Setup deaktiviert.\nLokale Anpassungen # Die Konfiguration erfolgt über die values.yaml-Datei.\nWichtige Einstellungen # Speicher: Die Prometheus-Datenbank (TSDB) wird auf einem Persistent Volume mit einer Größe von 5 GB gespeichert, das über den nfs-client (NFS) bereitgestellt wird. Ingress: Der direkte Zugriff auf die Prometheus-Weboberfläche über einen Ingress ist derzeit deaktiviert. Die Daten werden typischerweise über ein Dashboard-Tool wie Grafana visualisiert. Komponenten: Das Helm-Chart installiert neben dem Prometheus-Server auch den prometheus-node-exporter (als DaemonSet auf jedem Node) und kube-state-metrics, um eine umfassende Überwachung des Clusters zu gewährleisten. Installation # Die Anwendung wird mittels Kustomize und Helm durch ArgoCD im Kubernetes-Cluster bereitgestellt. Die Konfiguration befindet sich im apps/prometheus-Verzeichnis. Eine manuelle Installation kann mit folgendem Befehl durchgeführt werden:\n1kubectl kustomize --enable-helm apps/prometheus | kubectl apply -n prometheus -f - Abhängigkeiten # Ein laufender Kubernetes-Cluster. Eine konfigurierte StorageClass (nfs-client in diesem Fall) für persistenten Speicher. ","date":"12 Januar 2026","externalUrl":null,"permalink":"/posts/homelab/k3s-prod/apps/prometheus/readme/","section":"Beiträge","summary":"This document describes the Prometheus deployment, an open-source monitoring and alerting toolkit.","title":"Prometheus","type":"posts"},{"content":" quaecki.de # Diese Anwendung stellt die statische Webseite für quaecki.de bereit.\nStatus # Deployment # Quelle # Die Anwendung wird als generisches app-template Helm-Chart von BJW-S Labs bereitgestellt. Das verwendete Docker-Image ist ein benutzerdefiniertes Nginx-Image, das von git.zyria.de bezogen wird.\nDokumentation # Der Quellcode der Webseite befindet sich im Git-Repository pyrox/quaecki_de auf der internen Forgejo-Instanz.\nFunktion # Dieser Dienst stellt eine einfache statische Webseite bereit. Ein Init-Container oder ein Sidecar-Container innerhalb des Pods klont den pages-Branch des Git-Repositories pyrox/quaecki_de. Der Nginx-Container dient dann als Webserver und liefert die geklonten Dateien aus.\nLokale Anpassungen # Die Konfiguration erfolgt über die values.yaml-Datei.\nWichtige Umgebungsvariablen # SITE_REPO: Die URL des Git-Repositories, das den Inhalt der Webseite enthält. SITE_BRANCH: Der Branch, der ausgecheckt werden soll (pages). Wichtige Einstellungen # Ingress: Der Zugriff auf die Webseite wird über einen Ingress mit den Hostnamen quaecki.de und www.quaecki.de ermöglicht. Installation # Die Anwendung wird mittels Kustomize und Helm durch ArgoCD im Kubernetes-Cluster bereitgestellt. Die Konfiguration befindet sich im apps/quaecki-de-Verzeichnis. Eine manuelle Installation kann mit folgendem Befehl durchgeführt werden:\n1kubectl kustomize --enable-helm apps/quaecki-de | kubectl apply -n quaecki-de -f - Abhängigkeiten # Ein laufender Kubernetes-Cluster. Ein konfigurierter Ingress-Controller (z.B. Traefik). Eine Zertifikatsmanagement-Lösung (z.B. cert-manager) zur Bereitstellung von TLS-Zertifikaten. ","date":"12 Januar 2026","externalUrl":null,"permalink":"/posts/homelab/k3s-prod/apps/quaecki-de/readme/","section":"Beiträge","summary":"This document describes the deployment of the static website for quaecki.de.","title":"quaecki.de Website","type":"posts"},{"content":" Reloader # Deployment # Quelle # Die Anwendung wird als Helm-Chart von Stakater bezogen.\nDokumentation # Die Dokumentation für Reloader ist auf GitHub zu finden.\nFunktion # Reloader ist ein nützliches Werkzeug im GitOps-Workflow. Wenn eine ConfigMap oder ein Secret aktualisiert wird, sorgt Reloader dafür, dass die Pods, die diese Ressourcen verwenden, automatisch neu gestartet werden. Ohne Reloader würden solche Pods ihre alte Konfiguration weiterverwenden, bis sie manuell neu gestartet werden. Dies wird erreicht, indem Reloader die Deployments, StatefulSets oder DaemonSets, die eine Annotation wie reloader.stakater.com/auto: \u0026quot;true\u0026quot; haben, patcht und so einen Rolling-Upgrade auslöst.\nLokale Anpassungen # Die Konfiguration erfolgt über die values.yaml-Datei.\nWichtige Einstellungen # reloadOnCreate: Ist auf true gesetzt, was bedeutet, dass Reloader auch dann einen Neustart auslöst, wenn eine zugehörige ConfigMap oder ein Secret neu erstellt wird. Installation # Die Anwendung wird mittels Kustomize und Helm durch ArgoCD im Kubernetes-Cluster bereitgestellt. Die Konfiguration befindet sich im apps/reloader-Verzeichnis. Eine manuelle Installation kann mit folgendem Befehl durchgeführt werden:\n1kubectl kustomize --enable-helm apps/reloader | kubectl apply -n reloader -f - Abhängigkeiten # Reloader ist eine eigenständige Anwendung ohne externe Abhängigkeiten.\n","date":"12 Januar 2026","externalUrl":null,"permalink":"/posts/homelab/k3s-prod/apps/reloader/readme/","section":"Beiträge","summary":"Dieses Dokument beschreibt die Bereitstellung des Reloader, eines Kubernetes-Controllers, der Pod-Neustarts bei ConfigMap- oder Secret-Updates automatisiert.","title":"Reloader","type":"posts"},{"content":" Roundcube Webmail # Roundcube Webmail is an open-source, browser-based IMAP email client known for its desktop-like user interface and extensive features. It allows users to access and manage their emails through a web browser.\nStatus # Deployment # Quelle # Die Anwendung wird als generisches app-template Helm-Chart von BJW-S Labs bereitgestellt. Das verwendete Docker-Image ist das offizielle roundcube/roundcubemail.\nDokumentation # Die offizielle Dokumentation von Roundcube ist unter roundcube.net/support zu finden.\nFunktion # Roundcube bietet einen Webmail-Zugang zum Mailstack. Benutzer können sich mit ihren E-Mail-Zugangsdaten anmelden und ihre E-Mails direkt im Browser verwalten, ohne einen dedizierten E-Mail-Client konfigurieren zu müssen.\nLokale Anpassungen # Die Konfiguration erfolgt über Umgebungsvariablen in der values.yaml-Datei.\nPersistent storage for configuration, temporary files, HTML content, and the MariaDB database is configured using hostPath volumes, pointing to an NFS share. The values.yaml file contains specific environment variables for Roundcube, including IMAP/SMTP server details and database connection settings. Custom Traefik middlewares are defined for stripping prefixes and handling redirects. Roundcube configuration files (managesieve.php, default.php) are managed as secrets. Installation # Die Anwendung wird mittels Kustomize und Helm durch ArgoCD im Kubernetes-Cluster bereitgestellt. Die Konfiguration befindet sich im apps/roundcube-Verzeichnis. Eine manuelle Installation kann mit folgendem Befehl durchgeführt werden:\n1kubectl kustomize --enable-helm apps/roundcube | kubectl apply -n roundcube -f - Abhängigkeiten # A running Kubernetes cluster. A configured Ingress controller (e.g., Traefik). Eine Zertifikatsmanagement-Lösung (z.B. cert-manager) zur Bereitstellung von TLS-Zertifikaten. An IMAP server for receiving emails. An SMTP server for sending emails. Ein NFS-Server für persistenten Speicher. ","date":"12 Januar 2026","externalUrl":null,"permalink":"/posts/homelab/k3s-prod/apps/roundcube/readme/","section":"Beiträge","summary":"This document describes the Roundcube Webmail deployment, an open-source, browser-based IMAP email client.","title":"Roundcube Webmail","type":"posts"},{"content":" Self Service Password # LTB Self-Service Password is a PHP-based web application that allows users to change or reset their passwords within an LDAP directory. It reduces the need for IT intervention in password management.\nStatus # Deployment # Quelle # Die Anwendung wird als generisches app-template Helm-Chart von BJW-S Labs bereitgestellt. Das verwendete Docker-Image ist ldaptive/self-service-password. The project is part of the LDAP Tool Box (LTB) project. The Docker image used is docker.io/ltbproject/self-service-password.\nDokumentation # Die Dokumentation für Self Service Password ist auf der Projekt-Webseite zu finden.\nFunktion # Diese Anwendung stellt eine einfache Weboberfläche zur Verfügung, auf der Benutzer ihren Benutzernamen eingeben können. Wenn der Benutzer im angebundenen LDAP-Verzeichnis gefunden wird, sendet die Anwendung eine E-Mail mit einem Token an die hinterlegte E-Mail-Adresse. Mit diesem Token kann der Benutzer dann auf einer zweiten Seite ein neues Passwort für seinen Account setzen.\nLokale Anpassungen # Die Konfiguration erfolgt über die values.yaml-Datei, in der die Konfigurationsdatei config.php als ConfigMap definiert ist.\nWichtige Einstellungen # Ingress: Der Zugriff auf die Weboberfläche wird über einen Ingress mit einem Hostnamen ermöglicht. LDAP-Konfiguration: Die config.php enthält alle notwendigen Parameter für die Verbindung zum LDAP-Server, einschließlich Host, Port, Bind-DN und Such-Basis. SMTP-Konfiguration: In der config.php sind ebenfalls die SMTP-Server-Einstellungen für den Versand der Token-E-Mails hinterlegt. Installation # Die Anwendung wird mittels Kustomize und Helm durch ArgoCD im Kubernetes-Cluster bereitgestellt. Die Konfiguration befindet sich im apps/self-service-password-Verzeichnis. Eine manuelle Installation kann mit folgendem Befehl durchgeführt werden:\n1kubectl kustomize --enable-helm apps/self-service-password | kubectl apply -n self-service-password -f - Abhängigkeiten # A running Kubernetes cluster. A configured Ingress controller (e.g., Traefik). A certificate management solution (e.g., cert-manager) to provide TLS certificates. Ein zugängliches LDAP-Verzeichnis (z.B. Active Directory). ","date":"12 Januar 2026","externalUrl":null,"permalink":"/posts/homelab/k3s-prod/apps/self-service-password/readme/","section":"Beiträge","summary":"This document describes the deployment of LTB Self-Service Password, a web application for users to manage their own passwords in an LDAP directory.","title":"Self Service Password","type":"posts"},{"content":" solarchart.de # Status # Deployment # Quelle # Die Anwendung wird als generisches app-template Helm-Chart von BJW-S Labs bereitgestellt. Das verwendete Docker-Image ist ein benutzerdefiniertes Nginx-Image, das von git.zyria.de bezogen wird.\nDokumentation # Der Quellcode der Webseite befindet sich im Git-Repository pyrox/solarchart_de auf der internen Forgejo-Instanz.\nFunktion # Dieser Dienst stellt eine einfache statische Webseite bereit. Ein Init-Container oder ein Sidecar-Container innerhalb des Pods klont den pages-Branch des Git-Repositories pyrox/solarchart_de. Der Nginx-Container dient dann als Webserver und liefert die geklonten Dateien aus.\nLokale Anpassungen # Die Konfiguration erfolgt über die values.yaml-Datei.\nWichtige Umgebungsvariablen # SITE_REPO: Die URL des Git-Repositories, das den Inhalt der Webseite enthält. SITE_BRANCH: Der Branch, der ausgecheckt werden soll (pages). Wichtige Einstellungen # Ingress: Der Zugriff auf die Webseite wird über einen Ingress mit den Hostnamen solarchart.de und www.solarchart.de ermöglicht. Installation # Die Anwendung wird mittels Kustomize und Helm durch ArgoCD im Kubernetes-Cluster bereitgestellt. Die Konfiguration befindet sich im apps/solarchart-de-Verzeichnis. Eine manuelle Installation kann mit folgendem Befehl durchgeführt werden:\n1kubectl kustomize --enable-helm apps/solarchart-de | kubectl apply -n solarchart-de -f - Abhängigkeiten # Ein laufender Kubernetes-Cluster. Ein konfigurierter Ingress-Controller (z.B. Traefik). Eine Zertifikatsmanagement-Lösung (z.B. cert-manager) zur Bereitstellung von TLS-Zertifikaten. ","date":"12 Januar 2026","externalUrl":null,"permalink":"/posts/homelab/k3s-prod/apps/solarchart-de/readme/","section":"Beiträge","summary":"Dieses Dokument beschreibt die Bereitstellung der statischen Webseite für SolarChart, die Solarprognosedaten bereitstellt.","title":"SolarChart Website","type":"posts"},{"content":" SolarFlow Topic Mapper (ACE1500) # Diese Anwendung ist ein benutzerdefinierter solarflow-topic-mapper. Sie ist dafür verantwortlich, MQTT-Themen für ein spezifisches SolarFlow-Gerät (ACE1500) abzubilden.\nStatus # Nichts zu prüfen.\nDeployment # Quelle # Der Dienst verwendet ein benutzerdefiniertes Docker-Image, das im privaten Repository auf git.zyria.de gehostet wird.\nDokumentation # Für diese benutzerdefinierte Anwendung ist keine öffentliche Dokumentation verfügbar.\nFunktion # Diese Anwendung verbindet sich mit einem SolarFlow-Gerät und bildet dessen Daten auf MQTT-Themen ab, was die Integration mit anderen Systemen ermöglicht, die MQTT-Nachrichten konsumieren.\nLokale Anpassungen # Die values.yaml-Datei enthält spezifische Umgebungsvariablen, die die Anwendung für die Verbindung mit einem bestimmten SolarFlow-Gerät und MQTT-Broker konfigurieren. Die Bereitstellung wird durch Keel automatisch auf das latest-Tag aktualisiert, wie durch die Annotationen in values.yaml definiert. Installation # Der Dienst wird als Helm-Chart unter Verwendung des app-template aus dem bjw-s-labs Repository bereitgestellt. Die Bereitstellung wird durch die kustomization.yaml-Datei verwaltet.\n1# Beispielhafter Befehl zur Bereitstellung 2kubectl kustomize --enable-helm . | kubectl apply -n ace1500 -f - Abhängigkeiten # Ein laufender Kubernetes-Cluster. Ein zugänglicher MQTT-Broker (z.B. Mosquitto), der über den internen DNS-Namen mosquitto.mosquitto.svc.cluster.local erreichbar ist. Ein SolarFlow-Gerät mit den in values.yaml angegebenen Produkt- und Geräte-IDs. ","date":"12 Januar 2026","externalUrl":null,"permalink":"/posts/homelab/k3s-prod/apps/ace1500/readme/","section":"Beiträge","summary":"Dieses Dokument beschreibt die Bereitstellung einer benutzerdefinierten SolarFlow Topic Mapper-Anwendung für ein spezifisches SolarFlow-Gerät (ACE1500).","title":"SolarFlow Topic Mapper (ACE1500)","type":"posts"},{"content":" SolarFlow Topic Mapper (SF1200) # Deployment # Quelle # Die Anwendung wird als generisches app-template Helm-Chart von BJW-S Labs bereitgestellt. Das verwendete Docker-Image pyrox/solarflow-topic-mapper ist eine Eigenentwicklung und wird von git.zyria.de bezogen.\nDokumentation # Der Quellcode für den Topic Mapper befindet sich im Git-Repository pyrox/solarflow-topic-mapper auf der internen Forgejo-Instanz.\nFunktion # Der Zendure SolarFlow sendet seine Daten über eine MQTT-Bridge an den lokalen Mosquitto-Broker. Die MQTT-Topics enthalten jedoch gerätespezifische IDs, was die Weiterverarbeitung (z.B. in Home Assistant) umständlich macht. Dieser Dienst abonniert die gerätespezifischen Topics, extrahiert die Daten und veröffentlicht sie unter neuen, sauberen und generischen Topics (z.B. solarflow/sf1200/power/solar anstatt iot/73bkTV/EYnQYD8V/properties/report). Dies vereinfacht die Integration in andere Systeme erheblich. Diese spezifische Instanz ist für das SF1200-System zuständig.\nLokale Anpassungen # Die Konfiguration erfolgt über Umgebungsvariablen in der values.yaml-Datei.\nWichtige Umgebungsvariablen # SF_PRODUCT_ID: Die Produkt-ID des SolarFlow-Geräts. SF_DEVICE_ID: Die eindeutige Geräte-ID des SolarFlow-Geräts. MQTT_HOST: Die Adresse des internen MQTT-Brokers. Installation # Die Anwendung wird mittels Kustomize und Helm durch ArgoCD im Kubernetes-Cluster bereitgestellt. Die Konfiguration befindet sich im apps/sf1200-Verzeichnis. Eine manuelle Installation kann mit folgendem Befehl durchgeführt werden:\n1kubectl kustomize --enable-helm apps/sf1200 | kubectl apply -n sf1200 -f - Abhängigkeiten # Ein laufender Kubernetes-Cluster. Ein zugänglicher MQTT-Broker (z.B. Mosquitto). Ein SolarFlow-Gerät mit den angegebenen Produkt- und Geräte-IDs. ","date":"12 Januar 2026","externalUrl":null,"permalink":"/posts/homelab/k3s-prod/apps/sf1200/readme/","section":"Beiträge","summary":"Dieser Dienst verbindet sich mit dem lokalen MQTT-Broker und mappt die spezifischen, geräteabhängigen MQTT-Topics des Zendure SolarFlow Systems auf generische, leichter verständliche Topics um.","title":"SolarFlow Topic Mapper (SF1200)","type":"posts"},{"content":" SolarFlow Topic Mapper (SF2000-2) # Deployment # Quelle # Die Anwendung wird als generisches app-template Helm-Chart von BJW-S Labs bereitgestellt. Das verwendete Docker-Image pyrox/solarflow-topic-mapper ist eine Eigenentwicklung und wird von git.zyria.de bezogen.\nDokumentation # Der Quellcode für den Topic Mapper befindet sich im Git-Repository pyrox/solarflow-topic-mapper auf der internen Forgejo-Instanz.\nFunktion # Der Zendure SolarFlow sendet seine Daten über eine MQTT-Bridge an den lokalen Mosquitto-Broker. Die MQTT-Topics enthalten jedoch gerätespezifische IDs, was die Weiterverarbeitung (z.B. in Home Assistant) umständlich macht. Dieser Dienst abonniert die gerätespezifischen Topics, extrahiert die Daten und veröffentlicht sie unter neuen, sauberen und generischen Topics (z.B. solarflow/sf2000-2/power/solar anstatt iot/A8yh63/38J249px/properties/report). Dies vereinfacht die Integration in andere Systeme erheblich. Diese spezifische Instanz ist für das zweite SF2000-System zuständig.\nLokale Anpassungen # Die Konfiguration erfolgt über Umgebungsvariablen in der values.yaml-Datei.\nWichtige Umgebungsvariablen # SF_PRODUCT_ID: Die Produkt-ID des SolarFlow-Geräts. SF_DEVICE_ID: Die eindeutige Geräte-ID des SolarFlow-Geräts. MQTT_HOST: Die Adresse des internen MQTT-Brokers. Installation # Die Anwendung wird mittels Kustomize und Helm durch ArgoCD im Kubernetes-Cluster bereitgestellt. Die Konfiguration befindet sich im apps/sf2000-2-Verzeichnis. Eine manuelle Installation kann mit folgendem Befehl durchgeführt werden:\n1kubectl kustomize --enable-helm apps/sf2000-2 | kubectl apply -n sf2000-2 -f - Abhängigkeiten # Mosquitto: Ein funktionierender MQTT-Broker ist zwingend erforderlich. Zendure SolarFlow Bridge: Die MQTT-Bridge im Mosquitto-Broker für dieses Gerät muss konfiguriert und aktiv sein. ","date":"12 Januar 2026","externalUrl":null,"permalink":"/posts/homelab/k3s-prod/apps/sf2000-2/readme/","section":"Beiträge","summary":"Dieser Dienst verbindet sich mit dem lokalen MQTT-Broker und mappt die spezifischen, geräteabhängigen MQTT-Topics des Zendure SolarFlow Systems auf generische, leichter verständliche Topics um.","title":"SolarFlow Topic Mapper (SF2000-2)","type":"posts"},{"content":" SolarFlow Topic Mapper (SF2000) # Deployment # Quelle # Die Anwendung wird als generisches app-template Helm-Chart von BJW-S Labs bereitgestellt. Das verwendete Docker-Image pyrox/solarflow-topic-mapper ist eine Eigenentwicklung und wird von git.zyria.de bezogen.\nDokumentation # Der Quellcode für den Topic Mapper befindet sich im Git-Repository pyrox/solarflow-topic-mapper auf der internen Forgejo-Instanz.\nFunktion # Der Zendure SolarFlow sendet seine Daten über eine MQTT-Bridge an den lokalen Mosquitto-Broker. Die MQTT-Topics enthalten jedoch gerätespezifische IDs, was die Weiterverarbeitung (z.B. in Home Assistant) umständlich macht. Dieser Dienst abonniert die gerätespezifischen Topics, extrahiert die Daten und veröffentlicht sie unter neuen, sauberen und generischen Topics (z.B. solarflow/sf2000/power/solar anstatt iot/A8yh63/DP9tHwb8/properties/report). Dies vereinfacht die Integration in andere Systeme erheblich. Diese spezifische Instanz ist für das erste SF2000-System zuständig.\nLokale Anpassungen # Die Konfiguration erfolgt über Umgebungsvariablen in der values.yaml-Datei.\nWichtige Umgebungsvariablen # SF_PRODUCT_ID: Die Produkt-ID des SolarFlow-Geräts. SF_DEVICE_ID: Die eindeutige Geräte-ID des SolarFlow-Geräts. MQTT_HOST: Die Adresse des internen MQTT-Brokers. Installation # Die Anwendung wird mittels Kustomize und Helm durch ArgoCD im Kubernetes-Cluster bereitgestellt. Die Konfiguration befindet sich im apps/sf2000-Verzeichnis. Eine manuelle Installation kann mit folgendem Befehl durchgeführt werden:\n1kubectl kustomize --enable-helm apps/sf2000 | kubectl apply -n sf2000 -f - Abhängigkeiten # Ein laufender Kubernetes-Cluster. Ein zugänglicher MQTT-Broker (z.B. Mosquitto). Ein SolarFlow-Gerät mit den angegebenen Produkt- und Geräte-IDs. ","date":"12 Januar 2026","externalUrl":null,"permalink":"/posts/homelab/k3s-prod/apps/sf2000/readme/","section":"Beiträge","summary":"Dieser Dienst verbindet sich mit dem lokalen MQTT-Broker und mappt die spezifischen, geräteabhängigen MQTT-Topics des Zendure SolarFlow Systems auf generische, leichter verständliche Topics um.","title":"SolarFlow Topic Mapper (SF2000)","type":"posts"},{"content":" System Upgrade Controller # The Rancher System Upgrade Controller is a Kubernetes-native upgrade controller for nodes. It introduces a new Custom Resource Definition (CRD) called \u0026ldquo;Plan\u0026rdquo; to define upgrade policies, allowing for automated and controlled upgrades of Kubernetes clusters.\nDeployment # Quelle # System Upgrade Controller GitHub Repository Rancher Dokumentation zum System Upgrade Controller Dokumentation # Die Dokumentation für den Controller ist auf GitHub zu finden.\nFunktion # Dieser Controller automatisiert die Aktualisierung des zugrundeliegenden Kubernetes-Systems (k3s). Er funktioniert, indem er Plan-Custom-Ressourcen überwacht. Ein Plan definiert, wie ein Upgrade durchgeführt werden soll, einschließlich:\nVersion: Die Zielversion von k3s. Concurrency: Wie viele Nodes gleichzeitig aktualisiert werden dürfen. Node Selector: Welche Nodes aktualisiert werden sollen (z.B. zuerst die Control-Plane, dann die Worker). Upgrade-Befehl: Das Skript, das auf dem Node ausgeführt wird, um das Upgrade durchzuführen. Wenn ein Plan im Cluster erstellt oder geändert wird, erstellt der Controller für jeden ausgewählten Node einen Job, der den Node sperrt (cordon), die Workloads darauf beendet (drain), das Upgrade-Skript ausführt und den Node anschließend wieder freigibt (uncordon).\nLokale Anpassungen # Die Konfiguration erfolgt über die values.yaml- und plans.yaml-Dateien.\nWichtige Einstellungen # plans.yaml: Diese Datei enthält die eigentlichen Upgrade-Pläne. Es gibt typischerweise separate Pläne für die Control-Plane-Nodes und die Worker-Nodes, um ein gestaffeltes Upgrade zu ermöglichen und die Cluster-Verfügbarkeit zu gewährleisten. Tolerations: Der Controller-Pod hat Tolerations für CriticalAddonsOnly, um sicherzustellen, dass er auch auf dedizierten Control-Plane-Nodes laufen kann. Installation # Die Anwendung wird mittels Kustomize und Helm durch ArgoCD im Kubernetes-Cluster bereitgestellt. Die Konfiguration befindet sich im apps/system-upgrade-controller-Verzeichnis. Eine manuelle Installation kann mit folgendem Befehl durchgeführt werden:\n1kubectl kustomize --enable-helm apps/system-upgrade-controller | kubectl apply -n system-upgrade -f - Abhängigkeiten # Der Controller ist eine eigenständige Anwendung, die tief in die Verwaltung des Kubernetes-Clusters eingreift. Er hat keine externen Anwendungsabhängigkeiten.\n","date":"12 Januar 2026","externalUrl":null,"permalink":"/posts/homelab/k3s-prod/apps/system-upgrade-controller/readme/","section":"Beiträge","summary":"This document describes the deployment of the Rancher System Upgrade Controller, a Kubernetes-native upgrade controller for nodes.","title":"System Upgrade Controller","type":"posts"},{"content":"","date":"12 Januar 2026","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":" Traefik # Traefik is a modern load balancer and reverse proxy designed as an Ingress Controller for Kubernetes. It offers dynamic configuration, automatic service discovery, and a wide range of features for managing external access to services within a cluster.\nStatus # Deployment # Quelle # Die Anwendung wird als Helm-Chart von Traefik Labs bezogen.\nDokumentation # Die offizielle Dokumentation von Traefik ist unter doc.traefik.io/traefik/ zu finden.\nTraefik GitHub Repository Funktion # Traefik ist das \u0026ldquo;Eingangstor\u0026rdquo; zum Kubernetes-Cluster. Es lauscht auf den Ports 80 und 443 auf den Cluster-Nodes und leitet eingehende HTTP/S-Anfragen basierend auf dem Hostnamen und dem Pfad an den richtigen internen Service weiter. Die Konfiguration erfolgt dynamisch: Traefik überwacht die Kubernetes-API auf Ingress- und IngressRoute-Ressourcen und passt seine Routing-Regeln automatisch an, wenn neue Dienste bereitgestellt oder entfernt werden.\nLokale Anpassungen # Die Konfiguration erfolgt über die values.yaml- und middlewares.yaml-Dateien.\nWichtige Einstellungen # Deployment-Art: Traefik wird als DaemonSet bereitgestellt. Das bedeutet, dass auf jedem Node im Cluster ein Traefik-Pod läuft, was für eine hohe Verfügbarkeit und eine gleichmäßige Verteilung des eingehenden Traffics sorgt. EntryPoints: Es sind drei EntryPoints definiert: web (Port 80): Für unverschlüsselten HTTP-Traffic (wird typischerweise auf HTTPS umgeleitet). websecure (Port 443): Für verschlüsselten HTTPS-Traffic. synapse (Port 8448): Ein dedizierter TCP-EntryPoint für die Matrix-Synapse-Föderation. Provider: Sowohl der kubernetesCRD-Provider (für IngressRoute, Middleware, etc.) als auch der kubernetesIngress-Provider (für Standard-Ingress-Ressourcen) sind aktiviert. Middlewares: In middlewares.yaml werden globale Middlewares definiert, die auf IngressRoutes angewendet werden können. Dazu gehören wahrscheinlich Middlewares für die Authentifizierung über Authentik oder für die Beschränkung des Zugriffs auf lokale Netzwerke. Persistenz: Persistenz ist aktiviert, um z.B. ACME-Zertifikate (von Let\u0026rsquo;s Encrypt) zu speichern. Installation # Die Anwendung wird mittels Kustomize und Helm durch ArgoCD im Kubernetes-Cluster bereitgestellt. Die Konfiguration befindet sich im apps/traefik-Verzeichnis. Eine manuelle Installation kann mit folgendem Befehl durchgeführt werden:\n1kubectl kustomize --enable-helm apps/traefik | kubectl apply -n kube-system -f - Abhängigkeiten # A running Kubernetes cluster. MetalLB for LoadBalancer IP allocation. Cert-manager for TLS certificate management. Authentik (wenn Forward-Authentifizierungs-Middleware verwendet wird). ","date":"12 Januar 2026","externalUrl":null,"permalink":"/posts/homelab/k3s-prod/apps/traefik/readme/","section":"Beiträge","summary":"This document describes the Traefik Ingress Controller deployment, a modern load balancer and reverse proxy for Kubernetes.","title":"Traefik Ingress Controller","type":"posts"},{"content":" Trivy Operator # Deployment # Quelle # Die Anwendung wird als Helm-Chart von Aqua Security bezogen.\nDokumentation # Die Dokumentation für den Trivy Operator ist auf der Aqua Security Webseite zu finden.\nFunktion # Der Trivy Operator automatisiert die Sicherheitsüberprüfung des Kubernetes-Clusters. Er läuft als Controller und führt folgende Aufgaben aus:\nSchwachstellenscans: Er scannt die Images aller laufenden Pods auf bekannte CVEs (Common Vulnerabilities and Exposures). Konfigurations-Audits: Er prüft die Konfiguration von Kubernetes-Ressourcen (wie Deployments, Pods, etc.) auf unsichere Einstellungen und Best-Practice-Verstöße. Die Ergebnisse dieser Scans werden als VulnerabilityReport- und ConfigAuditReport-Custom-Ressourcen im jeweiligen Namespace der gescannten Ressource gespeichert. Diese Berichte können dann von anderen Tools (z.B. Prometheus, Lens) ausgelesen und zur Überwachung der Cluster-Sicherheit verwendet werden.\nLokale Anpassungen # Die Konfiguration erfolgt über die values.yaml-Datei.\nWichtige Einstellungen # Schweregrad: Es werden nur Schwachstellen mit dem Schweregrad HIGH oder CRITICAL gemeldet. ignoreUnfixed: Ist auf true gesetzt, was bedeutet, dass nur Schwachstellen gemeldet werden, für die bereits ein Patch oder eine neue Image-Version verfügbar ist. Scanner: Sowohl der Schwachstellen-Scanner als auch der Konfigurations-Audit-Scanner sind aktiviert. Monitoring: Ein ServiceMonitor wird erstellt, um Metriken über die Scan-Ergebnisse für Prometheus bereitzustellen. Installation # Die Anwendung wird mittels Kustomize und Helm durch ArgoCD im Kubernetes-Cluster bereitgestellt. Die Konfiguration befindet sich im apps/trivy-Verzeichnis. Eine manuelle Installation kann mit folgendem Befehl durchgeführt werden:\n1kubectl kustomize --enable-helm apps/trivy | kubectl apply -n trivy-system -f - Abhängigkeiten # Prometheus: Optional, zur Erfassung und Visualisierung der Scan-Metriken. ","date":"12 Januar 2026","externalUrl":null,"permalink":"/posts/homelab/k3s-prod/apps/trivy/readme/","section":"Beiträge","summary":"Dieses Dokument beschreibt die Bereitstellung des Trivy Operators, eines Kubernetes-nativen Sicherheitstools, das Ihren Cluster kontinuierlich auf Sicherheitsprobleme scannt.","title":"Trivy Operator","type":"posts"},{"content":" UniFi Netzwerk-Anwendung # Die UniFi Netzwerk-Anwendung (ehemals UniFi Controller) ist eine von Ubiquiti entwickelte Software-Defined Networking (SDN)-Plattform. Sie dient als zentrale Verwaltungsschnittstelle für UniFi-Netzwerkgeräte, einschließlich Gateways, Switches und Wi-Fi-Zugangspunkte.\nStatus # Deployment # Komponenten # Diese UniFi Netzwerk-Anwendung-Bereitstellung umfasst:\nUniFi Netzwerk-Anwendung: Die Kernanwendung zur Verwaltung von UniFi-Geräten. MongoDB: Die Datenbank, die von der UniFi Netzwerk-Anwendung zur Speicherung ihrer Daten verwendet wird. Quelle # UniFi Netzwerk-Anwendung: Ubiquiti Official Website LinuxServer.io UniFi Netzwerk-Anwendung Image: Docker Hub MongoDB: Official Docker Image Dokumentation # Die offizielle Dokumentation von Ubiquiti UniFi ist unter help.ui.com zu finden.\nFunktion # Der UniFi Network Controller ist die zentrale Verwaltungssoftware für das im Netzwerk eingesetzte UniFi-Equipment. Er ermöglicht die Konfiguration von WLAN-Netzwerken, VLANs, Firewall-Regeln und vielem mehr. Die UniFi-Geräte im Netzwerk verbinden sich mit diesem Controller, um ihre Konfiguration zu erhalten und Statusinformationen zu melden.\nLokale Anpassungen # Persistenter Speicher für Konfigurations- und MongoDB-Daten wird über hostPath-Volumes konfiguriert, die auf einen NFS-Share zeigen. Das Deployment stellt sowohl einen Ingress für den Webzugriff als auch einen LoadBalancer-Dienst für den direkten Zugriff und die Gerätekommunikation bereit. Der LoadBalancer-Dienst ist mit spezifischen IPv4- und IPv6-Adressen unter Verwendung von MetalLB konfiguriert. Mehrere Ports sind für die UniFi-Gerätekommunikation freigegeben (STUN, Gerätekommunikation, Webadministration, AP-Erkennung, L2-Netzwerk). Eine benutzerdefinierte mongoinit ConfigMap wird verwendet, um die MongoDB-Datenbank mit einem bestimmten Benutzer zu initialisieren. Wichtige Einstellungen # Service-Typ: Der Controller wird über einen LoadBalancer-Service mit einer dedizierten IP-Adresse im lokalen Netzwerk bereitgestellt. Dies ist notwendig, damit die UniFi-Geräte den Controller zuverlässig finden können (über den \u0026ldquo;Inform Host\u0026rdquo;). Ingress: Zusätzlich ist die Weboberfläche über einen Ingress mit einem Hostnamen erreichbar. Zeitzone: Die Zeitzone ist auf Europe/Berlin gesetzt. Speicher: Die Konfiguration des Controllers wird auf einem bestehenden Persistent Volume Claim (unifi-controller-config) gespeichert. Installation # Die Anwendung wird mittels Kustomize und Helm durch ArgoCD im Kubernetes-Cluster bereitgestellt. Die Konfiguration befindet sich im apps/unifi-controller-Verzeichnis. Eine manuelle Installation kann mit folgendem Befehl durchgeführt werden:\n1kubectl kustomize --enable-helm apps/unifi-controller | kubectl apply -n unifi-controller -f - Abhängigkeiten # Ein laufender Kubernetes-Cluster. Ein konfigurierter Ingress-Controller (z.B. Traefik). Eine Zertifikatsmanagement-Lösung (z.B. cert-manager) zur Bereitstellung von TLS-Zertifikaten. MetalLB für die Zuweisung von LoadBalancer-IPs. Ein NFS-Server für persistenten Speicher. ","date":"12 Januar 2026","externalUrl":null,"permalink":"/posts/homelab/k3s-prod/apps/unifi-controller/readme/","section":"Beiträge","summary":"Dieses Dokument beschreibt die Bereitstellung der UniFi Netzwerk-Anwendung, einer Software-Defined Networking (SDN)-Plattform zur Verwaltung von UniFi-Geräten.","title":"UniFi Netzwerk-Anwendung","type":"posts"},{"content":" Uptime Kuma # Uptime Kuma is a self-hosted, open-source monitoring tool that allows you to track the uptime and performance of various services. It can monitor HTTP/HTTPS websites, TCP ports, Ping, DNS records, SSL certificates, databases, Docker containers, and more.\nDeployment # Quelle # The project is based on the Uptime Kuma GitHub Repository. The Docker image used is docker.io/louislam/uptime-kuma.\nDokumentation # Uptime Kuma Official Website Uptime Kuma GitHub Repository Funktion # Uptime Kuma wird verwendet, um die Erreichbarkeit der verschiedenen im Cluster und im Netzwerk betriebenen Dienste zu überwachen. Es prüft in regelmäßigen Abständen, ob die konfigurierten Endpunkte (z.B. Webseiten, APIs) erreichbar sind und korrekt antworten. Die Ergebnisse werden auf einer öffentlichen Statusseite unter status.zyria.de angezeigt. Diese Statusseite verwendet auch die Badges, die in den anderen README.md-Dateien dieses Projekts eingebunden sind. Bei Ausfällen kann Uptime Kuma Benachrichtigungen über verschiedene Kanäle (z.B. E-Mail, Matrix) versenden.\nLokale Anpassungen # Die Konfiguration erfolgt über die values.yaml-Datei.\nWichtige Einstellungen # Ingress: Der Zugriff auf die Weboberfläche und die Statusseite wird über einen Ingress mit dem Hostnamen status.zyria.de ermöglicht. Speicher: Die Konfiguration von Uptime Kuma (einschließlich aller Monitore und Benachrichtigungseinstellungen) wird auf einem bestehenden Persistent Volume Claim (uptime-kuma-config) gespeichert. Installation # Die Anwendung wird mittels Kustomize und Helm durch ArgoCD im Kubernetes-Cluster bereitgestellt. Die Konfiguration befindet sich im apps/uptime-kuma-Verzeichnis. Eine manuelle Installation kann mit folgendem Befehl durchgeführt werden:\n1kubectl kustomize --enable-helm apps/uptime-kuma | kubectl apply -n uptime-kuma -f - Abhängigkeiten # A running Kubernetes cluster. A configured Ingress controller (e.g., Traefik). A certificate management solution (e.g., cert-manager) to provide TLS certificates, as configured in certs.yaml. Ein NFS-Server für persistenten Speicher. ","date":"12 Januar 2026","externalUrl":null,"permalink":"/posts/homelab/k3s-prod/apps/uptime-kuma/readme/","section":"Beiträge","summary":"This document describes the Uptime Kuma deployment, a self-hosted, open-source monitoring tool for tracking service uptime and performance.","title":"Uptime Kuma","type":"posts"},{"content":" whoami # Status # Deployment # Quelle # Die Anwendung wird als einfaches Deployment mit dem offiziellen traefik/whoami-Image bereitgestellt.\nDokumentation # Die Dokumentation für das whoami-Image ist auf Docker Hub und GitHub zu finden.\nFunktion # Dieser Dienst dient als einfaches \u0026ldquo;Echo\u0026rdquo; für HTTP-Anfragen. Er ist nützlich, um zu überprüfen, ob der Ingress-Controller (Traefik) korrekt funktioniert, ob Anfragen richtig weitergeleitet werden und welche Header (z.B. X-Forwarded-For) vom Proxy gesetzt werden.\nLokale Anpassungen # Die Anwendung wird direkt über Kubernetes-Manifeste (app.yaml) und nicht über ein Helm-Chart bereitgestellt.\nWichtige Einstellungen # IngressRoute: Der Dienst ist über eine Traefik IngressRoute unter dem Hostnamen whoami.zyria.de erreichbar. Zertifikat: Ein Zertifikat für die Domain wird automatisch von cert-manager ausgestellt. Installation # Die Anwendung wird durch ArgoCD im Kubernetes-Cluster bereitgestellt. Die Konfiguration befindet sich im apps/whoami-Verzeichnis. Eine manuelle Installation kann mit folgendem Befehl durchgeführt werden:\n1kubectl apply -k apps/whoami Abhängigkeiten # A running Kubernetes cluster. Ein konfigurierter Ingress-Controller (z.B. Traefik). ","date":"12 Januar 2026","externalUrl":null,"permalink":"/posts/homelab/k3s-prod/apps/whoami/readme/","section":"Beiträge","summary":"whoami ist ein minimaler Webserver, der HTTP-Request-Header, Hostname und IP-Adresse des Pods anzeigt. Er wird hauptsächlich für Debugging-Zwecke im Kontext von Ingress und Netzwerk-Routing verwendet.","title":"whoami","type":"posts"},{"content":" WireGuard VPN Server # WireGuard is a streamlined VPN protocol designed for speed, security, and simplicity. This deployment provides a WireGuard VPN server, allowing secure and private network access.\nStatus # Deployment # Quelle # Die Anwendung wird als benutzerdefiniertes Helm-Chart aus einem privaten Repository auf git.zyria.de bezogen. Es basiert vermutlich auf dem wg-easy Projekt, wurde aber stark erweitert.\nDokumentation # Die Dokumentation für die verwendete Web-UI ist auf GitHub zu finden.\nWireGuard Documentation Funktion # Dieser Dienst bietet einen zentralen und einfach zu verwaltenden VPN-Zugang zum Netzwerk.\nWireGuard-Server: Der Kern des Dienstes, der den verschlüsselten VPN-Tunnel bereitstellt. Web-Portal: Eine Weboberfläche unter vpn.zyria.de, auf der sich Benutzer anmelden können. LDAP-Integration: Benutzer werden gegen ein Active Directory authentifiziert. Nur Mitglieder einer bestimmten Gruppe (wireguardEnabled) dürfen sich anmelden. Self-Provisioning: Nach der Anmeldung können Benutzer ihre eigenen VPN-Clients (Peers) erstellen, QR-Codes zum einfachen Einrichten auf Mobilgeräten scannen und Konfigurationsdateien herunterladen. Bastion Host: Ein integrierter SSH-Server, der als sicherer Sprung-Host ins Netzwerk dienen kann. Lokale Anpassungen # The deployment uses a custom Helm chart tailored for this specific WireGuard setup. It integrates with an LDAP directory for user authentication and dynamic peer configuration. It exposes a LoadBalancer service for the VPN tunnel (UDP) and SSH access (TCP). Persistent storage for WireGuard configuration and portal data is configured using hostPath volumes. SSH host keys are mounted from host paths. A custom sshd-config is applied for the SSH bastion. Der Bastion Container stellt einen SSH Jumphost bereit. Primär für die Nutzung von Ansible. Damit lassen sich alle Geräte hinter den NATs erreichen, welche im VPN sind. Wg-portal stellt eine leichte Möglichkeit bereit den Mitarbeitern vordefinierte Zugänge zum VPN bereitzustellen. Der Container nutzt das Netzwerk des wireguard Containers und muss deshalb mit ihm zusammen neu gestartet werden. Die Konfiguration erfolgt über Umgebungsvariablen in der values.yaml-Datei.\nWichtige Umgebungsvariablen # LDAP_URL, LDAP_BASEDN, LDAP_USER, LDAP_PASSWORD: Konfigurieren die Verbindung zum Active Directory. LDAP_SYNC_FILTER: Definiert, welche Benutzer Zugriff auf den VPN-Dienst haben. SELF_PROVISIONING: Ist auf true gesetzt, um Benutzern die eigenständige Verwaltung ihrer Geräte zu ermöglichen. MAIL_FROM, EMAIL_HOST: Konfigurieren den E-Mail-Versand für Benachrichtigungen. Wichtige Einstellungen # LoadBalancer: Der WireGuard-VPN-Port (51194/UDP) und der SSH-Port (3022/TCP) werden über einen LoadBalancer-Service mit einer dedizierten externen IP-Adresse bereitgestellt. Ingress: Der Zugriff auf das Web-Portal wird über einen Ingress mit einem Hostnamen ermöglicht. Speicher: Die WireGuard-Konfigurationen und die Daten des Web-Portals werden persistent auf einem NFS-Share gespeichert. Installation # Die Anwendung wird mittels Kustomize und Helm durch ArgoCD im Kubernetes-Cluster bereitgestellt. Die Konfiguration befindet sich im apps/wireguard-Verzeichnis. Eine manuelle Installation kann mit folgendem Befehl durchgeführt werden:\n1kubectl kustomize --enable-helm apps/wireguard | kubectl apply -n wireguard -f - Abhängigkeiten # A running Kubernetes cluster. A configured Ingress controller (e.g., Traefik). A certificate management solution (e.g., cert-manager) to provide TLS certificates. MetalLB for LoadBalancer IP allocation. An accessible LDAP directory for user authentication. Ein NFS-Server für persistenten Speicher. ","date":"12 Januar 2026","externalUrl":null,"permalink":"/posts/homelab/k3s-prod/apps/wireguard/readme/","section":"Beiträge","summary":"This document describes the WireGuard VPN Server deployment, a modern, fast, and secure VPN protocol.","title":"WireGuard VPN Server","type":"posts"},{"content":" Zammad # Status # Deployment # Quelle # Die Anwendung wird als Helm-Chart von Zammad bezogen.\nDokumentation # Die offizielle Dokumentation von Zammad ist unter docs.zammad.org zu finden.\nZammad GitHub Repository Funktion # Zammad ist ein leistungsstarkes System zur Verwaltung von Kundenkommunikation. Es ermöglicht Support-Teams, Anfragen zentral zu erfassen, zu priorisieren, zuzuweisen und zu beantworten. Das System besteht aus mehreren Komponenten, die vom Helm-Chart verwaltet werden, darunter:\nZammad Rails Server: Die Hauptanwendung. Zammad Websocket Server: Für Echtzeit-Kommunikation in der UI. Zammad Scheduler: Für Hintergrundaufgaben. PostgreSQL: Die primäre Datenbank. Elasticsearch: Für eine schnelle und leistungsfähige Volltextsuche in Tickets und Artikeln. Lokale Anpassungen # Die Konfiguration erfolgt über die values.yaml-Datei und Patches in der kustomization.yaml.\nWichtige Einstellungen # Ingress: Der Zugriff auf die Weboberfläche wird über einen Ingress mit dem Hostnamen service.zyria.de ermöglicht. Ressourcen: Sowohl für PostgreSQL als auch für Elasticsearch sind hohe Ressourcenanforderungen (2xlarge) voreingestellt, um eine gute Leistung zu gewährleisten. Datenbank-Initialisierung: Ein initdb-Skript wird über einen Patch in den PostgreSQL-Pod gemountet, um die Datenbank bei der ersten Erstellung zu initialisieren. Auto-Wizard: Der Einrichtungs-Assistent, der beim ersten Start erscheint, ist deaktiviert, da die Konfiguration über andere Wege erfolgt. 1Setting.set(\u0026#39;system_bcc\u0026#39;, \u0026#39;service+Sent@casa-due-pur.de\u0026#39;) 2Setting.set(\u0026#39;ui_ticket_create_notes\u0026#39;, { 3 :\u0026#34;phone-in\u0026#34;=\u0026gt;\u0026#34;Du erstellst einen eingehenden Anruf.\u0026#34;, 4 :\u0026#34;phone-out\u0026#34;=\u0026gt;\u0026#34;Du erstellst einen ausgehenden Anruf.\u0026#34;, 5 :\u0026#34;email-out\u0026#34;=\u0026gt;\u0026#34;Du wirst eine E-Mail senden.\u0026#34; 6 }) 7Setting.set(\u0026#39;ui_ticket_add_article_hint\u0026#39;, { 8 :\u0026#34;note-internal\u0026#34;=\u0026gt;\u0026#34;Du erstellst eine INTERNE Notiz. Nur andere Mitarbeiter werden diese sehen können.\u0026#34;, 9 :\u0026#34;note-public\u0026#34;=\u0026gt;\u0026#34;Du erstellst eine öffentlich Notiz. Der Kunde kann diese unter Umständen sehen.\u0026#34;, 10 :\u0026#34;phone-internal\u0026#34; =\u0026gt; \u0026#34;Du erstellst eine INTERNE Telefonnotiz. Nur andere Mitarbeiter werden diese sehen können.\u0026#34;, 11 :\u0026#34;phone-public\u0026#34;=\u0026gt;\u0026#34;Du erstellst eine öffentliche Telefonnotiz. Der Kunde kann diese unter Umständen sehen.\u0026#34;, 12 :\u0026#34;email-internal\u0026#34; =\u0026gt; \u0026#34;Du erstellst eine externe Mail. Der Kunde kann diese sehen.\u0026#34;, 13 :\u0026#34;email-public\u0026#34;=\u0026gt;\u0026#34;Du erstellst eine externe Email. Der Kunde kann diese sehen.\u0026#34; 14 }) 15Setting.set(\u0026#39;ui_user_organization_selector_with_email\u0026#39;, true) 16 17 18Setting.set(\u0026#39;ui_sidebar_open_ticket_indicator_colored\u0026#39;, true) Installation # Die Anwendung wird mittels Kustomize und Helm durch ArgoCD im Kubernetes-Cluster bereitgestellt. Die Konfiguration befindet sich im apps/zammad-Verzeichnis. Eine manuelle Installation kann mit folgendem Befehl durchgeführt werden:\n1kubectl kustomize --enable-helm apps/zammad | kubectl apply -n zammad -f - Abhängigkeiten # A running Kubernetes cluster. A configured Ingress controller (e.g., Traefik). A certificate management solution (e.g., cert-manager) to provide TLS certificates, as configured in certs.yaml. Ein NFS-Server für persistenten Speicher (für das initdb-Volume). ","date":"12 Januar 2026","externalUrl":null,"permalink":"/posts/homelab/k3s-prod/apps/zammad/readme/","section":"Beiträge","summary":"This document describes the Zammad Helpdesk deployment, an open-source, web-based helpdesk and customer support system.","title":"Zammad Helpdesk","type":"posts"},{"content":" zyria.de Webseite # Diese Anwendung stellt die statische Webseite für zyria.de bereit. Der Inhalt wird aus einem Git-Repository mittels git-sync synchronisiert.\nStatus # Deployment # Quelle # Die Anwendung wird als generisches app-template Helm-Chart von BJW-S Labs bereitgestellt. Es wird ein Webserver (Nginx) installiert, welcher eine statische Seite ausliefert.\nDokumentation # Der Quellcode der Webseite befindet sich im Git-Repository pyrox/zyria_de auf der internen Forgejo-Instanz.\nNginx Official Website: https://nginx.org/ Git-Sync GitHub Repository: https://github.com/kubernetes/git-sync Funktion # Dieser Dienst stellt die Webseite zyria.de mithilfe des Git-Sync-Musters bereit. Er besteht aus zwei Containern, die sich ein Volume teilen:\ngit-sync: Dieser Sidecar-Container klont in regelmäßigen Abständen (alle 60 Sekunden) den pages-Branch des pyrox/zyria_de-Git-Repositories in ein geteiltes Volume. Nginx: Der Hauptcontainer, der als Webserver dient und die vom git-sync-Container bereitgestellten Dateien aus dem geteilten Volume ausliefert. Dieses Muster stellt sicher, dass Änderungen an der Webseite, die in das Git-Repository gepusht werden, automatisch und ohne Neustart des Pods live geschaltet werden.\nLokale Anpassungen # Die Konfiguration erfolgt über die values.yaml-Datei.\nWichtige Einstellungen # Git-Sync: Die Startargumente des gitsync-Containers definieren das Repository, den Branch und das Synchronisationsintervall. Ingress: Der Zugriff auf die Webseite wird über einen Ingress mit den Hostnamen zyria.de und www.zyria.de ermöglicht. Speicher: Ein emptyDir-Volume wird verwendet, um die geklonten Dateien zwischen den beiden Containern zu teilen. Die Daten sind nicht persistent über Pod-Neustarts hinweg, werden aber bei jedem Start neu geklont. Installation # Die Anwendung wird mittels Kustomize und Helm durch ArgoCD im Kubernetes-Cluster bereitgestellt. Die Konfiguration befindet sich im apps/zyria-de-Verzeichnis. Eine manuelle Installation kann mit folgendem Befehl durchgeführt werden:\n1kubectl kustomize --enable-helm apps/zyria-de | kubectl apply -n zyria-de -f - Abhängigkeiten # Ein laufender Kubernetes-Cluster. Ein konfigurierter Ingress-Controller (z.B. Traefik). Eine Zertifikatsmanagement-Lösung (z.B. cert-manager) zur Bereitstellung von TLS-Zertifikaten. ","date":"12 Januar 2026","externalUrl":null,"permalink":"/posts/homelab/k3s-prod/apps/zyria-de/readme/","section":"Beiträge","summary":"Dieses Dokument beschreibt die Bereitstellung der statischen Webseite für zyria.de, deren Inhalt aus einem Git-Repository synchronisiert wird.","title":"zyria.de Webseite","type":"posts"},{"content":"Wie man Debian zuverlässig auf den Stand bringt. Danke an doku.lrz.de die dies bei jedem Release aufschreiben.\nDebian stellt auch eine Anleitung zur Verfügung.\nProxmox hat für jedes Release einen Wiki Artikel. Auf für den Proxmox Backup Server.\nVorbereitungen # 1apt update 2apt full-upgrade Paketquellen aktualisieren # Versionen # Versionsnummer Versionsname Replace 9 Stretch :%s/stretch/buster/ 10 buster :%s/buster/bullseye/ 11 bullseye :%s/bullseye/bookworm/ 12 bookworm :%s/bookworm/trixie/ Default sources.list # 1cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/apt/sources.list 2deb http://deb.debian.org/debian trixie main contrib non-free non-free-firmware 3deb http://deb.debian.org/debian trixie-updates main contrib non-free non-free-firmware 4deb http://deb.debian.org/debian-security/ trixie-security main contrib non-free non-free-firmware 5EOF Alle Sources ändern # 1sed -i \u0026#39;s:bookworm:trixie:\u0026#39; /etc/apt/sources.list 2sed -i \u0026#39;s:bookworm:trixie:\u0026#39; /etc/apt/sources.list.d/*.list Apt Preferences prüfen # /etc/apt/preferences.d/*\n/etc/apt/preferences\nUpgrade durchführen # 1apt update 2 3# Möglicher Zwischenschritt 4# apt upgrade --without-new-pkgs 5 6apt upgrade 7 8 9# Möglicher Zwischenschritt 10# apt full-upgrade 11 12apt dist-upgrade Netzwerkinterface prüfen # 1udevadm test-builtin net_setup_link /sys/class/net/enp1s0 Neues sources Format realisieren # 1apt modernize-sources 2 3apt update 4 5rm /etc/apt/sources.list.bak /etc/apt/sources.list.d/*.bak Nach dem Update # 1systemctl reboot 2systemctl -a --failed 3journalctl -b -p notice Übersprungene neue Konfigurationen prüfen # 1find /etc -name \u0026#34;*.dpkg-dist\u0026#34; Pakete aufräumen # 1apt autoremove Per apt oder aptitude Pakete aufräumen\n","date":"13 Dezember 2025","externalUrl":null,"permalink":"/posts/homelab/debian-upgrade/","section":"Beiträge","summary":"\u003cp\u003eWie man Debian zuverlässig auf den Stand bringt. Danke an \u003ca\n  href=\"https://doku.lrz.de/upgrade-auf-debian-13-trixie-1921298568.html\"\n    target=\"_blank\"\n  \u003edoku.lrz.de\u003c/a\u003e die dies bei jedem Release aufschreiben.\u003c/p\u003e\n\u003cp\u003eDebian stellt auch eine \u003ca\n  href=\"https://www.debian.org/releases/stable/release-notes/upgrading.de.html\"\n    target=\"_blank\"\n  \u003eAnleitung\u003c/a\u003e zur Verfügung.\u003c/p\u003e\n\u003cp\u003eProxmox hat für jedes Release einen \u003ca\n  href=\"https://pve.proxmox.com/wiki/Category:Upgrade\"\n    target=\"_blank\"\n  \u003eWiki\u003c/a\u003e Artikel. Auf für den \u003ca\n  href=\"https://pbs.proxmox.com/wiki/Category:Upgrade\"\n    target=\"_blank\"\n  \u003eProxmox Backup Server\u003c/a\u003e.\u003c/p\u003e\n\n\u003ch2 class=\"relative group\"\u003eVorbereitungen\n    \u003cdiv id=\"vorbereitungen\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#vorbereitungen\" aria-label=\"Anker\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"ln\"\u003e1\u003c/span\u003e\u003cspan class=\"cl\"\u003eapt update\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"ln\"\u003e2\u003c/span\u003e\u003cspan class=\"cl\"\u003eapt full-upgrade\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch2 class=\"relative group\"\u003ePaketquellen aktualisieren\n    \u003cdiv id=\"paketquellen-aktualisieren\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#paketquellen-aktualisieren\" aria-label=\"Anker\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\n\u003ch3 class=\"relative group\"\u003eVersionen\n    \u003cdiv id=\"versionen\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#versionen\" aria-label=\"Anker\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eVersionsnummer\u003c/th\u003e\n          \u003cth\u003eVersionsname\u003c/th\u003e\n          \u003cth\u003eReplace\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e9\u003c/td\u003e\n          \u003ctd\u003eStretch\u003c/td\u003e\n          \u003ctd\u003e:%s/stretch/buster/\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e10\u003c/td\u003e\n          \u003ctd\u003ebuster\u003c/td\u003e\n          \u003ctd\u003e:%s/buster/bullseye/\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e11\u003c/td\u003e\n          \u003ctd\u003ebullseye\u003c/td\u003e\n          \u003ctd\u003e:%s/bullseye/bookworm/\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e12\u003c/td\u003e\n          \u003ctd\u003ebookworm\u003c/td\u003e\n          \u003ctd\u003e:%s/bookworm/trixie/\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003ch3 class=\"relative group\"\u003eDefault sources.list\n    \u003cdiv id=\"default-sourceslist\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#default-sourceslist\" aria-label=\"Anker\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"ln\"\u003e1\u003c/span\u003e\u003cspan class=\"cl\"\u003ecat \u003cspan class=\"s\"\u003e\u0026lt;\u0026lt;EOF \u0026gt; /etc/apt/sources.list\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"ln\"\u003e2\u003c/span\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s\"\u003edeb http://deb.debian.org/debian trixie main contrib non-free non-free-firmware\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"ln\"\u003e3\u003c/span\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s\"\u003edeb http://deb.debian.org/debian trixie-updates main contrib non-free non-free-firmware\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"ln\"\u003e4\u003c/span\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s\"\u003edeb http://deb.debian.org/debian-security/ trixie-security main contrib non-free non-free-firmware\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"ln\"\u003e5\u003c/span\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s\"\u003eEOF\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch3 class=\"relative group\"\u003eAlle Sources ändern\n    \u003cdiv id=\"alle-sources-ändern\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#alle-sources-%c3%a4ndern\" aria-label=\"Anker\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"ln\"\u003e1\u003c/span\u003e\u003cspan class=\"cl\"\u003esed -i \u003cspan class=\"s1\"\u003e\u0026#39;s:bookworm:trixie:\u0026#39;\u003c/span\u003e /etc/apt/sources.list\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"ln\"\u003e2\u003c/span\u003e\u003cspan class=\"cl\"\u003esed -i \u003cspan class=\"s1\"\u003e\u0026#39;s:bookworm:trixie:\u0026#39;\u003c/span\u003e /etc/apt/sources.list.d/*.list\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch3 class=\"relative group\"\u003eApt Preferences prüfen\n    \u003cdiv id=\"apt-preferences-prüfen\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#apt-preferences-pr%c3%bcfen\" aria-label=\"Anker\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h3\u003e\n\u003cp\u003e/etc/apt/preferences.d/*\u003c/p\u003e","title":"Debian Upgrade","type":"posts"},{"content":"Hier Sammle ich Snippets, welche mir das Tippen ersparen sollen und für Kubernetes öfter mal gebraucht werden.\nLösche alte ReplicaSets # 1 2kubectl get rs --all-namespaces -o wide | awk \u0026#39;$3==0 \u0026amp;\u0026amp; $4==0 \u0026amp;\u0026amp; $5==0 {print \u0026#34;-n\u0026#34;, $1, $2}\u0026#39; | xargs -L1 kubectl delete rs OCI Helm Chart in Forgejo importieren # 1helm pull oci://codeberg.org/wrenix/helm-charts/forgejo-runner --version 0.6.6 2helm push *.tgz oci://git.zyria.de/pyrox 3helm cm-push *.tgz zyria Finalizer löschen # 1kubectl get linstorsatellites.piraeus.io -o name | sed -e \u0026#39;s/.*\\///g\u0026#39; | xargs -I {} kubectl patch linstorsatellites.piraeus.io {} -p \u0026#39;{\u0026#34;metadata\u0026#34;: {\u0026#34;finalizers\u0026#34;: null}}\u0026#39; --type merge Update Postgres mit Podman/Docker # 1podman run --rm --name pgauto -it \\ 2 --mount type=bind,source=./,target=/var/lib/postgresql/data \\ 3 -e POSTGRES_PASSWORD=password \\ 4 -e PGAUTO_ONESHOT=yes \\ 5 -e PGAUTO_REINDEX=no \\ 6 pgautoupgrade/pgautoupgrade:16-bookworm Zeige alle Nodes mit dem Label performance # 1kubectl get nodes -o=jsonpath=\u0026#34;{range .items[*]}{.metadata.name}: {.metadata.labels.performance}{\u0026#39;\\n\u0026#39;}{end}\u0026#34; Lösche alle Events # 1kubectl delete events --all -A Lösche alle nicht gebundenen Images # 1/usr/local/bin//k3s crictl rmi --prune ","date":"18 November 2025","externalUrl":null,"permalink":"/posts/homelab/kubernets/","section":"Beiträge","summary":"\u003cp\u003eHier Sammle ich Snippets, welche mir das Tippen ersparen sollen und für Kubernetes öfter mal gebraucht werden.\u003c/p\u003e","title":"Kubernetes Snippets","type":"posts"},{"content":"","date":"4 November 2025","externalUrl":null,"permalink":"/tags/windows/","section":"Tags","summary":"","title":"Windows","type":"tags"},{"content":" Anleitung: Neuen Windows-Rechner in Active Directory (AD) aufnehmen # Diese Anleitung beschreibt den Prozess, wie man einen fabrikneuen Windows-Rechner (im \u0026ldquo;Out-of-Box Experience\u0026rdquo;-Modus, OOBE) einrichtet, die erzwungene Anmeldung mit einem Microsoft-Konto umgeht und den Rechner anschließend in eine lokale Active Directory-Domäne aufnimmt.\nVoraussetzungen # Physischer Zugriff auf den neuen Rechner. Ein Netzwerkkabel (LAN) mit Verbindung zum lokalen Netzwerk, in dem sich der Domänencontroller (DC) befindet. Der vollqualifizierte Domänenname (FQDN) der Domäne (z. B. meinefirma.local). Ein Active Directory-Benutzerkonto mit der Berechtigung, Computer zur Domäne hinzuzufügen (z. B. ein Domänen-Admin-Konto). Die IP-Adresse des Domänencontrollers, da dieser auch als DNS-Server fungieren muss. Bios prüfen # Prüfe alle Bioseinstellungen Gehe alle Einstellungen durch und passe so notwendig an. Dokumentiere іm Inventar alle Änderungen. Teil 1: Windows-Ersteinrichtung (OOBE) ohne Microsoft-Konto # Das Ziel ist, die Ersteinrichtung mit einem lokalen Administratorkonto abzuschließen. Moderne Windows-Versionen (besonders Home/Pro) versuchen, die Erstellung eines Microsoft-Kontos zu erzwingen, wenn eine Internetverbindung besteht.\nMethode A: Der \u0026ldquo;OOBE\\BYPASSNRO\u0026rdquo;-Befehl (Empfohlen für Win 11) # Dies ist die zuverlässigste Methode, um die Option \u0026ldquo;Ich habe kein Internet\u0026rdquo; zu erzwingen.\nMan startet den neuen Rechner. Man führt die ersten Schritte aus (Sprache, Region, Tastaturlayout).\nMan erreicht den Bildschirm \u0026ldquo;Stellen wir eine Verbindung mit einem Netzwerk her\u0026rdquo;. Man schließt den PC noch NICHT an das LAN-Kabel an.\nMan drückt die Tastenkombination Shift + F10. Ein schwarzes Eingabeaufforderungsfenster (CMD) öffnet sich.\nMan gibt folgenden Befehl ein und drückt Enter:\n1OOBE\\BYPASSNRO Der Computer wird sofort neu gestartet und durchläuft die OOBE erneut.\nMan wählt wieder Region und Tastatur.\nWenn man zum Netzwerkbildschirm kommt, sieht man nun (trotz der Aufforderung, sich zu verbinden) eine neue Option: \u0026ldquo;Ich habe kein Internet\u0026rdquo;. Man wählt diese aus.\nAuf dem nächsten Bildschirm wählt man \u0026ldquo;Mit eingeschränkter Einrichtung fortfahren\u0026rdquo;.\nMan wird nun aufgefordert, einen lokalen Benutzer zu erstellen. Man gibt einen Benutzernamen ein (z. B. LokalerAdmin oder Support).\nMan vergibt ein sicheres lokales Passwort. (Man kann die Sicherheitsfragen bei Bedarf mit beliebigen Werten füllen).\nMan schließt die restlichen OOBE-Schritte ab (Datenschutzeinstellungen etc.), bis man den Desktop sieht.\nMethode B: Die \u0026ldquo;Dummy-E-Mail\u0026rdquo;-Methode (Alternativ) # Wenn Methode A fehlschlägt oder man bereits mit dem Internet verbunden ist:\nMan führt die OOBE bis zum Anmeldebildschirm \u0026ldquo;Mit Microsoft anmelden\u0026rdquo; durch. Man gibt eine nicht existierende E-Mail-Adresse ein, z. B.: no@thankyou.com oder a@a.com. Man klickt auf \u0026ldquo;Weiter\u0026rdquo;. Man gibt ein beliebiges Passwort ein (z. B. 123) und klickt auf \u0026ldquo;Anmelden\u0026rdquo;. Windows zeigt eine Fehlermeldung an wie \u0026ldquo;Ein Fehler ist aufgetreten\u0026rdquo; oder \u0026ldquo;Dieses Konto wurde gesperrt\u0026rdquo;. Man klickt auf \u0026ldquo;Weiter\u0026rdquo;. Windows sollte nun aufgeben und einem die Erstellung eines lokalen Kontos anbieten. Man erstellt das lokale Administratorkonto wie in Methode A beschrieben. Teil 2: Vorbereitung des Rechners für den Domänenbeitritt # Nachdem man sich mit dem in Teil 1 erstellten lokalen Administratorkonto am Desktop angemeldet hat:\nNetzwerk verbinden: Man schließt jetzt das LAN-Kabel an.\nDNS-Server prüfen (Kritisch!): Der Rechner muss den Domänencontroller als DNS-Server verwenden, sonst kann er die Domäne nicht finden.\nMan drückt Win + R, gibt ncpa.cpl ein und drückt Enter. Man klickt mit der rechten Maustaste auf die \u0026ldquo;Ethernet\u0026rdquo;-Verbindung -\u0026gt; \u0026ldquo;Eigenschaften\u0026rdquo;. Man wählt \u0026ldquo;Internetprotokoll, Version 4 (TCP/IPv4)\u0026rdquo; und klickt auf \u0026ldquo;Eigenschaften\u0026rdquo;. WICHTIG: Wenn der Netzwerk-DHCP den korrekten DNS (den DC) nicht automatisch zuweist, muss man ihn hier manuell eintragen: Man wählt \u0026ldquo;Folgende DNS-Serveradressen verwenden\u0026rdquo;. Man gibt bei \u0026ldquo;Bevorzugter DNS-Server\u0026rdquo; die IP-Adresse des Domänencontrollers ein. Man klickt auf \u0026ldquo;OK\u0026rdquo;, um die Fenster zu schließen. (Optional, aber empfohlen) Computername ändern:\nStandardmäßig haben Rechner kryptische Namen (z. B. DESKTOP-L8A4D3J). Es ist Best Practice, vor dem Domänenbeitritt einen aussagekräftigen Namen zu vergeben (z. B. WKS-IT-01). Man drückt Win + R, gibt sysdm.cpl ein und drückt Enter. Man klickt im Tab \u0026ldquo;Computername\u0026rdquo; auf die Schaltfläche \u0026ldquo;Ändern\u0026hellip;\u0026rdquo;. Man gibt den neuen Computernamen ein. Man klickt auf \u0026ldquo;OK\u0026rdquo;. Man wird zu einem Neustart aufgefordert. Man startet den PC neu. Teil 3: Der Domäne beitreten (Domain Join) # Man meldet sich nach dem Neustart (falls man den Namen geändert hat) wieder mit dem lokalen Admin-Konto an.\nMan öffnet erneut die Systemeigenschaften: Man drückt Win + R, gibt sysdm.cpl ein und drückt Enter. Man klickt im Tab \u0026ldquo;Computername\u0026rdquo; auf die Schaltfläche \u0026ldquo;Ändern\u0026hellip;\u0026rdquo;. Man wählt unter \u0026ldquo;Mitglied von\u0026rdquo; die Option \u0026ldquo;Domäne\u0026rdquo; aus. Man gibt den vollqualifizierten Domänennamen (FQDN) der Domäne ein (z. B. meinefirma.local). Man klickt auf \u0026ldquo;OK\u0026rdquo;. Es erscheint ein Anmeldefenster. Man gibt hier die Anmeldedaten eines AD-Benutzers ein, der die Berechtigung hat, Computer hinzuzufügen (z. B. MEINEFIRMA\\Administrator oder ein delegiertes Konto). Man klickt auf \u0026ldquo;OK\u0026rdquo;. Wenn die Verbindung erfolgreich war, erscheint die Meldung: \u0026ldquo;Willkommen in der Domäne [meinefirma.local]\u0026rdquo;. Man bestätigt alle folgenden Dialoge. Man wird aufgefordert, den Computer neu zu starten. Man führt den Neustart durch. Teil 4: Überprüfung und Anmeldung # Nach dem Neustart befindet sich der Rechner in der Domäne.\nAuf dem Anmeldebildschirm sieht man nun die Option, sich anzumelden (eventuell muss man auf \u0026ldquo;Anderer Benutzer\u0026rdquo; klicken). Man meldet sich jetzt nicht mehr mit dem lokalen Konto an, sondern mit einem Domänen-Benutzerkonto. Benutzername: BENUTZERNAME@meinefirma.local (UPN) Oder im Format: MEINEFIRMA\\Benutzername (SamAccountName) Passwort: Das AD-Passwort des Benutzers. Der Rechner erstellt ein neues Benutzerprofil für diesen Domänenbenutzer. Überprüfung (Optional): Um sicherzugehen, öffnet man nach der Anmeldung erneut sysdm.cpl. Unter \u0026ldquo;Computername\u0026rdquo; sollte nun der \u0026ldquo;Vollständige Computername\u0026rdquo; als [computername].[meinefirma.local] angezeigt werden. ","date":"4 November 2025","externalUrl":null,"permalink":"/posts/homelab/windows_install/","section":"Beiträge","summary":"\u003ch2 class=\"relative group\"\u003eAnleitung: Neuen Windows-Rechner in Active Directory (AD) aufnehmen\n    \u003cdiv id=\"anleitung-neuen-windows-rechner-in-active-directory-ad-aufnehmen\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#anleitung-neuen-windows-rechner-in-active-directory-ad-aufnehmen\" aria-label=\"Anker\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cp\u003eDiese Anleitung beschreibt den Prozess, wie man einen fabrikneuen Windows-Rechner (im \u0026ldquo;Out-of-Box Experience\u0026rdquo;-Modus, OOBE) einrichtet, die erzwungene Anmeldung mit einem Microsoft-Konto umgeht und den Rechner anschließend in eine lokale Active Directory-Domäne aufnimmt.\u003c/p\u003e","title":"Windows Installation","type":"posts"},{"content":" Snippets # Remotedesktop aktivieren und in der Firewall freischalten # 1Set-ItemProperty -Path \u0026#39;HKLM:\\System\\CurrentControlSet\\Control\\Terminal Server\u0026#39; -name \u0026#34;fDenyTSConnections\u0026#34; -value 0 2Enable-NetFirewallRule -DisplayGroup \u0026#34;Remote Desktop\u0026#34; ","date":"4 November 2025","externalUrl":null,"permalink":"/posts/homelab/windows_snippets/","section":"Beiträge","summary":"All der Kram, der sonst nirgends hin gehört","title":"Windows Snippets","type":"posts"},{"content":" Snippets # IT Links Sammlung von Links die ich spannend finde und/oder genutzt habe, oder welche ich mir merken möchte.\nMicrosoft Administrator Sites Microsoft Technische Dokumentation Samba Wiki LVM vergrößern Proxmox Festplatte vergrößern Borgbackup bei Hetzner Borgbackup mit einer Storagebox bei Hetzner einrichten\nAnsible Linksammlung zum Thema Ansible\nAnsible Lockdown Ansible Lockdown Git Ansible Lockdown Per-Host Lockdown Configuration Ansible Role Macos Defaults Ansible Hardening dev-sec.io Ansible hardening Testing Ansible Geerlingguy - Jeff Geerling Macos Macos related Links\nManage all your installed software at one place with Homebrew Bundle Logcheck bootstrap.sh/rootfs/etc/logcheck/ignore.d.server at master · Samayel/bootstrap.sh bwesterb/x-logcheck: Extra ignore rules for logcheck fabi125/logcheck-rules: My personal set of additional logcheck rules frlan/logcheck-local-rules: A collection of addition local rules Logcheck logcheck-ignores/dovecot-extra at master · kheiken/logcheck-ignores logcheck-rules/ignore.d.server at master · Pilat66/logcheck-rules logcheck-rules/rules at master · trilader/logcheck-rules logcheck/rulefiles/linux/ignore.d.server at master · dnnr/logcheck logcheck/rulefiles/linux/ignore.d.server at master · tbarbette/logcheck muokata/logcheck-extra: extra logcheck rules my-logcheck-rules/ignore.d.server at master · sylvestre/my-logcheck-rules SSH nur mit Passwort # 1ssh -o PubkeyAuthentication=no -o PreferredAuthentications=password Download Spam # 1wget -q -N -P /var/lib/spamassassin/training/downloads/ http://untroubled.org/spam/$$(date +%%Y-%%m.7z -d \u0026#39;last month\u0026#39;) 2wget -q -N -P /var/lib/spamassassin/training/downloads/ http://untroubled.org/spam/$$(date +%%Y-%%m.7z) Mastodon Wartung # 1bundle exec rake db:migrate 2tootctl statuses remove --days=7 3tootctl preview_cards remove --days=7 4tootctl media remove --days=14 5tootctl media remove-orphans 6tootctl cache clear 7tootctl accounts cull Podman Kubernetes Manifest starten mit systemd # 1systemctl enable --now podman-kube@$(systemd-escape $(pwd)/).service Container bauen mit Podman1 # 1export IMAGE_VERSION=$(git tag --sort=-version:refname | grep -E \u0026#39;^[0-9]+\\.[0-9]+\\.[0-9]+$\u0026#39; | sort -V | tail -n 1) 2 3podman manifest create keel:${IMAGE_VERSION} 4 5podman build --platform linux/amd64,linux/arm64 --manifest localhost/keel:${IMAGE_VERSION} -f Dockerfile 6 7podman manifest push localhost/keel:${IMAGE_VERSION} docker://git.zyria.de/pyrox/keel:${IMAGE_VERSION} 8 9podman manifest push localhost/keel:${IMAGE_VERSION} docker://docker.io/ricariel/keel:${IMAGE_VERSION} Ram cache leeren # 1sync; echo 1 \u0026gt; /proc/sys/vm/drop_caches Leeren branch in git erstellen # 1git switch --orphan pages 2git commit --allow-empty -m \u0026#34;Initial commit on orphan branch\u0026#34; 3git push -u origin pages Zone aus Samba extrahieren # 1#!/bin/sh 2# 3# Extract DNS zone from Samba4 native DNS using samba-tool 4# 5# Prerequistes: 6# samba krb5-workstation 7 8SAMBA_TOOL=samba-tool 9 10#kinit Administrator 11#trap \u0026#34;kdestroy; rm -f $TMPFILE\u0026#34; 0 1 2 15 12#klist 13## -k=yes doesn\u0026#39;t work... 14## Using --password isn\u0026#39;t secure. 15USER=Administrator 16echo -n \u0026#34;$USER password:\u0026#34; \u0026gt;\u0026amp;2; stty -echo; read PASS; stty echo; echo \u0026#39;\u0026#39; \u0026gt;\u0026amp;2 17echo \u0026#39;Please ignore \u0026#34;Cannot do GSSAPI to an IP address\u0026#34; errors...\u0026#39; \u0026gt;\u0026amp;2 18 19## Extract zones 20# 2 zone(s) found 21# 22# pszZoneName : a.example.or.jp 23# Flags : DNS_RPC_ZONE_DSINTEGRATED DNS_RPC_ZONE_UPDATE_SECURE 24# ZoneType : DNS_ZONE_TYPE_PRIMARY 25# Version : 50 26# dwDpFlags : DNS_DP_AUTOCREATED DNS_DP_DOMAIN_DEFAULT DNS_DP_ENLISTED 27# pszDpFqdn : DomainDnsZones.a.example.or.jp 28# 29# pszZoneName : _msdcs.a.example.or.jp 30# Flags : DNS_RPC_ZONE_DSINTEGRATED DNS_RPC_ZONE_UPDATE_SECURE 31# ZoneType : DNS_ZONE_TYPE_PRIMARY 32# Version : 50 33# dwDpFlags : DNS_DP_AUTOCREATED DNS_DP_FOREST_DEFAULT DNS_DP_ENLISTED 34# pszDpFqdn : ForestDnsZones.a.example.or.jp 35 36#$SAMBA_TOOL dns zonelist localhost --kerberos=yes \u0026gt; $TMPFILE 37ZONES=`$SAMBA_TOOL dns zonelist localhost -U \u0026#34;$USER\u0026#34; --password \u0026#34;$PASS\u0026#34; | awk \u0026#39;$1 ~ /pszZoneName/{print $3}\u0026#39;` 38 39## queryzone $zone \u0026#34;ForestDnsZones\u0026#34; 40queryzone () { 41 local zone=\u0026#34;$1\u0026#34; 42 local entry=\u0026#34;$2\u0026#34; 43 local qentry=\u0026#34;$entry\u0026#34; 44 local name 45 local children 46 local lhs 47 test -z \u0026#34;$entry\u0026#34; \u0026amp;\u0026amp; qentry=\u0026#34;@\u0026#34; 48 49 $SAMBA_TOOL dns query localhost $zone \u0026#34;$qentry\u0026#34; ALL -U \u0026#34;$USER\u0026#34; --password \u0026#34;$PASS\u0026#34; | 50# Name=, Records=3, Children=0 51# SOA: serial=1, refresh=900, retry=600, expire=86400, minttl=3600, ns=ad01.a.example.or.jp., email=hostmaster.a.example.or.jp. (flags=600000f0, serial=1, ttl=3600) 52# NS: ad01.a.example.or.jp. (flags=600000f0, serial=1, ttl=900) 53# A: 100.64.96.31 (flags=600000f0, serial=1, ttl=900) 54# Name=_msdcs, Records=0, Children=0 55# Name=_sites, Records=0, Children=1 56# Name=_tcp, Records=0, Children=4 57 while read line; do 58 set $line 59 case \u0026#34;$1\u0026#34; in 60 Name=*,) 61 name=`expr $1 : \u0026#39;Name=\\([^,]*\\)*,\u0026#39;` 62 children=`expr $3 : \u0026#39;Children=\\([0-9]*\\)\u0026#39;` 63 if [ $children -gt 0 ]; then 64 queryzone $zone $name${entry:+.}$entry 65 fi 66 if [ -z \u0026#34;$name\u0026#34; ]; then 67 if [ -z \u0026#34;$entry\u0026#34; ]; then 68 lhs=\u0026#34;@\u0026#34; 69 else 70 lhs=\u0026#34;${entry}\u0026#34; 71 fi 72 else 73 lhs=\u0026#34;${name}${entry:+.}${entry}\u0026#34; 74 fi 75 ;; 76 SOA:) 77# SOA: serial=1, refresh=900, retry=600, expire=86400, minttl=3600, ns=ad01.a.example.or.jp., email=hostmaster.a.example.or.jp. (flags=600000f0, serial=1, ttl=3600) 78 echo \u0026#34;$@\u0026#34; | sed -e \u0026#39;s/.*serial=\\([0-9]*\\), refresh=\\([0-9]*\\), retry=\\([0-9]*\\), expire=\\([0-9]*\\), minttl=\\([0-9]*\\), ns=\\([^,]*\\), email=\\([^,]*\\) (flags=.*, serial=[0-9]*, ttl=\\([0-9]*\\))/\u0026#39;\u0026#34;${name:-@}\u0026#34;\u0026#39; \\8 IN SOA \\6 \\7 \\1 \\2 \\3 \\4 \\5/\u0026#39; 79 ;; 80# NS: ad01.a.example.or.jp. (flags=600000f0, serial=1, ttl=900) 81# A: 100.64.96.31 (flags=600000f0, serial=1, ttl=900) 82 NS:|A:|AAAA:) 83 echo \u0026#34;$@\u0026#34; | sed -ne \u0026#39;s/\\([^ ]*\\): \\([^ ]*\\) (flags=[0-9a-f]*, serial=[0-9]*, ttl=\\([0-9]*\\)).*/\u0026#39;\u0026#34;${lhs}\u0026#34;\u0026#39; \\3 IN \\1 \\2/p\u0026#39; 84 ;; 85# SRV: ad01.a.example.or.jp. (88, 0, 100) (flags=f0, serial=1, ttl=900) 86 SRV:|MX:) 87 echo \u0026#34;$@\u0026#34; | sed -ne \u0026#39;s/\\([^ ]*\\): \\([^ ]*\\) (\\([0-9]*\\), \\([0-9]*\\), \\([0-9]*\\)) (flags=[0-9a-f]*, serial=[0-9]*, ttl=\\([0-9]*\\)).*/\u0026#39;\u0026#34;${lhs}\u0026#34;\u0026#39; \\6 IN \\1 \\4 \\5 \\3 \\2/p\u0026#39; 88 ;; 89# CNAME: ad01.a.example.or.jp. (flags=f0, serial=1, ttl=900) 90 CNAME:) 91 echo \u0026#34;$@\u0026#34; | sed -ne \u0026#39;s/\\([^ ]*\\): \\([^ ]*\\) (flags=[0-9a-f]*, serial=[0-9]*, ttl=\\([0-9]*\\)).*/\u0026#39;\u0026#34;${lhs}\u0026#34;\u0026#39; \\3 IN \\1 \\2/p\u0026#39; 92 ;; 93 *) 94 echo \u0026#34;ERROR unknown record type $1; aborting\u0026#34; \u0026gt;\u0026amp;2; exit 1 95 ;; 96 esac 97 done 98} 99 100echo Zones: $ZONES \u0026gt;\u0026amp;2 101for zone in $ZONES; do 102 echo \u0026#39;$ORIGIN\u0026#39; $zone 103 echo \u0026#39;\u0026#39; 104 queryzone $zone \u0026#34;\u0026#34; 105 echo \u0026#39;\u0026#39; 106done ;# zone $ZONES Schlüssel aus Windows extrahieren # 1Set WshShell = CreateObject(\u0026#34;WScript.Shell\u0026#34;) 2MsgBox ConvertToKey(WshShell.RegRead(\u0026#34;HKLM\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\\DigitalProductId\u0026#34;)) 3 4Function ConvertToKey(Key) 5Const KeyOffset = 52 6i = 28 7Chars = \u0026#34;BCDFGHJKMPQRTVWXY2346789\u0026#34; 8Do 9Cur = 0 10x = 14 11Do 12Cur = Cur * 256 13Cur = Key(x + KeyOffset) + Cur 14Key(x + KeyOffset) = (Cur \\ 24) And 255 15Cur = Cur Mod 24 16x = x -1 17Loop While x \u0026gt;= 0 18i = i -1 19KeyOutput = Mid(Chars, Cur + 1, 1) \u0026amp; KeyOutput 20If (((29 - i) Mod 6) = 0) And (i \u0026lt;\u0026gt; -1) Then 21i = i -1 22KeyOutput = \u0026#34;-\u0026#34; \u0026amp; KeyOutput 23End If 24Loop While i \u0026gt;= 0 25ConvertToKey = KeyOutput 26End Function Festplatten in Proxmox bewegen # Dieses Script migriert alle Festplatten einer VM auf einen anderen Storage\n1#!/bin/bash 2 3VMID=0 4STORAGE_DEST=\u0026#34;\u0026#34; 5HOST_DEST=\u0026#34;\u0026#34; 6 7while getopts \u0026#34;:i:s:h\u0026#34; opt 8do 9 case $opt in 10 i ) VMID=$OPTARG ;; 11 s ) STORAGE_DEST=$OPTARG ;; 12 h ) HOST_DEST=$OPTARG ;; 13 \\? ) echo \u0026#34;Error\u0026#34; 14 exit 1 ;; 15 : ) echo \u0026#34;Option -$OPTARG requires an argument\u0026#34; 16 exit 1 ;; 17 esac 18done 19 20VMS=\u0026#34;$(qm list | egrep \u0026#34;[0-9]{3}\u0026#34; | awk \u0026#39;{print $1}\u0026#39;)\u0026#34; 21echo $VMS 22 23if [[ $VMID==\u0026#34;all\u0026#34; ]] 24then 25 for j in $VMS; do 26 DISCOS=\u0026#34;$(qm config $j | egrep \u0026#34;^virtio[0-9]|^scsi[0-9]\u0026#34; | awk \u0026#39;{print $1}\u0026#39; | tr -d \u0026#34;:\u0026#34;)\u0026#34; 27 echo $DISCOS 28 for i in $DISCOS; do 29 qm move_disk $j $i $STORAGE_DEST --delete --format qcow2 30 done 31 done 32else 33 DISCOS=\u0026#34;$(qm config $VMID | egrep \u0026#34;^virtio[0-9]|^scsi[0-9]\u0026#34; | awk \u0026#39;{print $1}\u0026#39; | tr -d \u0026#34;:\u0026#34;)\u0026#34; 34 for i in $DISCOS; do 35 qm move_disk $VMID $i $STORAGE_DEST --delete --format qcow2 36 done 37 38fi 39#qm migrate $VMID $HOST_DEST --online Reset aller lokalem Sicherheitsrichtlinien # 1secedit /configure /cfg %windir%\\inf\\defltbase.inf /db defltbase.sdb /verbose Einspielen neuer lokaler Sicherheitsrichtlinien # 1secedit /configure /db %windir%\\security\\new.sdb /cfg C:\\Temp\\Unternehmenssicherheit_W11.inf /overwrite /log C:\\Temp\\security_log.txt Reset aller Policies unter Windows # 1secedit /configure /cfg %windir%\\inf\\defltbase.inf /db defltbase.sdb /verbose 2 3RD /S /Q \u0026#34;%WinDir%\\System32\\GroupPolicyUsers\u0026#34; 4RD /S /Q \u0026#34;%WinDir%\\System32\\GroupPolicy\u0026#34; 5 6reg delete \u0026#34;HKCU\\Software\\Microsoft\\Windows\\CurrentVersion\\Policies\u0026#34; /f 7reg delete \u0026#34;HKCU\\Software\\Microsoft\\WindowsSelfHost\u0026#34; /f 8reg delete \u0026#34;HKCU\\Software\\Policies\u0026#34; /f 9reg delete \u0026#34;HKLM\\Software\\Microsoft\\Policies\u0026#34; /f 10reg delete \u0026#34;HKLM\\Software\\Microsoft\\Windows\\CurrentVersion\\Policies\u0026#34; /f 11reg delete \u0026#34;HKLM\\Software\\Microsoft\\Windows\\CurrentVersion\\WindowsStore\\WindowsUpdate\u0026#34; /f 12reg delete \u0026#34;HKLM\\Software\\Microsoft\\WindowsSelfHost\u0026#34; /f 13reg delete \u0026#34;HKLM\\Software\\Policies\u0026#34; /f 14reg delete \u0026#34;HKLM\\Software\\WOW6432Node\\Microsoft\\Policies\u0026#34; /f 15reg delete \u0026#34;HKLM\\Software\\WOW6432Node\\Microsoft\\Windows\\CurrentVersion\\Policies\u0026#34; /f 16reg delete \u0026#34;HKLM\\Software\\WOW6432Node\\Microsoft\\Windows\\CurrentVersion\\WindowsStore\\WindowsUpdate\u0026#34; /f 17 18gpupdate /force Reset der User Folder Redirection Policy unter Windows # 1Windows Registry Editor Version 5.00 2 3[HKEY_CURRENT_USER\\Software\\Microsoft\\Windows\\CurrentVersion\\Explorer\\User Shell Folders] 4\u0026#34;AppData\u0026#34;=hex(2):25,00,55,00,53,00,45,00,52,00,50,00,52,00,4f,00,46,00,49,00,\\ 5 4c,00,45,00,25,00,5c,00,41,00,70,00,70,00,44,00,61,00,74,00,61,00,5c,00,52,\\ 6 00,6f,00,61,00,6d,00,69,00,6e,00,67,00,00,00 7\u0026#34;Cache\u0026#34;=hex(2):25,00,55,00,53,00,45,00,52,00,50,00,52,00,4f,00,46,00,49,00,4c,\\ 8 00,45,00,25,00,5c,00,41,00,70,00,70,00,44,00,61,00,74,00,61,00,5c,00,4c,00,\\ 9 6f,00,63,00,61,00,6c,00,5c,00,4d,00,69,00,63,00,72,00,6f,00,73,00,6f,00,66,\\ 10 00,74,00,5c,00,57,00,69,00,6e,00,64,00,6f,00,77,00,73,00,5c,00,54,00,65,00,\\ 11 6d,00,70,00,6f,00,72,00,61,00,72,00,79,00,20,00,49,00,6e,00,74,00,65,00,72,\\ 12 00,6e,00,65,00,74,00,20,00,46,00,69,00,6c,00,65,00,73,00,00,00 13\u0026#34;Cookies\u0026#34;=hex(2):25,00,55,00,53,00,45,00,52,00,50,00,52,00,4f,00,46,00,49,00,\\ 14 4c,00,45,00,25,00,5c,00,41,00,70,00,70,00,44,00,61,00,74,00,61,00,5c,00,52,\\ 15 00,6f,00,61,00,6d,00,69,00,6e,00,67,00,5c,00,4d,00,69,00,63,00,72,00,6f,00,\\ 16 73,00,6f,00,66,00,74,00,5c,00,57,00,69,00,6e,00,64,00,6f,00,77,00,73,00,5c,\\ 17 00,43,00,6f,00,6f,00,6b,00,69,00,65,00,73,00,00,00 18\u0026#34;Desktop\u0026#34;=hex(2):25,00,55,00,53,00,45,00,52,00,50,00,52,00,4f,00,46,00,49,00,\\ 19 4c,00,45,00,25,00,5c,00,44,00,65,00,73,00,6b,00,74,00,6f,00,70,00,00,00 20\u0026#34;Favorites\u0026#34;=hex(2):25,00,55,00,53,00,45,00,52,00,50,00,52,00,4f,00,46,00,49,00,\\ 21 4c,00,45,00,25,00,5c,00,46,00,61,00,76,00,6f,00,72,00,69,00,74,00,65,00,73,\\ 22 00,00,00 23\u0026#34;History\u0026#34;=hex(2):25,00,55,00,53,00,45,00,52,00,50,00,52,00,4f,00,46,00,49,00,\\ 24 4c,00,45,00,25,00,5c,00,41,00,70,00,70,00,44,00,61,00,74,00,61,00,5c,00,4c,\\ 25 00,6f,00,63,00,61,00,6c,00,5c,00,4d,00,69,00,63,00,72,00,6f,00,73,00,6f,00,\\ 26 66,00,74,00,5c,00,57,00,69,00,6e,00,64,00,6f,00,77,00,73,00,5c,00,48,00,69,\\ 27 00,73,00,74,00,6f,00,72,00,79,00,00,00 28\u0026#34;Local AppData\u0026#34;=hex(2):25,00,55,00,53,00,45,00,52,00,50,00,52,00,4f,00,46,00,\\ 29 49,00,4c,00,45,00,25,00,5c,00,41,00,70,00,70,00,44,00,61,00,74,00,61,00,5c,\\ 30 00,4c,00,6f,00,63,00,61,00,6c,00,00,00 31\u0026#34;My Music\u0026#34;=hex(2):25,00,55,00,53,00,45,00,52,00,50,00,52,00,4f,00,46,00,49,00,\\ 32 4c,00,45,00,25,00,5c,00,4d,00,75,00,73,00,69,00,63,00,00,00 33\u0026#34;My Pictures\u0026#34;=hex(2):25,00,55,00,53,00,45,00,52,00,50,00,52,00,4f,00,46,00,49,\\ 34 00,4c,00,45,00,25,00,5c,00,50,00,69,00,63,00,74,00,75,00,72,00,65,00,73,00,\\ 35 00,00 36\u0026#34;My Video\u0026#34;=hex(2):25,00,55,00,53,00,45,00,52,00,50,00,52,00,4f,00,46,00,49,00,\\ 37 4c,00,45,00,25,00,5c,00,56,00,69,00,64,00,65,00,6f,00,73,00,00,00 38\u0026#34;NetHood\u0026#34;=hex(2):25,00,55,00,53,00,45,00,52,00,50,00,52,00,4f,00,46,00,49,00,\\ 39 4c,00,45,00,25,00,5c,00,41,00,70,00,70,00,44,00,61,00,74,00,61,00,5c,00,52,\\ 40 00,6f,00,61,00,6d,00,69,00,6e,00,67,00,5c,00,4d,00,69,00,63,00,72,00,6f,00,\\ 41 73,00,6f,00,66,00,74,00,5c,00,57,00,69,00,6e,00,64,00,6f,00,77,00,73,00,5c,\\ 42 00,4e,00,65,00,74,00,77,00,6f,00,72,00,6b,00,20,00,53,00,68,00,6f,00,72,00,\\ 43 74,00,63,00,75,00,74,00,73,00,00,00 44\u0026#34;Personal\u0026#34;=hex(2):25,00,55,00,53,00,45,00,52,00,50,00,52,00,4f,00,46,00,49,00,\\ 45 4c,00,45,00,25,00,5c,00,44,00,6f,00,63,00,75,00,6d,00,65,00,6e,00,74,00,73,\\ 46 00,00,00 47\u0026#34;Programs\u0026#34;=hex(2):25,00,55,00,53,00,45,00,52,00,50,00,52,00,4f,00,46,00,49,00,\\ 48 4c,00,45,00,25,00,5c,00,41,00,70,00,70,00,44,00,61,00,74,00,61,00,5c,00,52,\\ 49 00,6f,00,61,00,6d,00,69,00,6e,00,67,00,5c,00,4d,00,69,00,63,00,72,00,6f,00,\\ 50 73,00,6f,00,66,00,74,00,5c,00,57,00,69,00,6e,00,64,00,6f,00,77,00,73,00,5c,\\ 51 00,53,00,74,00,61,00,72,00,74,00,20,00,4d,00,65,00,6e,00,75,00,5c,00,50,00,\\ 52 72,00,6f,00,67,00,72,00,61,00,6d,00,73,00,00,00 53\u0026#34;Recent\u0026#34;=hex(2):25,00,55,00,53,00,45,00,52,00,50,00,52,00,4f,00,46,00,49,00,4c,\\ 54 00,45,00,25,00,5c,00,41,00,70,00,70,00,44,00,61,00,74,00,61,00,5c,00,52,00,\\ 55 6f,00,61,00,6d,00,69,00,6e,00,67,00,5c,00,4d,00,69,00,63,00,72,00,6f,00,73,\\ 56 00,6f,00,66,00,74,00,5c,00,57,00,69,00,6e,00,64,00,6f,00,77,00,73,00,5c,00,\\ 57 52,00,65,00,63,00,65,00,6e,00,74,00,00,00 58\u0026#34;SendTo\u0026#34;=hex(2):25,00,55,00,53,00,45,00,52,00,50,00,52,00,4f,00,46,00,49,00,4c,\\ 59 00,45,00,25,00,5c,00,41,00,70,00,70,00,44,00,61,00,74,00,61,00,5c,00,52,00,\\ 60 6f,00,61,00,6d,00,69,00,6e,00,67,00,5c,00,4d,00,69,00,63,00,72,00,6f,00,73,\\ 61 00,6f,00,66,00,74,00,5c,00,57,00,69,00,6e,00,64,00,6f,00,77,00,73,00,5c,00,\\ 62 53,00,65,00,6e,00,64,00,54,00,6f,00,00,00 63\u0026#34;Startup\u0026#34;=hex(2):25,00,55,00,53,00,45,00,52,00,50,00,52,00,4f,00,46,00,49,00,\\ 64 4c,00,45,00,25,00,5c,00,41,00,70,00,70,00,44,00,61,00,74,00,61,00,5c,00,52,\\ 65 00,6f,00,61,00,6d,00,69,00,6e,00,67,00,5c,00,4d,00,69,00,63,00,72,00,6f,00,\\ 66 73,00,6f,00,66,00,74,00,5c,00,57,00,69,00,6e,00,64,00,6f,00,77,00,73,00,5c,\\ 67 00,53,00,74,00,61,00,72,00,74,00,20,00,4d,00,65,00,6e,00,75,00,5c,00,50,00,\\ 68 72,00,6f,00,67,00,72,00,61,00,6d,00,73,00,5c,00,53,00,74,00,61,00,72,00,74,\\ 69 00,75,00,70,00,00,00 70\u0026#34;Start Menu\u0026#34;=hex(2):25,00,55,00,53,00,45,00,52,00,50,00,52,00,4f,00,46,00,49,\\ 71 00,4c,00,45,00,25,00,5c,00,41,00,70,00,70,00,44,00,61,00,74,00,61,00,5c,00,\\ 72 52,00,6f,00,61,00,6d,00,69,00,6e,00,67,00,5c,00,4d,00,69,00,63,00,72,00,6f,\\ 73 00,73,00,6f,00,66,00,74,00,5c,00,57,00,69,00,6e,00,64,00,6f,00,77,00,73,00,\\ 74 5c,00,53,00,74,00,61,00,72,00,74,00,20,00,4d,00,65,00,6e,00,75,00,00,00 75\u0026#34;Templates\u0026#34;=hex(2):25,00,55,00,53,00,45,00,52,00,50,00,52,00,4f,00,46,00,49,00,\\ 76 4c,00,45,00,25,00,5c,00,41,00,70,00,70,00,44,00,61,00,74,00,61,00,5c,00,52,\\ 77 00,6f,00,61,00,6d,00,69,00,6e,00,67,00,5c,00,4d,00,69,00,63,00,72,00,6f,00,\\ 78 73,00,6f,00,66,00,74,00,5c,00,57,00,69,00,6e,00,64,00,6f,00,77,00,73,00,5c,\\ 79 00,54,00,65,00,6d,00,70,00,6c,00,61,00,74,00,65,00,73,00,00,00 80\u0026#34;{374DE290-123F-4565-9164-39C4925E467B}\u0026#34;=hex(2):25,00,55,00,53,00,45,00,52,00,\\ 81 50,00,52,00,4f,00,46,00,49,00,4c,00,45,00,25,00,5c,00,44,00,6f,00,77,00,6e,\\ 82 00,6c,00,6f,00,61,00,64,00,73,00,00,00 83\u0026#34;PrintHood\u0026#34;=hex(2):25,00,55,00,53,00,45,00,52,00,50,00,52,00,4f,00,46,00,49,00,\\ 84 4c,00,45,00,25,00,5c,00,41,00,70,00,70,00,44,00,61,00,74,00,61,00,5c,00,52,\\ 85 00,6f,00,61,00,6d,00,69,00,6e,00,67,00,5c,00,4d,00,69,00,63,00,72,00,6f,00,\\ 86 73,00,6f,00,66,00,74,00,5c,00,57,00,69,00,6e,00,64,00,6f,00,77,00,73,00,5c,\\ 87 00,50,00,72,00,69,00,6e,00,74,00,65,00,72,00,20,00,53,00,68,00,6f,00,72,00,\\ 88 74,00,63,00,75,00,74,00,73,00,00,00 Raid löschen # 1mdadm --stop /dev/md0 2for i in \u0026#34;0\u0026#34; \u0026#34;1\u0026#34;; do mdadm --zero-superblock /dev/nvme\u0026#34;$i\u0026#34;n1; done Raid erstellen # 1yes | mdadm --create -n 2 -l 1 /dev/md0 /dev/nvme[01]n1p1 Partition mit ext4 formatieren # mkfs.ext4 /dev/md0\noder\nyes | mkfs.ext4 -L boot /dev/md0\nSwap:\nmkswap -f /dev/vg-name/swap\nFestplatten Verschlüsselung # verschlüsseln # cryptsetup \u0026ndash;batch-mode -c aes-cbc-essiv:sha256 -s 256 -y luksFormat /dev/nvme0n1p4\nöffnen # cryptsetup luksOpen /dev/md1 crypt-root\nFestplatte Partitionieren # 1for i in \u0026#34;0\u0026#34; \u0026#34;1\u0026#34;; do 2parted -a optimal /dev/nvme\u0026#34;$i\u0026#34;n1 --script \\ 3unit s \\ 4mklabel gpt \\ 5mkpart esp 2048 128MB \\ 6mkpart grub 128MB 2048MB \\ 7mkpart raid 2048MB 50GB \\ 8mkpart linux 50GB 100% \\ 9set 1 esp on \\ 10set 2 raid on \\ 11set 3 raid on; 12done 13 14set 2 bios_grub on \\ 15set 2 boot on \\ LVM # Gerät hinzufügen # 1pvcreate /dev/mapper/crypt-root 2vgcreate vg-name /dev/mapper/crypt-root 3 4lvcreate -n root -L 4G vg-name 5lvcreate -n var-log -l100%FREE vg-name Git # Alle Repos mit Zugriff auf den lokalen Rechner clonen\n1tea login add 2for i in $(tea repos ls --fields ssh -o simple -l git.zyria.de -T source); do git clone $i; done https://blog.while-true-do.io/podman-multi-arch-images/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"19 September 2025","externalUrl":null,"permalink":"/posts/homelab/snippets/","section":"Beiträge","summary":"All der Kram, der sonst nirgends hin gehört","title":"Snippets","type":"posts"},{"content":"","date":"14 August 2025","externalUrl":null,"permalink":"/posts/homelab/netzwerk/","section":"Beiträge","summary":"","title":"","type":"posts"},{"content":"Auf dieser Homepage ist (zumindest jetzt) nichts zu finden.\nAber?!\n","date":"3 Juli 2025","externalUrl":null,"permalink":"/","section":"Eine (scheinbar) leere Homepage","summary":"\u003cp\u003eAuf dieser Homepage ist (zumindest jetzt) nichts zu finden.\u003c/p\u003e\n\u003cp\u003e\u003ca\n  href=\"/why/\"\u003eAber?!\u003c/a\u003e\u003c/p\u003e","title":"Eine (scheinbar) leere Homepage","type":"page"},{"content":"Eine nicht vollständige Sitemap\nDie Sitemap im XML Format kann unter sitemap.xml herunter geladen werden.\nÜber mich 24 Juli 2023 Datenschutzerklärung 21 Juli 2023 Impressum 21 Juli 2023 Operational Lifecycle Management: Orchestrierter Shutdown für Kubernetes 27 Oktober 2024 Ansible Konfiguration für k3s-prod Dieses Verzeichnis enthält Ansible-Playbooks und Konfigurationen zur Automatisierung der Bereitstellung und Verwaltung des k3s Kubernetes Produktionsclusters. 12 Januar 2026 Ansible Semaphore Dokumentation für die Bereitstellung von Ansible Semaphore 12 Januar 2026 ArgoCD Dokumentation für die Bereitstellung von Argo CD 12 Januar 2026 Authentik Dokumentation für die Bereitstellung von Authentik 12 Januar 2026 Cert-Manager Dokumentation für die Bereitstellung von Cert-Manager 12 Januar 2026 CoreDNS Ein flexibler und erweiterbarer DNS-Server für Kubernetes. 12 Januar 2026 CoreDNS Ein flexibler und erweiterbarer DNS-Server für Kubernetes. 12 Januar 2026 CoreDNS Ein flexibler und erweiterbarer DNS-Server für Kubernetes. 12 Januar 2026 DMARC Report Dokumentation für die Bereitstellung des DMARC Report Tools 12 Januar 2026 Endpoint Copier Operator Dokumentation für die Bereitstellung des Endpoint Copier Operators 12 Januar 2026 Forgejo Dokumentation für die Bereitstellung von Forgejo 12 Januar 2026 Forgejo Runner Dokumentation für die Bereitstellung von Forgejo Runner 12 Januar 2026 Headscale Dokumentation für die Bereitstellung von Headscale 12 Januar 2026 HedgeDoc Dokumentation für die Bereitstellung von HedgeDoc 12 Januar 2026 Home Assistant Eine Open-Source-Plattform für Heimautomatisierung. 12 Januar 2026 hx53.de Stellt die statische Webseite hx53.de bereit. 12 Januar 2026 IT-Tools Eine Sammlung nützlicher Online-Werkzeuge für Entwickler und IT-Profis. 12 Januar 2026 k3s Konfiguration Basis-Konfiguration und Add-ons für die k3s-Distribution. 12 Januar 2026 Keel Automatisiert die Aktualisierung von Kubernetes-Workloads und Helm-Releases. 12 Januar 2026 Kubernetes Descheduler Dokumentation für die Bereitstellung des Kubernetes Deschedulers 12 Januar 2026 Longhorn Ein cloud-natives, verteiltes Blockspeicher-System für Kubernetes. 12 Januar 2026 Mailstack Ein vollständiger, containerisierter E-Mail-Server-Stack. 12 Januar 2026 Maintenance Page Eine globale Wartungsseite für den Traefik Ingress Controller. 12 Januar 2026 Mastodon Ein dezentraler, föderierter Social-Media-Dienst. 12 Januar 2026 Matrix Ein offener Standard für dezentrale, sichere Echtzeitkommunikation. 12 Januar 2026 Matrix Synapse Admin Eine Weboberfläche zur Administration des Matrix Synapse Homeservers. 12 Januar 2026 MetalLB Ein Load-Balancer für Bare-Metal-Kubernetes-Cluster. 12 Januar 2026 Mosquitto Ein leichtgewichtiger Open-Source-MQTT-Broker. 12 Januar 2026 MTA-STS Policy Server Stellt die MTA-STS-Richtliniendatei für mehrere Domains bereit. 12 Januar 2026 Nextcloud Eine Open-Source-Plattform für Heimautomatisierung. 12 Januar 2026 NFS Subdir External Provisioner Documentation for the NFS Subdir External Provisioner deployment 12 Januar 2026 OpenDTU Eine Weboberfläche zur Überwachung von Hoymiles Solarinvertern. 12 Januar 2026 Paperless-ngx Documentation for the Paperless-ngx deployment 12 Januar 2026 Pi-hole Documentation for the Pi-hole deployment 12 Januar 2026 Plex Media Server Documentation for the Plex Media Server deployment 12 Januar 2026 Prometheus Documentation for the Prometheus monitoring system deployment 12 Januar 2026 quaecki.de Website Documentation for the quaecki.de website deployment 12 Januar 2026 Reloader Documentation for the Reloader deployment 12 Januar 2026 Roundcube Webmail Documentation for the Roundcube Webmail deployment 12 Januar 2026 Self Service Password Documentation for the Self-Service Password deployment 12 Januar 2026 SolarChart Website Documentation for the SolarChart website deployment 12 Januar 2026 SolarFlow Topic Mapper (ACE1500) Dokumentation für die Bereitstellung des SolarFlow Topic Mappers (ACE1500) 12 Januar 2026 SolarFlow Topic Mapper (SF1200) Ein MQTT-Hilfsdienst für das Zendure SolarFlow 1200 System. 12 Januar 2026 SolarFlow Topic Mapper (SF2000-2) Ein MQTT-Hilfsdienst für das zweite Zendure SolarFlow 2000 System. 12 Januar 2026 SolarFlow Topic Mapper (SF2000) Ein MQTT-Hilfsdienst für das Zendure SolarFlow 2000 System. 12 Januar 2026 System Upgrade Controller Documentation for the System Upgrade Controller deployment 12 Januar 2026 Traefik Ingress Controller Documentation for the Traefik Ingress Controller deployment 12 Januar 2026 Trivy Operator Documentation for the Trivy Operator deployment 12 Januar 2026 UniFi Netzwerk-Anwendung Dokumentation für die Bereitstellung der UniFi Netzwerk-Anwendung 12 Januar 2026 Uptime Kuma Documentation for the Uptime Kuma deployment 12 Januar 2026 whoami Ein einfacher Test-Dienst, der Informationen über die eingehende Anfrage zurückgibt. 12 Januar 2026 WireGuard VPN Server Documentation for the WireGuard VPN Server deployment 12 Januar 2026 Zammad Helpdesk Documentation for the Zammad Helpdesk deployment 12 Januar 2026 zyria.de Webseite Dokumentation für die Bereitstellung der zyria.de Webseite 12 Januar 2026 Debian Upgrade Debian Upgrade 13 Dezember 2025 Kubernetes Snippets Sammlung von Snippets, welche immer mal wieder benötigt wurden 18 November 2025 Windows Installation 4 November 2025 Windows Snippets Sammlung von Codesnippets 4 November 2025 Snippets Sammlung von Codesnippets 19 September 2025 14 August 2025 Sitemap Eine nicht vollständige Sitemap 2 Juli 2025 Eine (scheinbar) leere Homepage Die Seite ist doch gar nicht leer! 1 Juli 2025 Hardware im Homelab Hardware Übersicht 1 Juli 2025 Hetzner Server Wir wir eine Maschine bei Hetzner zu einem Proxmox Server machen. 1 Juli 2025 Logins Zugang zu den Servern 1 Juli 2025 Passwörter Wie werden Passwörter bei uns verwaltet 1 Juli 2025 Samba Schema Das Samba Schema wurde durch folgende Entitäten erweitert 1 Juli 2025 Server Beschreibung der Serverlandschaft 1 Juli 2025 Server-,Diensteverwaltung Möglichkeiten die Server und Dienste zu verwalten 1 Juli 2025 Setup Windows Client Wie man einen Windows Client in die Firma einbindet. 1 Juli 2025 Software Eingesetzte Software auf den Nodes 1 Juli 2025 Windows autologon 1 Juli 2025 WireGuard mit systemd 1 Juli 2025 Addc Beschreibung von addc.zyria.de 1 Juli 2025 Dev Beschreibung von dev.zyria.de 1 Juli 2025 Solaranlage 22 März 2024 Linksammlung rund um Strom 8 März 2024 Erste Hilfe Erste Anlaufstelle bei Problemen 19 Oktober 2023 ","date":"2 Juli 2025","externalUrl":null,"permalink":"/sitemap/","section":"Eine (scheinbar) leere Homepage","summary":"\u003cp\u003eEine nicht vollständige Sitemap\u003c/p\u003e","title":"Sitemap","type":"page"},{"content":" Scheinbar leer? Nichts zu sehen? # Nein aber der Inhalt ist nicht stetig. Hauptsächlich zum spielen, testen und zur persönlichen Dokumentation. Die Domain Zyria.de ist für meine persönliche Infrastruktur zuständig.\nDies ist keine polierte Unternehmensseite. Dies ist ein persönlicher, technischer Raum. Der wahre Wert liegt nicht in einer schicken Landing-Page, sondern im Code und in der Dokumentation.\nZyria # Der Domainname zyria.de wurde gewürfelt und ist ein reines Phantasiewort. Die Domain war kurz, frei und günstig. Jegliche Hinweise auf andere gleichlautende Namen sind rein zufällig.\nAndere Domains # Kirchner.social Für die Familie. Derzeit wird Mastodon und Matrix damit betrieben.\nQuaecki.de Für mein Kind zu Ihrer freien Verfügung. Derzeit nicht verwendet.\nhx53.de Für Experimente. Sonst nicht genutzt.\n","date":"1 Juli 2025","externalUrl":null,"permalink":"/why/","section":"Eine (scheinbar) leere Homepage","summary":"\u003ch2 class=\"relative group\"\u003eScheinbar leer? Nichts zu sehen?\n    \u003cdiv id=\"scheinbar-leer-nichts-zu-sehen\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#scheinbar-leer-nichts-zu-sehen\" aria-label=\"Anker\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cp\u003eNein aber der Inhalt ist nicht stetig. Hauptsächlich zum spielen, testen und zur persönlichen Dokumentation. Die Domain Zyria.de ist für meine persönliche Infrastruktur zuständig.\u003c/p\u003e\n\u003cp\u003eDies ist keine polierte Unternehmensseite. Dies ist ein persönlicher, technischer Raum. Der wahre Wert liegt nicht in einer schicken Landing-Page, sondern im Code und in der Dokumentation.\u003c/p\u003e","title":"Eine (scheinbar) leere Homepage","type":"page"},{"content":"","date":"1 Juli 2025","externalUrl":null,"permalink":"/tags/%C3%BCber-mich/","section":"Tags","summary":"","title":"Über Mich","type":"tags"},{"content":"","date":"1 Juli 2025","externalUrl":null,"permalink":"/tags/active-directory/","section":"Tags","summary":"","title":"Active Directory","type":"tags"},{"content":"","date":"1 Juli 2025","externalUrl":null,"permalink":"/tags/extern/","section":"Tags","summary":"","title":"Extern","type":"tags"},{"content":"","date":"1 Juli 2025","externalUrl":null,"permalink":"/tags/hardware/","section":"Tags","summary":"","title":"Hardware","type":"tags"},{"content":"Hier findet man eine Übersicht über die im Homelab verwendete Hardware.\nSpeicher (Storage) # Stromversorgung \u0026amp; USV # Zusätzliche Geräte (IoT, SBCs) # Netzwerk-Infrastruktur # Fritzbox 7590 Unifi US 8 PoE 150W (2x) Unifi Nano HD (2x) Unifi U6 Pro Compute-Ressourcen (Server) # Aktuell nutze ich Mini-PC\u0026rsquo;s als Server, da Sie relativ günstig und stromsparend sind.\nSetup mit Ansible # Nachdem ein Server das erste mal gestartet wurde und der Storage wie auch das remote unlock konfiguriert wurde, übernimmt Ansible die volle Kontrolle.\nLisa # Primärfunktion # Virtualisierungshost\nZusätzlich installierte Software # keine\nHardware Anpassungen # Lisa wurde mit einem Akasa Gehäuse ausgestattet und wird daher passiv gekühlt\nTechnische Spezifikationen # Merkmal Spezifikation Typ Mini-PC Chipsatz System-on-Chip (SoC) CPU Intel Core i7-8559U, 4C/8T, 2.70-4.50GHz, 8MB+1MB Cache, 28W TDP, 20W cTDP, Codename \u0026ldquo;Coffee Lake-U\u0026rdquo; (Coffee Lake, Intel 14nm++) RAM 2x DDR4 SO-DIMM, dual PC4-19200S/​DDR4-2400, max. 32GB (UDIMM) Festplatte 1x SATA 6Gb/​s, 1x M.2/​M-Key (PCIe 3.0 x4/​SATA, 2280/​2242) Schächte intern 1x 2.5\u0026quot; (9.5mm) Grafik Intel Iris Plus Graphics 655 (iGPU), 48EU/384SP, 0.30-1.20GHz, Architektur \u0026ldquo;Gen 9.5\u0026rdquo; (Coffee Lake GT3e) Anschlüsse hinten 1x HDMI 2.0a, 2x USB-A 3.1 (10Gb/​s), 1x Thunderbolt 3 (40Gb/​s), 1x RJ-45 1Gb LAN (Intel I219-V), 1x Hohlbuchse (Netzanschluss) Anschlüsse vorne/Seite 1x USB-A 3.1 (10Gb/​s), 1x USB-A 3.1 (10Gb/​s, BC), 1x 3.5mm Klinke, 1x microSD Wireless Wi-Fi 5 (WLAN 802.11a/​b/​g/​n/​ac, 2x2, Intel 9560, verlötet), Bluetooth 5.1 Cardreader microSD Netzteil extern, 1x Hohlstecker (90W) VESA-Halterung 75x75/​100x100 (inkludiert) Abmessungen (BxHxT) 117x51x112mm Volumen 0.7l CPU-Kühler inkludiert Besonderheiten Sicherheitsschloss (Kensington), Infrarot-Sensor Datenblatt Intel ARK\nMarge # Primärfunktion # Backuphost\nZusätzlich installierte Software # Virtualisierungshost\nHardware Anpassungen # Marge wurde mit einem Akasa Gehäuse ausgestattet und wird daher passiv gekühlt\nMarge verfügt über ein externes Thunderbolt 3 Storage mit folgender Spezifikation # Merkmal Spezifikation Anschluss intern 4x 2.5\u0026quot; SATA 6Gb/​s Formfaktor 2.5\u0026quot; Anschlüsse extern 1x Thunderbolt 3 /​ USB-C 3.1 (40Gb/​s), 1x Thunderbolt 3 Out /​ USB-C 3.1 mit DisplayPort-Out 1.4 (40Gb/​s) RAID-Level 0/​1/​JBOD Lüfter 1x 60mm Abmessungen 193x116x96mm Gewicht 1.1kg Farbe schwarz Besonderheiten Kensington Security Slot Datenblatt\nTechnische Spezifikationen # Merkmal Spezifikation Typ Mini-PC Chipsatz System-on-Chip (SoC) CPU Intel Core i5-8259U, 4C/8T, 2.30-3.80GHz, 6MB+1MB Cache, 28W TDP, 20W cTDP, Codename \u0026ldquo;Coffee Lake-U\u0026rdquo; (Coffee Lake, Intel 14nm++) RAM 2x DDR4 SO-DIMM, dual PC4-19200S/​DDR4-2400, max. 32GB (UDIMM) Festplatte 1x SATA 6Gb/​s, 1x M.2/​M-Key (PCIe 3.0 x4/​SATA, 2280/​2242) Schächte intern 1x 2.5\u0026quot; (9.5mm) Grafik Intel Iris Plus Graphics 655 (iGPU), 48EU/384SP, 0.30-1.05GHz, Architektur \u0026ldquo;Gen 9.5\u0026rdquo; (Coffee Lake GT3e) Anschlüsse hinten 1x HDMI 2.0a, 2x USB-A 3.1 (10Gb/​s), 1x Thunderbolt 3 (40Gb/​s), 1x RJ-45 1Gb LAN (Intel I219-V), 1x Hohlbuchse (Netzanschluss) Anschlüsse vorne/Seite 1x USB-A 3.1 (10Gb/​s), 1x USB-A 3.1 (10Gb/​s, BC), 1x 3.5mm Klinke, 1x microSD Wireless Wi-Fi 5 (WLAN 802.11a/​b/​g/​n/​ac, 2x2, Intel 9560, verlötet), Bluetooth 5.1 Cardreader microSD Netzteil extern, 1x Hohlstecker (90W) VESA-Halterung 75x75/​100x100 (inkludiert) Abmessungen (BxHxT) 117x51x112mm Volumen 0.7l CPU-Kühler inkludiert Besonderheiten Sicherheitsschloss (Kensington), Infrarot-Sensor Datenblatt Intel ARK\nAbe # Primärfunktion # Virtualisierungshost\nZusätzlich installierte Software # keine\nHardware Anpassungen # Abe wurde mit einem Akasa Gehäuse ausgestattet und wird daher passiv gekühlt\nTechnische Spezifikationen # Merkmal Spezifikation Typ Mini-PC Chipsatz System-on-Chip (SoC) Sockel N/A CPU Intel Core i7-1165G7, 4C/8T, 1.20-4.70GHz, 12MB+5MB Cache, 12W TDP, 12-28W cTDP, Codename \u0026ldquo;Tiger Lake-UP3\u0026rdquo; (Willow Cove, Intel 10nm SuperFin) RAM 2x DDR4 SO-DIMM, dual PC4-25600S/​DDR4-3200, max. 64GB (UDIMM) Festplatte 1x SATA 6Gb/​s, 1x M.2/​M-Key (PCIe 4.0 x4, 2280), 1x M.2/​B-Key (PCIe 3.0 x1/​SATA, 2242) Schächte intern 1x 2.5\u0026quot; (7mm) Schächte extern N/A Grafik Intel Iris Xe Graphics (iGPU), 96EU/768SP, 1.30GHz, Architektur \u0026ldquo;Gen 12.1\u0026rdquo; (Tiger Lake GT2) GPU-Anschlüsse N/A Anschlüsse hinten 2x HDMI 2.0b, 1x USB-A 3.1 (10Gb/​s), 1x USB-A 2.0 (480Mb/​s), 1x Thunderbolt 4 (40Gb/​s), 1x Thunderbolt 3 (40Gb/​s), 1x RJ-45 2.5Gb LAN (Intel I225-LM), 1x Hohlbuchse (Netzanschluss) Nachrüstbare Anschlüsse hinten 1x seriell Anschlüsse vorne/Seite 2x USB-A 3.1 (10Gb/​s) Erweiterungsslots 1x M.2/​E-Key (Intel CNVi, 2230, belegt mit WiFi+BT-Modul) Wireless Wi-Fi 6 (WLAN 802.11a/​b/​g/​n/​ac/​ax, 2x2, Intel AX201), Bluetooth 5.2 Cardreader N/A Netzteil extern, 1x Hohlstecker (120W/​19V) VESA-Halterung 75x75/​100x100 (inkludiert) Abmessungen (BxHxT) 117x54x112mm Volumen 0.7l CPU-Kühler inkludiert Besonderheiten Sicherheitsschloss (Kensington), 4x Display Support, geeignet für Dauerbetrieb Datenblatt Intel ARK\nNed # Primärfunktion # Virtualisierungshost\nZusätzlich installierte Software # keine\nHardware Anpassungen # keine\nTechnische Spezifikationen # Merkmal Spezifikation Typ Mini-PC Chipsatz System-on-Chip (SoC) CPU AMD Ryzen 7 8840U, 8C/16T, 3.30-5.10GHz, 16MB+8MB Cache, 28W TDP, 15-30W cTDP, Codename \u0026ldquo;Hawk Point\u0026rdquo; (Zen 4, TSMC 4nm) RAM 2x DDR5 SO-DIMM, dual PC5-44800S/​DDR5-5600, max. 96GB (UDIMM) Festplatte 1x M.2/​M-Key (PCIe 4.0 x4, 2280/​2242), 1x M.2/​M-Key (PCIe 4.0 x4, 2242) Grafik AMD Radeon 780M (iGPU), 12CU/768SP, 2.70GHz, Architektur \u0026ldquo;RDNA 3\u0026rdquo; (Hawk Point) Anschlüsse hinten 2x HDMI 2.1, 2x USB-A 2.0 (480Mb/​s), 1x RJ-45 2.5Gb LAN (Realtek RTL8125BG), 1x RJ-45 1Gb LAN (Realtek RTL8111EPV), 1x Hohlbuchse (Netzanschluss) Anschlüsse vorne/Seite 2x USB4 mit DisplayPort 1.4a (40Gb/​s), 1x USB-A 3.1 (10Gb/​s), 1x 3.5mm Klinke (Realtek ALC256) Erweiterungsslots 1x M.2/​E-Key (PCIe, 2230, belegt mit WiFi+BT-Modul) Wireless Wi-Fi 6E (WLAN 802.11a/​b/​g/​n/​ac/​ax, 2x2), Bluetooth 5.2 Audio Realtek ALC256 Netzteil extern, 1x Hohlstecker (120W/​19V) VESA-Halterung 75x75/​100x100 (inkludiert) Abmessungen (BxHxT) 117.5x47.9x110mm Gewicht 1.00kg Volumen 0.6l CPU-Kühler inkludiert Besonderheiten Sicherheitsschloss (Kensington), 4x Display Support, TPM 2.0, Fernwartung mit kompatibler CPU (DMTF DASH) Anleitung\nMoe # Primärfunktion # Virtualisierungshost\nZusätzlich installierte Software # keine\nHardware Anpassungen # Thread/Matter Stick\nTechnische Spezifikationen # Merkmal Spezifikation Typ Mini-PC Chipsatz System-on-Chip (SoC) CPU AMD Ryzen 7 8840U, 8C/16T, 3.30-5.10GHz, 16MB+8MB Cache, 28W TDP, 15-30W cTDP, Codename \u0026ldquo;Hawk Point\u0026rdquo; (Zen 4, TSMC 4nm) RAM 2x DDR5 SO-DIMM, dual PC5-44800S/​DDR5-5600, max. 96GB (UDIMM) Festplatte 1x M.2/​M-Key (PCIe 4.0 x4, 2280/​2242), 1x M.2/​M-Key (PCIe 4.0 x4, 2242) Grafik AMD Radeon 780M (iGPU), 12CU/768SP, 2.70GHz, Architektur \u0026ldquo;RDNA 3\u0026rdquo; (Hawk Point) Anschlüsse hinten 2x HDMI 2.1, 2x USB-A 2.0 (480Mb/​s), 1x RJ-45 2.5Gb LAN (Realtek RTL8125BG), 1x RJ-45 1Gb LAN (Realtek RTL8111EPV), 1x Hohlbuchse (Netzanschluss) Anschlüsse vorne/Seite 2x USB4 mit DisplayPort 1.4a (40Gb/​s), 1x USB-A 3.1 (10Gb/​s), 1x 3.5mm Klinke (Realtek ALC256) Erweiterungsslots 1x M.2/​E-Key (PCIe, 2230, belegt mit WiFi+BT-Modul) Wireless Wi-Fi 6E (WLAN 802.11a/​b/​g/​n/​ac/​ax, 2x2), Bluetooth 5.2 Audio Realtek ALC256 Netzteil extern, 1x Hohlstecker (120W/​19V) VESA-Halterung 75x75/​100x100 (inkludiert) Abmessungen (BxHxT) 117.5x47.9x110mm Gewicht 1.00kg Volumen 0.6l CPU-Kühler inkludiert Besonderheiten Sicherheitsschloss (Kensington), 4x Display Support, TPM 2.0, Fernwartung mit kompatibler CPU (DMTF DASH) Anleitung\n","date":"1 Juli 2025","externalUrl":null,"permalink":"/posts/homelab/hardware/","section":"Beiträge","summary":"\u003cp\u003eHier findet man eine Übersicht über die im Homelab verwendete Hardware.\u003c/p\u003e","title":"Hardware im Homelab","type":"posts"},{"content":" Server bei Hetzner # Hardware # Aktuell sind alle Server bei Hetzner AX101 Server. Die Server sind Ipv6 only bestellt und beziehen eine IPv4 aus einem Hetzner vSwitch.\nHetzner Config # Im Robot Reverse-DNS-Eintrag - Ipv4 und Ipv6 - setzen. Auch an den vswitch denken.\nBestellung von Servern # IPv4 Option. Nach der Installation wird diese direkt gekündigt. Die IPv4 Adresse bekommt der Server später über den Vswitch. Server werden immer mit Rescue System bestellt. Wir installieren unsere Maschinen selbst. Bei der Bestellung unbedingt einen SSH Key hinterlegen. Status prüfen und Puffer von etwa acht Stunden einplanen. (Lieber einen Tag zu früh bezahlen als ohne Server sein) Nach der Bestellung # Alle notwendigen Informationen werden uns Email zugesandt. Prüfe den Fingerprint des Public Keys in der Email. Sollte der Key nicht stimmen, starte den Server erneut und mit korrekten Schlüssel in das Rescue System. Prüfe beim Einloggen den Host-Key Fingerprint. Sollte dieser nicht korrekt sein, wende dich an den Hetzner Support Hetzner überprüft seine Hardware vor der Auslieferung. Sollte etwas auffällig sein, Server reklamieren. So der Server bereits eine längere Laufzeit hinter sich hat, die Lebensdauer der SSD überprüfen und gegebenenfalls tauschen lassen. Installation # Git auf dem Server installieren Repository holen Daten beschaffen: a. FQDN = überlege einen passenden Namen für die Maschine. a. CRYPT_PASS = lass dir ein sicheres Passwort in deinem Passwortgenerator generieren. a. IPV4 = Die Adresse aus der Auslieferungsmail von Hetzner\n| 1git clone https://git.zyria.de/pyrox/hetzner-install.git 2./install.sh FQDN CRYPT_PASS IPV4 3systemctl reboot Neustart, wenn keine Fehlermeldungen kamen.\nTODO: STORAGE beschreiben\nRemote unlock einrichten. Per SSH funktioniert das entsperren der Luks Partitionen bereits. Damit dies automatisch funktioniert, müssen noch alle Tang Server hinzugefügt werden, welche die Partitionen entsperren dürfen. Dazu als erstes die neue IP in der Firewall freischalten und dann folgende Kommandos auf der Kommandozeile eingeben\nSollte die Installation über das Hetzner Script fehlschlagen, folgende Anleitung nutzen: Proxmox Install on Top of Debian\nTang und Clevis # Wie in dieser Anleitung installieren auf allen physischen Hosts. Hinzufügen zu den internen Tang Servern\n1clevis luks bind -d /dev/md1 tang \u0026#39;{\u0026#34;url\u0026#34;: \u0026#34;http://[ipv6]\u0026#34;}\u0026#39; Proxmox Backup Server # Der Installation von Proxmox Backup Server folgen.\nEinrichtung des Servers mittels Ansible # 1ansible-playbook --limit \u0026lt;servername\u0026gt; site.yml -k -K ","date":"1 Juli 2025","externalUrl":null,"permalink":"/posts/homelab/hetzner_install/","section":"Beiträge","summary":"Anbei ist der Ablauf beschrieben einen Hetzner Server in das System einzubinden.","title":"Hetzner Server","type":"posts"},{"content":"","date":"1 Juli 2025","externalUrl":null,"permalink":"/posts/homelab/","section":"Beiträge","summary":"","title":"Kleines Homelab","type":"posts"},{"content":"Alle Zugangsdaten sind entweder in der Persönlichen Keepass Datendank (für personalisierte Logindaten) oder in der IT Datenbank. Der Login auf die Server geschieht über SSH Schlüssel. Ein Login mit dem Passwort ist nicht vorgesehen.\nFür den externen Zugriff ist der Login über VPN oder einem Bastion Host bastion@vpn.zyria.de:3022 möglich. Generell sollte jedoch ein Login auf die Server nur zum Suchen von Fehlern stattfinden. Die Verwaltung erfolgt ausschliesslich über Ansible. Jede händisch durchgeführte Änderung muss im Nachhinein in Ansible eingepflegt und getestet werden.\n","date":"1 Juli 2025","externalUrl":null,"permalink":"/posts/homelab/login/","section":"Beiträge","summary":"\u003cp\u003eAlle Zugangsdaten sind entweder in der Persönlichen Keepass Datendank (für\npersonalisierte Logindaten) oder in der IT Datenbank. Der Login auf die Server\ngeschieht über SSH Schlüssel. Ein Login mit dem Passwort ist nicht vorgesehen.\u003c/p\u003e\n\u003cp\u003eFür den externen Zugriff ist der Login über \u003ca\n  href=\"https://vpn.zyria.de\"\n    target=\"_blank\"\n  \u003eVPN\u003c/a\u003e oder einem Bastion Host\n\u003ca\n  href=\"mailto:bastion@vpn.zyria.de\"\u003ebastion@vpn.zyria.de\u003c/a\u003e:3022 möglich. \u003c/p\u003e\n\u003cp\u003eGenerell sollte jedoch ein Login auf die Server nur zum Suchen von Fehlern\nstattfinden. Die Verwaltung erfolgt ausschliesslich über Ansible. Jede händisch\ndurchgeführte Änderung muss im Nachhinein in Ansible eingepflegt und getestet\nwerden.\u003c/p\u003e","title":"Logins","type":"posts"},{"content":"","date":"1 Juli 2025","externalUrl":null,"permalink":"/tags/maschinen/","section":"Tags","summary":"","title":"Maschinen","type":"tags"},{"content":" Was wir nicht tun # Passwörter werden nicht im Browser oder in Dokumenten z.B. Text,Word,Excel gesichert. Passwörter werden niemals durch Mitarbeiter verschickt. Nicht per Email, Telefon, SMS etc. Eine Ausnahme bildet die IT, welche im Ausnahmefall nicht persönlich vor Ort sein kann und selbstständig dafür sorgt, dass die Übertragung sicher ist. Für die Übertragung sind unten beschriebene Datenbanken gedacht. Hat ein Mitarbeiter keinen Zugriff auf eine Datenbank und benötigt ein Kennwort, bitte die IT kontaktieren. Wie wir mit Kennworten umehen # Wir benutzen einen dezentralen Passwortmanager (unter Windows Keepassxc), welcher auf allen Rechnern installiert ist. Dieser ermöglicht das sichere Speichern von sensiblen Daten und ist universell im ganzen Unternehmen einsetzbar. Passwörter werden auch für den Transport von A nach B immer in einer Datenbank gesichert. Das Kennwort für diese Datenbank kann bei persönlichen Datenbanken selbst vergeben werden. Bei gemeinsam genutzen Datenbanken ist die IT Ansprechpartner für die Verwaltung der Kennwörter. Der Ablageort für gemeinsam genutze Kennwörter ist Y:\\Zugangsdaten\nDatenbanken werden mit wachsender größe Langsam. Das Aufteilen in mehrere Datenbanken sollte in Betracht gezogen werden, wenn es organisatorisch sinnvoll ist. Sich selbst Kennwörter ausdenken ist oft nicht ausreichend. Wir benutzen daher zufällig erzeugte Kennwörter aus Keepassxc. Die Kennwörter sind nicht dazu gedacht, dass sie von Hand irgendwo eingetragen werden müssen und dürfen somit sehr komplex sein. Keepassxc zeigt an, wenn ein Kennwort schwach ist oder sogar bereits öffentlich bekannt ist. Entsprechendes Kennwort muss umgehend geändert werden so möglich. ","date":"1 Juli 2025","externalUrl":null,"permalink":"/posts/homelab/passworter/","section":"Beiträge","summary":"Umgang mit Passwörtern","title":"Passwörter","type":"posts"},{"content":"","date":"1 Juli 2025","externalUrl":null,"permalink":"/tags/samba/","section":"Tags","summary":"","title":"Samba","type":"tags"},{"content":" ms-MCS-AdmPwd und ms-MCS-AdmPwdExpirationTime # 1dn: CN=ms-MCS-AdmPwd,CN=Schema,cn=configuration,DC=ad,DC=zyria,DC=de 2changetype: add 3objectClass: attributeSchema 4ldapDisplayName: ms-MCS-AdmPwd 5adminDisplayName: ms-MCS-AdmPwd 6adminDescription: Stores password of local Administrator account on workstation 7attributeId: 1.2.840.113556.1.8000.2554.50051.45980.28112.18903.35903.6685103.1224907.2.1 8attributeSyntax: 2.5.5.5 9omSyntax: 19 10isSingleValued: TRUE 11systemOnly: FALSE 12searchFlags: 648 13isMemberOfPartialAttributeSet: FALSE 14showInAdvancedViewOnly: FALSE 15 16dn: CN=ms-MCS-AdmPwdExpirationTime,CN=Schema,cn=configuration,DC=ad,DC=zyria,DC=de 17changetype: add 18objectClass: attributeSchema 19ldapDisplayName: ms-MCS-AdmPwdExpirationTime 20adminDisplayName: ms-MCS-AdmPwdExpirationTime 21adminDescription: Stores timestamp of last password change 22attributeId: 1.2.840.113556.1.8000.2554.50051.45980.28112.18903.35903.6685103.1224907.2.2 23attributeSyntax: 2.5.5.16 24omSyntax: 65 25isSingleValued: TRUE 26systemOnly: FALSE 27searchFlags: 0 28isMemberOfPartialAttributeSet: FALSE 29showInAdvancedViewOnly: FALSE modify computer # 1dn: CN=computer,CN=Schema,cn=configuration,DC=ad,DC=zyria,DC=de 2changetype: Modify 3add: mayContain 4mayContain: ms-MCS-AdmPwd 5mayContain: ms-MCS-AdmPwdExpirationTime ldapPublicKey # 1#dn: CN=sshPublicKey,CN=Schema,CN=Configuration,DC=ad,DC=zyria,DC=de 2#changetype: add 3#objectClass: top 4#objectClass: attributeSchema 5#attributeID: 1.3.6.1.4.1.24552.500.1.1.1.13 6#cn: sshPublicKey 7#name: sshPublicKey 8#lDAPDisplayName: sshPublicKey 9#description: MANDATORY: OpenSSH Public key 10#attributeSyntax: 2.5.5.10 11#oMSyntax: 4 12#isSingleValued: FALSE 13#searchFlags: 8 14 15dn: CN=ldapPublicKey,CN=Schema,CN=Configuration,DC=ad,DC=zyria,DC=de 16changetype: add 17objectClass: top 18objectClass: classSchema 19governsID: 1.3.6.1.4.1.24552.500.1.1.2.0 20cn: ldapPublicKey 21name: ldapPublicKey 22description: MANDATORY: OpenSSH LPK objectclass 23lDAPDisplayName: ldapPublicKey 24subClassOf: top 25objectClassCategory: 3 26defaultObjectCategory: CN=ldapPublicKey,CN=Schema,CN=Configuration,DC=ad,DC=zyria,DC=de 27mayContain: sshPublicKey modify user # 1dn: CN=User,CN=Schema,CN=Configuration,DC=ad,DC=zyria,DC=de 2changetype: modify 3replace: auxiliaryClass 4auxiliaryClass: ldapPublicKey 5auxiliaryClass: posixAccount 6auxiliaryClass: shadowAccount ","date":"1 Juli 2025","externalUrl":null,"permalink":"/posts/homelab/samba-schema/","section":"Beiträge","summary":"Active Directory Schema Erweiterungen","title":"Samba Schema","type":"posts"},{"content":"Alle Maschinen dienen der Virtualisierung von diversen Systemen. Wir brauche die Flexibilität Systeme schnell skalieren zu können und auch durch neue zu ersetzen.\nPhysisch vorhandene Server # Die Installation von zusätzlichen Diensten auf den physischen Servern sollte dringend vermieden werden. So ein zusätzlicher Dienst installiert werden musste, ist eine Dokumentation dringend erforderlich.\nVirtuelle Maschinen # Virtuelle Maschinen sind so zu provisionieren, dass ein Server ausfallen kann und alle Dienste auf den verbliebenen Servern laufen können. Damit wir flexibel bleiben bei der Migration zu anderen Hostern und vielleicht in die Cloud werden virtuelle Maschinen anhand der Matrix bei Hetzner dimensioniert.\nAuf den virtuellen Maschinen wird primär Docker eingesetzt. In Ausnahmefällen nutzen wir auch dedizierte virtuelle Maschinen für einzelne Dienste(Active Directory,Windows). So kein Docker eingesetzt wird, sollte darauf geachtet werden, dass pro Dienst eine Maschine bereitgestellt wird.\n","date":"1 Juli 2025","externalUrl":null,"permalink":"/posts/homelab/server/","section":"Beiträge","summary":"\u003cp\u003eAlle Maschinen dienen der Virtualisierung von diversen Systemen. Wir brauche die Flexibilität Systeme schnell skalieren zu können und auch durch neue zu ersetzen.\u003c/p\u003e","title":"Server","type":"posts"},{"content":"Wir verwenden zur generellen Verwaltung von Servern Ansible. Für manche Dienste ist es jedoch notwendig oder praktikabler dies über vorhandene Portale zu realisieren.\nCore-Networks (DNS) # Unsere DNS Server werden bei Core-Networks gehostet. In deren Webinterface lassen sich alle relevanten Änderungen durchführen.\nHetzner (Server,IP,Reverse DNS) # Hetzner bietet ein Interface zur Steuerung an. Dort werden und müssen auch Reverse DNS Einträge angelegt. Für die Vswitch und für Server.\nCloudserver werden über ein eigenes Interface gesteuert.\nExternes Monitoring # Monitoring für einzenle Dienste und alle Server und IP\u0026rsquo;s über Hetrixtools\nAnydesk # Anydesk Verwaltung\nProxmox (VM) # Jeder Proxmox Host hat auch ein eigenes Webinterface, welches unter https://example.com:8006/ erreichen lässt. Für einen angenehmeren Zugang steht der Zugang über den Load Balancer bereit.\nVPN # Wirguard läuft in einem Docker Container mit Webinterface und wird komplett darüber konfiguriert.\nKonfiguration/Content (git) # Alle Konfigurationsdateien und Teile von statischem Content liegen auf einer Forgejo (ehemals Gitea) Instanz.\nNetzwerk # Wir setzen weitestgehend auf Zubehör von Unifi um das Netzwerk und Kameraüberwachung zu realisieren. Der Controller läuft in einem Docker Container.\nStatusseite/Monitoring # Über Uptime-Kuma wurde ein rudimentäres Monitoring eingrichet und den Mitarbeitern bekannt gegeben. Dies sollte die Erste Anlaufstelle sein, wenn etwas nicht funktioniert.\nCI/CD # Das Forgejo eigene CI/CD Werkzeug wird genutzt.\nSpamerkennung # Zur Spamabwehr nutzen wir Rspamd. Das Interface ist lässt die Konfiguration relevanter Parameter zu und ermöglicht Debugging bei der Mailzustellung.\nWerbeblocker/DNS # Pihole Webinterface\n","date":"1 Juli 2025","externalUrl":null,"permalink":"/posts/homelab/verwaltung/","section":"Beiträge","summary":"\u003cp\u003eWir verwenden zur generellen Verwaltung von Servern Ansible. Für manche Dienste\nist es jedoch notwendig oder praktikabler dies über vorhandene Portale zu\nrealisieren.\u003c/p\u003e\n\n\u003ch2 class=\"relative group\"\u003eCore-Networks (DNS)\n    \u003cdiv id=\"core-networks-dns\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#core-networks-dns\" aria-label=\"Anker\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cp\u003eUnsere DNS Server werden bei \u003ca\n  href=\"https://www.core-networks.de\"\n    target=\"_blank\"\n  \u003eCore-Networks\u003c/a\u003e\ngehostet. In deren \u003ca\n  href=\"https://iface.core-networks.de\"\n    target=\"_blank\"\n  \u003eWebinterface\u003c/a\u003e lassen sich\nalle relevanten Änderungen durchführen.\u003c/p\u003e","title":"Server-,Diensteverwaltung","type":"posts"},{"content":"","date":"1 Juli 2025","externalUrl":null,"permalink":"/tags/service/","section":"Tags","summary":"","title":"Service","type":"tags"},{"content":" Windows Rechner mit Professional Version kaufen (Aufgeräumt wird über Ansible)\nWindows Rechner vollständig laden\nBeim ersten Anschalten lokalen Benutzer oder Domäne auswählen.\nLokaler Benutzer ist der Hauptnutzer des Gerätes.\nKennwort wird in der IT Datenbank gesichert\nAlle Datensammelei während des Setups verbieten\nRechner in Domäne aufnehmen\nAnsible Setup Script starten (liegt im ansible-main Repo unter files)\nAnsible Variablen setzen für den Host (VPN)\nAnsible laufen lassen (Neustarts notwendig und werden nicht durch Ansible ausgeführt. Einfach solang bis es durchläuft)\nWawi testen\nRechner ausliefern und mit dem Endnutzer einrichten (Wawi, Mail, Browser)\n","date":"1 Juli 2025","externalUrl":null,"permalink":"/posts/homelab/setup_windows/","section":"Beiträge","summary":"\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eWindows Rechner mit Professional Version kaufen (Aufgeräumt wird über Ansible)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWindows Rechner vollständig laden\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eBeim ersten Anschalten lokalen Benutzer oder Domäne auswählen.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eLokaler Benutzer ist der Hauptnutzer des Gerätes.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eKennwort wird in der IT Datenbank gesichert\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eAlle Datensammelei während des Setups verbieten\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eRechner in Domäne aufnehmen\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eAnsible Setup Script starten (liegt im ansible-main Repo unter files)\u003c/p\u003e","title":"Setup Windows Client","type":"posts"},{"content":"Die eingesetzte Software partiell dokumentiert.\nSoftwareliste # Moe Proxmox Lisa Proxmox ","date":"1 Juli 2025","externalUrl":null,"permalink":"/posts/homelab/software/","section":"Beiträge","summary":"\u003cp\u003eDie eingesetzte Software partiell dokumentiert.\u003c/p\u003e","title":"Software","type":"posts"},{"content":"","date":"1 Juli 2025","externalUrl":null,"permalink":"/tags/software/","section":"Tags","summary":"","title":"Software","type":"tags"},{"content":"","date":"1 Juli 2025","externalUrl":null,"permalink":"/tags/vpn/","section":"Tags","summary":"","title":"Vpn","type":"tags"},{"content":" AutoLogon # Erstelle eine .reg Datei mit folgendem Inhalt und führe sie auf dem Zielrechner aus.\n1Windows Registry Editor Version 5.00 2 3[HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\\Winlogon] 4\u0026#34;AutoAdminLogon\u0026#34;=\u0026#34;1\u0026#34; 5\u0026#34;DefaultUserName\u0026#34;=\u0026#34;BENUTZERNAME\u0026#34; 6\u0026#34;DefaultDomainName\u0026#34;=\u0026#34;DOMÄNE\u0026#34; 7\u0026#34;DefaultPassword\u0026#34;=\u0026#39;*****\u0026#39; ","date":"1 Juli 2025","externalUrl":null,"permalink":"/posts/homelab/windows_autologon/","section":"Beiträge","summary":"\u003ch2 class=\"relative group\"\u003eAutoLogon\n    \u003cdiv id=\"autologon\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#autologon\" aria-label=\"Anker\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cp\u003eErstelle eine .reg Datei mit folgendem Inhalt und führe sie auf dem Zielrechner\naus.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-registry\" data-lang=\"registry\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"ln\"\u003e1\u003c/span\u003e\u003cspan class=\"cl\"\u003eWindows Registry Editor Version 5.00\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"ln\"\u003e2\u003c/span\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"ln\"\u003e3\u003c/span\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003e[\u003c/span\u003e\u003cspan class=\"nb\"\u003eHKEY_LOCAL_MACHINE\u003c/span\u003e\u003cspan class=\"k\"\u003e\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\\Winlogon]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"ln\"\u003e4\u003c/span\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"na\"\u003e\u0026#34;AutoAdminLogon\u0026#34;\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;1\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"ln\"\u003e5\u003c/span\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"na\"\u003e\u0026#34;DefaultUserName\u0026#34;\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;BENUTZERNAME\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"ln\"\u003e6\u003c/span\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"na\"\u003e\u0026#34;DefaultDomainName\u0026#34;\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#34;DOMÄNE\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"ln\"\u003e7\u003c/span\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"na\"\u003e\u0026#34;DefaultPassword\u0026#34;\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#39;*****\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e","title":"Windows autologon","type":"posts"},{"content":"Für eine WireGuard-Verbindung mit systemd-networkd und systemd-resolved.\nAlle Konfigurationsdateien gehören in /etc/systemd/network/.\nInterface # Das WireGuard-Interface anlegen (muss mit einer Zahl beginnen, z.B. 10-wg0.netdev) (Dokumentation)\n1[NetDev] 2Name=wg0 3Kind=wireguard 4Description=WireGuard-Tunnel für Casa Due Pur 5MTUBytes=1320 6 7[WireGuard] 8PrivateKey=... 9# Für alle Netze aus AllowedIPs soll automatisch eine Route erzeugt werden, in einer separaten Routing-Tabelle: 10RouteTable=123 11# Die verschlüsselten WireGuard-Pakete werden markiert: 12FirewallMark=123 13# Beides erlaubt spezielles Routing, konfiguriert in der *.network-Datei 14 15[WireGuardPeer] 16PublicKey=\u0026lt;Public Key\u0026gt; 17PresharedKey=... 18AllowedIPs=0.0.0.0/0,::1 19Endpoint = vpn.casa-due-pur.de:51194 20PersistentKeepalive=16 PrivateKey= und PresharedKey= entsprechend ausfüllen, den Rest sicherheitshalber nochmal abgleichen. Die Datei sollte für unprivilegierte Nutzer nicht lesbar sein, d.h. chown root:systemd-network ..., chmod 640 ....\nNetzwerk # Nun das passende Netzwerk zum Interface anlegen (z.B. wg0.network) (Dokumentation)\n1[Match] 2Name=wg0 3 4[Network] 5Address=... 6Address=... 7 8# Nur Domains mit dieser Endung über diesen DNS auflösen: 9Domains=~casa-due-pur.de 10DNS=2a01:4f8:fff0:22:ca9:16ff:fe6e:cc14 11DNS=159.69.238.89 12DNSDefaultRoute=false 13 14[Link] 15RequiredForOnline=no 16# Optional: nicht automatisch beim Systemstart verbinden 17ActivationPolicy=manual 18 19# Die verschlüsselten WireGuard-Pakete sind mit 123 markiert und dürfen nicht über diese Verbindung geleitet werden. 20# Die WireGuard-spezifischen Routen sind in einer Tabelle mit id 123. 21# Diese Policy wendet die spezifischen Routen ausschliesslich auf unmarkierte Pakete an. 22[RoutingPolicyRule] 23FirewallMark=123 24Table=123 25InvertRule=true 26Family=both Adressen (ipv4 und ipv6) eintragen. Der Tunnel kann mit networkctl up wg0 bzw networkctl down wg0 nach Bedarf verbunden und getrennt werden.\nSobald der Tunnel verbunden ist, lässt sich die Routing-Tabelle mit ip route show table 123 anzeigen. Die RoutingPolicy sollte in ip rule auftauchen.\nDNS # Wir haben jetzt zwei DNS-Server im System. Falls bei beiden Domains= gesetzt ist, muss der vorhandene Server explizite als default markiert werden (z.B. in eth0.network):\n1[Network] 2DNSDefaultRoute=true Firewall # Es wird kein Routing konfiguriert, daher sind normalerweise keine Änderungen an iptables notwendig.\nAm Beispiel einer Firewall mit striktem whitelisting sind mindestens folgende Verbindungen zu erlauben:\nWireGuard-Traffic zum Endpoint iptables -A OUTPUT -m owner ! --uid-owner 0-99999 -p udp -d 159.69.238.84 --dport 51194 -j ACCEPT (Wireguard-Pakete kommen aus dem Kernel und sind keinem Benutzer zugeordnet) Ausgehender traffic auf wg0: iptables -A OUTPUT -o wg0 -j ACCEPT, ggf. mit -m owner --uid-owner ... auf einen Nutzer beschränkt. DNS-Traffic über den Tunnel: iptables -A OUTPUT -m owner --uid-owner systemd-resolve -d 159.69.238.89 -o wg0 j ACCEPT Eingehende pings und andere ICMP-Pakete auf wg0: iptables -A INPUT -i wg0 -p icmp -j ACCEPT Optional anderer eingehender Traffic auf wg0, z.B. ssh: iptables -A INPUT -i wg0 -p tcp --dport 22 -j ACCEPT (kann nicht nach Nutzer gefiltert werden) Die Firewall muss verbindungsorientiert sein, üblicherweise mit einer der folgenden Methoden:\nZugehörige Pakete werden direkt akzeptiert, z.B. mit iptables -A INPUT/OUTPUT -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT (Tutorial) Verbindungen werden markiert (statt -j ACCEPT nutzt man -j CONNMARK --set-mark ...) und markierte Verbindungen werden erlaubt (iptables -A INPUT/OUTPUT -m connmark --mark ... -j ACCEPT). Diese Variante ist umständlicher, erlaubt aber Traffic Shaping oder separate Statistiken pro verwendeter Markierung. Für ipv6 sind äquivalente Regeln einzurichten.\n","date":"1 Juli 2025","externalUrl":null,"permalink":"/posts/homelab/wireguard_mit_systemd/","section":"Beiträge","summary":"\u003cp\u003eFür eine WireGuard-Verbindung mit systemd-networkd und systemd-resolved.\u003c/p\u003e","title":"WireGuard mit systemd","type":"posts"},{"content":" Primärfunktion # Samba Active Directory Domain Controller und Fileserver\nInstallierte Software # Samba-AD-DC ","date":"1 Juli 2025","externalUrl":null,"permalink":"/posts/homelab/vms/addc/","section":"Beiträge","summary":"\u003ch2 class=\"relative group\"\u003ePrimärfunktion\n    \u003cdiv id=\"primärfunktion\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#prim%c3%a4rfunktion\" aria-label=\"Anker\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cp\u003eSamba Active Directory Domain Controller und Fileserver\u003c/p\u003e\n\n\u003ch2 class=\"relative group\"\u003eInstallierte Software\n    \u003cdiv id=\"installierte-software\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#installierte-software\" aria-label=\"Anker\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eSamba-AD-DC\u003c/li\u003e\n\u003c/ul\u003e","title":"Addc","type":"posts"},{"content":" Primärfunktion # Host für Podman Container.\nInstallierte Software # cockpit podman k3s ","date":"1 Juli 2025","externalUrl":null,"permalink":"/posts/homelab/vms/dev/","section":"Beiträge","summary":"\u003ch2 class=\"relative group\"\u003ePrimärfunktion\n    \u003cdiv id=\"primärfunktion\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#prim%c3%a4rfunktion\" aria-label=\"Anker\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cp\u003eHost für Podman Container.\u003c/p\u003e\n\n\u003ch2 class=\"relative group\"\u003eInstallierte Software\n    \u003cdiv id=\"installierte-software\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#installierte-software\" aria-label=\"Anker\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003ecockpit\u003c/li\u003e\n\u003cli\u003epodman\u003c/li\u003e\n\u003cli\u003ek3s\u003c/li\u003e\n\u003c/ul\u003e","title":"Dev","type":"posts"},{"content":"","date":"27 Oktober 2024","externalUrl":null,"permalink":"/tags/ansible/","section":"Tags","summary":"","title":"Ansible","type":"tags"},{"content":"","date":"27 Oktober 2024","externalUrl":null,"permalink":"/categories/infrastructure/","section":"Categories","summary":"","title":"Infrastructure","type":"categories"},{"content":"","date":"27 Oktober 2024","externalUrl":null,"permalink":"/categories/kubernetes/","section":"Categories","summary":"","title":"Kubernetes","type":"categories"},{"content":"","date":"27 Oktober 2024","externalUrl":null,"permalink":"/tags/longhorn/","section":"Tags","summary":"","title":"Longhorn","type":"tags"},{"content":"","date":"27 Oktober 2024","externalUrl":null,"permalink":"/tags/recovery/","section":"Tags","summary":"","title":"Recovery","type":"tags"},{"content":"Übersicht über unsere Solaranlage mit Komponenten # Als Netto mit einem Solarkraftwerk in Selbstmontage geworben hat und unser Stromanbieter schon wieder eine Preiserhöhung ins Haus flatter ließ, war es um mich passiert. Es musste etwas passieren.\nZu Beginn meiner Reise habe ich mich für ein 830KWp Kraftwerk mit Deye Wechselrichter entschieden.\nAuflistung aller Komponenten unserer Solaranlage Komponente Kosten inkl. Versand іn € Bemerkung Netto Solarkraftwerk 702,94 Wechselrichter nicht mehr in Betrieb Juskys Aufständerung 299,98 Zur Boden und Balkonmontage geeignet Van der Valk Solar Systems ValkBox 3 (3mal) 134,97 Bodenmontage Anschlussleitung Kabel Betteri BC05 - Schuko 3m Field Connector BC05 Kabel 29,90 3 Verlängerungskabel beidseitig Solarstecker Solarkabel rot/schwarz 6mm² 2 x 10 m (3mal) 101,85 2x20m Solarkabel 6mm2, Solarkabel Verlängerung mit Stecker (2mal) 157,98 6mm² Solarkabel Y-Verteiler 2 Paar (2mal) 15,99 Verlängerungskabel beidseitig Solarstecker Solarkabel rot/schwarz 6mm² 2 x 10 m (3mal) 58,95 Solar Crimpzange Set für 2.5/4.0/6.0mm² Solarpanel PV Kabel, Male Female Kabelstecker Stecker mit SOMELINE® +Abisolierzange+Kabelschneider+Schraubenschlüssel 49,99 Solarflow Antennen (2mal) 8,49 Van der Valk Solar Systems ValkBox 3 (3mal) 131,67 Hoymiles Mikro-Wechselrichter HMS-1800-4T, 1.800 Watt 269,00 LONGi HI-MO 6m Explorer Solarpanel LR5-54HTH-430M, 430W Black Frame (3mal) 299,70 Trinasolar Solarpanel Vertex S+ TSM-NEG9R.28, 440 Watt, Black Frame (3mal) 344,70 OpenDTU 48,98 Was nicht in der Liste ist aber dennoch benötigt wird: Kabelbinder, Zangen, Schraubenschlüssel\n","date":"22 März 2024","externalUrl":null,"permalink":"/posts/solar/solaranlage/","section":"Beiträge","summary":"\u003ch2 class=\"relative group\"\u003eÜbersicht über unsere Solaranlage mit Komponenten\n    \u003cdiv id=\"übersicht-über-unsere-solaranlage-mit-komponenten\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#%c3%bcbersicht-%c3%bcber-unsere-solaranlage-mit-komponenten\" aria-label=\"Anker\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cp\u003eAls Netto mit einem Solarkraftwerk in Selbstmontage geworben hat und unser Stromanbieter schon wieder eine Preiserhöhung ins Haus flatter ließ, war es um mich passiert. Es musste etwas passieren.\u003c/p\u003e","title":"Solaranlage","type":"posts"},{"content":"Linksammlung zum Thema Strom in meinem Heim.\nSolarflow Sammlung von Links für den Aufbau und Betrieb meiner Solaranlage.\nHichi IR und Home Assistant Stromzähler mit einem ESP8266 / ESP32 mit Tasmota auslesen und darstellen\nSolarflow Hoymiles Homeassistant Git Repo mit nützlichen Infos\nSteuerung des Solarflow mit Solarflow Control Solarflow Bluetooth Manager Von Cloud trennen und an den loaklen MQTT binden\nSolarflow Statuspage and Control Private Zendure Austauschgruppe in Telegram Zendure SolarFlow - Speicher für BKW - Sammelthread Steckerlayout Solarflow Batteriesystem # Stecker-Pinout (Draufsicht auf Hub/Akku, nicht auf den Kabelstecker):\n1 _._._._._._ 2 / _ \\ 3 / /G\\ \\ 4 / _ \\_/ _ \\ 5| / \\ / \\ | 6| | + | | - | | 7| \\_/ \\_/ | 8| (1) | 9 \\(2) (5)/ 10 \\ (3) (4) / 11 \\_._._._._._/ +: Main positive power connection -: Main negative power connection (also used by battery as communication ground) G: Ground provided by hub, not connected in battery 1: +5V supply from battery to hub (possibly used during startup on battery power, unconnected on battery bottom port) 2: CAN_L 3: CAN_H 4: Hub button press detection (active low, pulled up by both hub and battery, chained between Öl batteries) 5: Hub presence detection (active low, pulled up by battery, grounded in hub, unconnected on battery bottom port) Solarflow Mqtt Zugangsdaten # 1# Mqtt Username = device_id 2# Password= 3 4#!/usr/bin/python 5import hashlib 6device_id = \u0026#34;\u0026lt;DEVICEID\u0026gt;\u0026#34; 7md5_hash = hashlib.md5(device_id.encode()).hexdigest().upper()[8:24] 8print(md5_hash) ","date":"8 März 2024","externalUrl":null,"permalink":"/posts/strom/","section":"Beiträge","summary":"\u003cp\u003eLinksammlung zum Thema Strom in meinem Heim.\u003c/p\u003e","title":"Linksammlung rund um Strom","type":"posts"},{"content":" Erster Schritt # Läuft das Gerät? Hast du das Gerät einmal neu gestartet? Abmelden und anmelden reicht nicht aus! Sind alle Kabel angeschlossen und richtig eingesteckt? Ist der Bildschirm angeschaltet? Was hast du getan, bevor das Gerät nicht mehr ging? Zweiter Schritt # Statusseite checken Dritter Schritt # Neustart des Rechners == (nach jeder Änderung!) == Vierter Schritt # Evtl. ausstehende Updates ausführen ","date":"19 Oktober 2023","externalUrl":null,"permalink":"/posts/homelab/erste_hilfe/","section":"Beiträge","summary":"\u003ch2 class=\"relative group\"\u003eErster Schritt\n    \u003cdiv id=\"erster-schritt\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#erster-schritt\" aria-label=\"Anker\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eLäuft das Gerät?\u003c/li\u003e\n\u003cli\u003eHast du das Gerät einmal neu gestartet? Abmelden und anmelden reicht nicht\naus!\u003c/li\u003e\n\u003cli\u003eSind alle Kabel angeschlossen und richtig eingesteckt?\u003c/li\u003e\n\u003cli\u003eIst der Bildschirm angeschaltet?\u003c/li\u003e\n\u003cli\u003eWas hast du getan, bevor das Gerät nicht mehr ging?\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2 class=\"relative group\"\u003eZweiter Schritt\n    \u003cdiv id=\"zweiter-schritt\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#zweiter-schritt\" aria-label=\"Anker\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca\n  href=\"https://status.zyria.com/status/index\"\n    target=\"_blank\"\n  \u003eStatusseite checken\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2 class=\"relative group\"\u003eDritter Schritt\n    \u003cdiv id=\"dritter-schritt\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#dritter-schritt\" aria-label=\"Anker\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eNeustart des Rechners == (nach jeder Änderung!) ==\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2 class=\"relative group\"\u003eVierter Schritt\n    \u003cdiv id=\"vierter-schritt\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#vierter-schritt\" aria-label=\"Anker\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e\n    \n\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eEvtl. ausstehende Updates ausführen\u003c/li\u003e\n\u003c/ul\u003e","title":"Erste Hilfe","type":"posts"},{"content":"","date":"21 Juli 2023","externalUrl":null,"permalink":"/tags/datenschutz/","section":"Tags","summary":"","title":"Datenschutz","type":"tags"},{"content":"","date":"21 Juli 2023","externalUrl":null,"permalink":"/tags/legal/","section":"Tags","summary":"","title":"Legal","type":"tags"},{"content":"","date":"21 Juli 2023","externalUrl":null,"permalink":"/tags/rechtstexte/","section":"Tags","summary":"","title":"Rechtstexte","type":"tags"},{"content":"","date":"21 Juli 2023","externalUrl":null,"permalink":"/tags/impressum/","section":"Tags","summary":"","title":"Impressum","type":"tags"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]